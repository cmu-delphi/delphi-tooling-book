# Introducing the ARX forecaster

The built-in ARX forecaster is a direct forecasting approach used to estimate a model for a particular target horizon. It is based upon the widely used autoregressive (AR) model, in which events in the future using a linear combination of events in the near past. An ARX model is an extension to the basic AR model where exogenous variables are included as predictors. Exogenous variables are incorporated as predictors into a forecasting model not because they are being predicted themselves, but because they add measurable value to it. So they are sometimes referred to as features or additional covariates. 

::: {.callout-note}
Preparing exogenous variables for input into an ARX model is not necessarily a simple task. For instance, it is fairly common to have to modify such variables to match the temporal resolution of the variable being forecasted upon. Additionally, there may be other considerations such as correlation or interactions between the exogenous variables. Since these are endeavours in feature selection and/or engineering, we will reserve further discussion of this for its own chapter.
:::

Similar to the flatline forecaster, prediction intervals based on the quantiles of the residuals of the training data are obtained separately for each combination of keys (most commonly by `geo_value`). Hence, the output is a data frame of point (optionally interval) forecasts at a single unique horizon for each unique combination of keys. The ARX forecaster (using quantile regression) is comparable to what the Delphi forecast team used for its COVID-19 forecasts over the course of the pandemic. 

## Example of using the ARX forecaster

```{r}
#| echo: false
#| message: false
#| warning: false
source("_common.R")
```

### Load required packages

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(epipredict)
library(workflows)
library(plotly)
```

### A brief re-introduction to the dataset

In our guided example using the ARX forecaster, we'll use the same `case_death_rate_subset` dataset that we used to showcase the flatline forecasterm which contains a subset of COVID-19 cases and deaths for US states and territories. To keep our example simple, we will only consider the data from Sept. 1 to Dec. 1, 2021.

```{r}
jhu <- case_death_rate_subset %>%
  dplyr::filter(time_value >= as.Date("2021-09-01") & time_value <= "2021-12-01")

jhu
```

### The basic mechanics of the ARX forecaster

Suppose that our primary goal is to predict death rates one week ahead of the last date available for each state. Mathematically, on day $t$, we want to predict new deaths $y$ that are $h$ days ahead at many locations $j$ using the death rate from today, 1 week ago, and 2 weeks ago. So for each location, we'll predict
$$
\hat{y}_{j,{t+h}} = \mu + a_0y_{j, t} +  a_7y_{j, t-7} + a_{14}y_{j, t-14}
$$
where $t$ is 2021-12-01, $h$ is 7 days, and $j$ is the state in our example. The $a_0$, $a_7$, and $a_14$ coefficients are obtained from fitting a model to the training data.

The above formulation makes it clear that the `arx_forecaster` function must have a predictors parameter in addition to a parameter for the data and the outcome. This is because there must be at least one predictor that is based on past values of the outcome. So the simplest way to use `arx_forecaster()` to predict the death rate one week into the future, is to create an AR-type model. And to do this, we must input a minimum of three arguments: the `epi_df` data, the outcome of `"death_rate"`, and the predictors that are based on `"death_rate"`. As we can see in the above equation and from the `arx_args_list()` documentation, the default number of lags used for the predictors are 0, 7, and 14 days. In other words, what this forecaster aims to do is to predict the outcome (`death_rate`) one week ahead based on the predictor (`death_rate`) values from today, 1 week ago and 2 weeks ago.

```{r}
ar_one_week_ahead <- arx_forecaster(jhu, outcome = "death_rate", predictors = "death_rate")
ar_one_week_ahead
```

To make sure we understand exactly how the predictors were created from the training data (lags and all), we can compare the molded/pre-processed training data that is contained in `one_week_ahead$epi_workflow$pre` to the raw training data.

```{r}
cbind(ar_one_week_ahead$epi_workflow$pre$mold$extras$roles$geo_value, 
      ar_one_week_ahead$epi_workflow$pre$mold$extras$roles$time_value, 
      ar_one_week_ahead$epi_workflow$pre$mold$predictors) %>% 
  head()
```

Now, let's just focus on the first row for `ak` and filter the corresponding raw data for that `geo_value` and any dates before the `time_value` given (since we only need to consider lags of `death_rate` to construct the predictors).

```{r}
jhu %>% filter(geo_value == "ak" & time_value <= "2021-09-15")
```

We can then check whether the predictor values from the pre-processed data have been correctly taken from the raw data. For example, the `lag_0_death_rate` should be the `death_rate` on the `time_value` of 2021-09-15 for `ak` in the raw data. And indeed it is. Similarly, we can confirm that the `lag_7_death_rate` is the value from "2021-09-08" and `lag_14_death_rate` is the value from "2021-09-01" in the raw `jhu` data. In this way, we can manually check that the model used the correct predictor data if we are so inclined.

Now that we are clear on how the lags work, we can proceed from a basic AR model to an ARX model by adding additional predictors (for any additional time-varying covariates that are being treated as exogenous variables).

```{r}
arx_one_week_ahead <- arx_forecaster(jhu, outcome = "death_rate", predictors = c("death_rate", "case_rate"))
arx_one_week_ahead
```

As with the flatline model, the result is an S3 object that by default outputs a short summary of the training data and the predictions. The object itself contains three components: the metadata, the `epi_workflow`, and the tibble of predictions. From the summary, we can note that the predictions are for the target date of Dec. 8, 2021, which is 7 days after the Dec. 1, 2021 forecast date (aka. the date on which the forecast was made). This implies that the 7 days ahead of the forecast date is the default for the target date. And as we saw for the flatline forecaster, to change this you must change the ahead parameter in the additional arguments list, `arx_args_list()`. If you look at the help file for that function, you will see that the key difference between this list and that for the flatline forecaster is that  here there's a lags parameter to enumerate the lags for predictors in autoregressive-type models. And we can tailor the number of lags to the predictor. For some practice, we'll change the lags argument for each of our `case_rate` and `death_rate` predictors as well as the ahead value to 5 days.

```{r}
arx_five_days_ahead_many_lags <- arx_forecaster(
  jhu,
  outcome = "death_rate",
  predictors = c("death_rate", "case_rate"), 
  args_list = arx_args_list(lags = list(case_rate = c(0, 1:7, 14, 21), 
                                        death_rate = c(0, 7, 14, 21)), 
                            ahead = 5L)
)
arx_five_days_ahead_many_lags
```

Now, instead of just inspecting the summary, we'll go a little deeper this time around and look into the `epi_workflow` as well as the pre-processing and post-processing operations that were carried out.

The `epi_workflow` is contains everything necessary to produce the predictions from the pre-processing, to the model fitting, to post-processing operations performed.

```{r}
arx_one_week_ahead$epi_workflow
```
Let's walk through the pre-processing that was done. The easy way to obtain that is to use `extract_preprocessor()` on the `epi_workflow` as follows:

```{r}
extract_preprocessor(arx_five_days_ahead_many_lags$epi_workflow)
```

Under Operations, we can see the pre-processing operations in the order they are carried out. Firstly lag the death and case rates by the stated lags (`step_epi_lag()`), then lead the death rate by 5 days (`step_epi_ahead()`), remove the rows containing NAs in the predictors or outcomes, and finally, do not restrict of the number of recent observations used in the training window (as in `step_training_window()` `n_training = Inf`). More pre-processing is done here in comparison to the flatline forecaster. In particular, there are now steps to lag each the predictor variables as well as to remove any rows that contain a NA value.

Next, let's have a brief look at the post-processing operations, which we can easily extract by using the `extract_frosting()` function on the `epi_workflow`.

```{r}
extract_frosting(arx_five_days_ahead_many_lags$epi_workflow)
```

We can see that the post-processing operations were to create the predictions and the corresponding 90% prediction intervals, add the forecast and target dates, and bound the predictions at zero. You may note that what was done here is the same as for the flatline forecaster.

Let's now move on to examining the predictions themselves. 

```{r}
arx_five_days_ahead_many_lags$predictions
```

For the target date of Dec. 6, 2021 (which is five days ahead of the forecast date), there is one prediction for the death rate per 100K inhabitants along with a 90% prediction interval for every state (`geo_value`). 

The following figure shows the prediction and prediction interval for three sample states: Arizona, New York, and Florida.

```{r}
#| fig-height: 5
#| code-fold: true
samp_geos <- c("az", "ny", "fl")

hist <- jhu %>%
  filter(geo_value %in% samp_geos)

preds <- arx_five_days_ahead_many_lags$predictions %>%
  filter(geo_value %in% samp_geos) %>%
  mutate(q = nested_quantiles(.pred_distn)) %>%
  unnest(q) %>%
  pivot_wider(names_from = tau, values_from = q)

ggplot(hist, aes(color = geo_value)) +
  geom_line(aes(time_value, death_rate)) +
  theme_bw() +
  geom_errorbar(data = preds, aes(x = target_date, ymin = `0.05`, ymax = `0.95`)) +
  geom_point(data = preds, aes(target_date, .pred)) +
  geom_vline(data = preds, aes(xintercept = forecast_date)) +
  scale_colour_viridis_d(name = "") +
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month") +
  facet_grid(geo_value ~ ., scales = "free_y") +
  theme(legend.position = "none") +
  labs(x = "", y = "Incident deaths per 100K\n inhabitants")
```
You may have noticed that the above figure mirrors that for the flatline forecaster. The vertical black line marks the forecast date. The point beyond that is the prediction and the band about it is the 90% prediction interval. In comparison to the flatline forecaster, the key difference is that these predictions are not simply the last values pulled forward in time. Rather, there's more variability, more separation from the recent past, as we would expect to see in nature.

What we've explored so far is for for a single ahead that is not very far off from the forecast date, but what do we see when we look at the predictions for various aheads?

Let's create predictions (and prediction intervals) for each of 1 to 28 days beyond the forecast date. To do this, we'll again use `map()` from `purrr` to apply the forecaster to each ahead value and then row bind the list of results.

```{r, warning = FALSE}
# Multiple predictions
out_df <- map(1:28, ~ arx_forecaster(
  epi_data = jhu,
  outcome = "death_rate",
  predictors = c("death_rate", "case_rate"),
  args_list = arx_args_list(ahead = .x)
)$predictions) %>%
  list_rbind()
```

Then, we can recycle the code we used to produce the plot for one forecast - the only difference being that we'll use `out_df` in place of `arx_five_days_ahead_many_lags$predictions`.

```{r}
#| fig-height: 5
#| code-fold: true
preds <- out_df %>% 
  filter(geo_value %in% samp_geos) %>% 
  mutate(q = nested_quantiles(.pred_distn)) %>% 
  unnest(q) %>%
  pivot_wider(names_from = tau, values_from = q)

ggplot(hist) +
  geom_line(aes(time_value, death_rate)) +
  geom_ribbon(
    data = preds,
    aes(x = target_date, ymin = `0.05`, ymax = `0.95`, fill = geo_value)
  ) +
  geom_point(data = preds, aes(target_date, .pred, colour = geo_value)) +
  geom_vline(data = preds, aes(xintercept = forecast_date)) +
  scale_colour_viridis_d() +
  scale_fill_viridis_d(alpha = .4) +
  scale_x_date(date_labels = "%b %Y") +
  facet_grid(rows = vars(geo_value)) +
  theme(legend.position = "none") +
  labs(x = "", y = "Incident deaths per 100K\n inhabitants")
```

All three bear a striking resemblance to what we'd expect when using the flatline forecaster. So, our simple ARX model either failed to capture the rise or fall in death rates or it aligns with the truth (there was a period of relative stability). Since we have the true number of deaths over the prediction period in the `case_death_rate_subset` data, let's go ahead and overlay them on our plot to see for ourselves how our model fared. The way that we will extend the black death rate lines is to not impose an upper bound of the data used in `hist` (so that we use all data available from our Sept. 2, 2021 start until the last date of Dec. 31, 2021).

```{r}
#| fig-height: 5
#| code-fold: true
hist <- case_death_rate_subset %>%
  dplyr::filter(time_value >= as.Date("2021-09-01")) %>%# No ub so we get up to the last date of Dec. 31, 2021
  filter(geo_value %in% samp_geos)
  
ggplot(hist) +
  geom_line(aes(time_value, death_rate)) +
  geom_ribbon(
    data = preds,
    aes(x = target_date, ymin = `0.05`, ymax = `0.95`, fill = geo_value)
  ) +
  geom_point(data = preds, aes(target_date, .pred, colour = geo_value)) +
  geom_vline(data = preds, aes(xintercept = forecast_date)) +
  scale_colour_viridis_d(alpha = .7) +
  scale_fill_viridis_d(alpha = .4) +
  scale_x_date(date_labels = "%b %Y") +
  facet_grid(rows = vars(geo_value)) +
  theme(legend.position = "none") +
  labs(x = "", y = "Incident deaths per 100K\n inhabitants")
```

We can see that the predictions get increasingly questionable as they get farther away from the forecast date (as indicated by the larger prediction bands), but still they are not entirely unreasonable. Certainly they are more more sensible than those produced by the flatline forecaster (in which we simply propagated the last observation forward). The major philosophical difference between the two forecasters is that the ARX forecaster is built on the premise that the recent past is indicative of the future, whereas the flatline forecaster proceeds as though nothing but the single most recent instance is indicative of the future.

It is clear that the ARX forecaster is almost always more justifiable than the flatline forecaster to make farther-reaching predictions. And yet, however reasonable its long-term forecasts can seem, we should be careful to not overreach and make major long term predictions with a model that only uses data from the recent past. Such predictions have little to no practical value. That is, in general, the ARX forecaster is more suited for short-term forecasts than for long-term forecasts and for instances where the future is well-represented by the past. For instance, if we expect there to be a period of major volatility coming up, but the recent past has been stable and uneventful, then a basic ARX model trained solely on the most recent past will fail to capture the volatility. In that case, amendments should be made or another model should be tried (though it may be hard to construct a good predictive model even in the best of such circumstances - in some cases it may be like trying to predict the unpredictable). This is an ever-present danger when modelling infectious diseases where there can be an explosion of cases or deaths rapidly and without much warning.

To wrap up this discussion, let's visually inspect how our predictions change over time and how they may be geospatially related. We will construct a dynamic chloropleth plot using `plotly`.

To prepare the data, we must extract the quantiles that make up the 90% predictive intervals (nested inside `.pred_distn`).

```{r}
all_the_states_df <- out_df %>% 
  mutate(q = nested_quantiles(.pred_distn)) %>% 
  unnest(q) %>%
  pivot_wider(names_from = tau, values_from = q)
```

We will base our code on that from the "Customize choropleth code" that we introduced at the end of [Regression In Tidymodels - Part 2](tidymodels-regression-part2.qmd#sec-interactive-plot). The two key changes to make to that code are to convert the target dates vector to a character vector (to be compatible with `plotly`) and to specify that frame is based on `target_date` when initializing the plotly-geo object (in `plot_geo()`). Making these changes will give us a slider bar so that we may see what happens over time for each state. 


```{r}
#| fig-height: 1.75
#| code-fold: true
# Convert target dates to char
all_the_states_df$target_date <- as.character(all_the_states_df$target_date)

# See on hover
all_the_states_df$hover <- with(all_the_states_df, 
                                paste(toupper(geo_value), "<br>", 
                                      "Pred death rate:", round(.pred, digits = 3), "<br>", 
                                      "90% pred distn:", paste(round(`0.05`, digits = 3), 
                                                               round(`0.95`, digits = 3), sep = ", ")))

# Give state boundaries a white border
l <- list(color = toRGB("white"), width = 2)

# Specify some map projection/options
g <- list(
  scope = 'usa',
  projection = list(type = 'albers usa'),
  showlakes = TRUE,
  lakecolor = toRGB('white')
)
# Initialize the plotly-geo object
fig <- plot_geo(all_the_states_df, locationmode = 'USA-states', frame=~target_date)
fig <- fig %>% add_trace(
  z = ~.pred, text = ~hover, hoverinfo = 'text', locations = ~toupper(geo_value),
  color = ~.pred, colors = 'Purples'
) 

# Add titles and such
fig <- fig %>% colorbar(title = "Death rate")
fig <- fig %>% layout(
  title = 'Predicted death rate (per 100,000) by state over time <br>(Hover for breakdown)',
  geo = g
)

fig
```

Now, you can either hit play in the above plot and watch what happens over time or you can manually slide the bar at the bottom and inspect the states by hovering over them.

One possible improvement to make to the above plot is to add the true death rates to it or to a similar plot that is positioned beside it. We will leave this extension as an exercise to the reader.

## What we've learned in a nutshell

While the ARX forecaster is a simple and intuitive model at it core, it is competitive with more complicated forecasters. So mastering the basics of this forecaster and equipping yourself with the knowledge to customize it can be fantastic to have in your forecasting toolbox.

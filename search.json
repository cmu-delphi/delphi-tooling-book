[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "",
    "text": "Preface\nThis book describes some of the functionality of the {epiprocess} and {epipredict} R packages, with an eye toward creating various types of signal processing and forecast creation for epidemiological data. The goal is to be able to load, inspect, process, and forecast — using simple baselines to more elaborate customizations."
  },
  {
    "objectID": "index.html#sec-installation",
    "href": "index.html#sec-installation",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "Installation",
    "text": "Installation\nThe following commands install the latest versions of the packages we use in this book:\n\n# install.packages(\"pak\")\n\n# Install our packages from GitHub:\npak::pkg_install(\"cmu-delphi/epidatr\")\npak::pkg_install(\"cmu-delphi/epiprocess\")\npak::pkg_install(\"cmu-delphi/epipredict\")\npak::pkg_install(\"cmu-delphi/epidatasets\")\n# Other model-fitting packages we use in this book (via epipredict):\npak::pkg_install(\"poissonreg\")\npak::pkg_install(\"ranger\")\npak::pkg_install(\"xgboost\")\n# Other data processing, model evaluation, example data, and other packages we\n# use in this book:\npak::pkg_install(\"RcppRoll\")\npak::pkg_install(\"tidyverse\")\npak::pkg_install(\"tidymodels\")\npak::pkg_install(\"broom\")\npak::pkg_install(\"performance\")\npak::pkg_install(\"modeldata\")\npak::pkg_install(\"see\")\npak::pkg_install(\"sessioninfo\")\n\nMuch of the data used for illustration can be loaded directly from Delphi’s Epidata API which is built and maintained by the Carnegie Mellon University Delphi research group. We have tried to provide most of the data used in these examples in a separate package, {epidatasets}, but it can also be accessed using {epidatr}, an R interface to the API and the successor to {covidcast}. These are also available from GitHub:\n\npak::pkg_install(\"cmu-delphi/epidatasets\")\npak::pkg_install(\"cmu-delphi/epidatr\")\n\n\n\nEncountering installation issues? Click here to show some potential solutions.\n\n\nLinux installation issues: compilation errors or slowness\nIf you are using Linux and encounter any compilation errors above, or if compilation is taking very long, you might try using the RStudio (now called Posit) Package Manager to install binaries. You can try running this command\n\noptions(\n  repos = c(\n    # contains binaries for Linux:\n    RSPM = \"https://packagemanager.rstudio.com/all/latest\",\n    # backup CRAN mirror of your choice:\n    CRAN = \"https://cran.rstudio.com/\"\n  )\n)\n\n\n\nReproducibility\nThe above commands will give you the current versions of the packages used in this book. If you’re having trouble reproducing some of the results, it may be due to package updates that took place after the book was last updated. To match the versions we used to generate this book, you can use the steps below.\n\nFirst: set up and store a GitHub PAT\nIf you don’t already have a GitHub PAT, you can use the following helper functions to create one:\n\n# Run this once:\ninstall.packages(\"usethis\")\n\n#&gt; Installing usethis [2.2.0] ...\n#&gt;  OK [linked cache in 0.22 milliseconds]\n#&gt; * Installed 1 package in 3.1 seconds.\n\nusethis::create_github_token(\n  scopes = \"public_repo\",\n  description = \"For public repo access\"\n)\n\nThis will open a web browser window allowing you to describe and customize settings of the PAT. Scroll to the bottom and click “Generate token”. You’ll see a screen that has ghp_&lt;lots of letters and numbers&gt; with a green background; you can click the two-squares (“copy”) icon to copy this ghp_...... string to the clipboard.\n\n\nEither A: Download and use the renv.lock\n\n# Run this once:\ninstall.packages(c(\"renv\", \"gitcreds\"))\ndownload.file(\"https://raw.githubusercontent.com/cmu-delphi/delphi-tooling-book/main/renv.lock\", \"delphi-tooling-book.renv.lock\")\n\n# Run this in a fresh session each time you'd like to use this set of versions.\n# Warning: don't save your GitHub PAT in a file you might share with others;\n# look into `gitcreds::gitcreds_set()` or `usethis::edit_r_environ()` instead.\nSys.setenv(\"GITHUB_PAT\" = \"ghp_............\")\nrenv::use(lockfile = \"delphi-tooling-book.renv.lock\")\n# If you get 401 errors, you may need to regenerate your GitHub PAT or check if\n# `gitcreds::gitcreds_get()` is detecting an old PAT you have saved somewhere.\n\n\n\nOr B: Download the book and use its .Rprofile\n\nDownload the book here and unzip it.\nOne-time setup: launch R inside the delphi-tooling-book directory (to use its .Rprofile file) and run\n\n\n# Warning: don't save your GitHub PAT in a file you might share with others;\n# look into `gitcreds::gitcreds_set()` or `usethis::edit_r_environ()` instead.\nSys.setenv(\"GITHUB_PAT\" = \"ghp_............\")\nrenv::restore() # downloads the appropriate package versions\n\n\nTo use this set of versions: launch R inside the delphi-tooling-book directory.\n\n\n\n\nOther issues\nPlease let us know! You can file an issue with the book here, or with one of the individual packages at their own issue pages: epidatr, epiprocess, epipredict."
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "Documentation",
    "text": "Documentation\nYou can view the complete documentation for these packages at\n\nhttps://cmu-delphi.github.io/epipredict,\nhttps://cmu-delphi.github.io/epiprocess,\nhttps://cmu-delphi.github.io/epidatasets,\nhttps://cmu-delphi.github.io/epidatr."
  },
  {
    "objectID": "index.html#attribution",
    "href": "index.html#attribution",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "Attribution",
    "text": "Attribution\nThis document contains a number of datasets that are a modified part of the COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University as republished in the COVIDcast Epidata API. These data are licensed under the terms of the Creative Commons Attribution 4.0 International license by the Johns Hopkins University on behalf of its Center for Systems Science in Engineering. Copyright Johns Hopkins University 2020.\nFrom the COVIDcast Epidata API: These signals are taken directly from the JHU CSSE COVID-19 GitHub repository without changes."
  },
  {
    "objectID": "index.html#quick-start-example",
    "href": "index.html#quick-start-example",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "Quick-start example",
    "text": "Quick-start example\nThese packages come with some built-in historical data for illustration, but up-to-date versions could be downloaded with the {epidatr} or {covidcast} packages and processed using {epiprocess}.1\n\nlibrary(epipredict)\njhu &lt;- case_death_rate_subset\njhu\n\n#&gt; An `epi_df` object, 20,496 x 4 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25.791826\n#&gt; \n#&gt; # A tibble: 20,496 × 4\n#&gt;   geo_value time_value case_rate death_rate\n#&gt; * &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 ak        2020-12-31      35.9      0.158\n#&gt; 2 al        2020-12-31      65.1      0.438\n#&gt; 3 ar        2020-12-31      66.0      1.27 \n#&gt; 4 as        2020-12-31       0        0    \n#&gt; 5 az        2020-12-31      76.8      1.10 \n#&gt; 6 ca        2020-12-31      96.0      0.751\n#&gt; # ℹ 20,490 more rows\n\n\nTo create and train a simple auto-regressive forecaster to predict the death rate two weeks into the future using past (lagged) deaths and cases, we could use the following function.\n\ntwo_week_ahead &lt;- arx_forecaster(\n  jhu,\n  outcome = \"death_rate\",\n  predictors = c(\"case_rate\", \"death_rate\"),\n  args_list = arx_args_list(\n    lags = list(case_rate = c(0, 1, 2, 3, 7, 14), death_rate = c(0, 7, 14)),\n    ahead = 14\n  )\n)\n\nIn this case, we have used a number of different lags for the case rate, while only using 3 weekly lags for the death rate (as predictors). The result is both a fitted model object which could be used any time in the future to create different forecasts, as well as a set of predicted values (and prediction intervals) for each location 14 days after the last available time value in the data.\n\ntwo_week_ahead$epi_workflow\n\n#&gt; ══ Epi Workflow [trained] ═══════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; Postprocessor: Frosting\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 6 Recipe Steps\n#&gt; \n#&gt; • step_epi_lag()\n#&gt; • step_epi_lag()\n#&gt; • step_epi_ahead()\n#&gt; • step_naomit()\n#&gt; • step_naomit()\n#&gt; • step_training_window()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)    lag_0_case_rate    lag_1_case_rate    lag_2_case_rate  \n#&gt;        -0.0073358          0.0030365          0.0012467          0.0009536  \n#&gt;   lag_3_case_rate    lag_7_case_rate   lag_14_case_rate   lag_0_death_rate  \n#&gt;         0.0011425          0.0012481          0.0003041          0.1351769  \n#&gt;  lag_7_death_rate  lag_14_death_rate  \n#&gt;         0.1471127          0.1062473  \n#&gt; \n#&gt; ── Postprocessor ────────────────────────────────────────────────────────────\n#&gt; 5 Frosting Layers\n#&gt; \n#&gt; • layer_predict()\n#&gt; • layer_residual_quantiles()\n#&gt; • layer_add_forecast_date()\n#&gt; • layer_add_target_date()\n#&gt; • layer_threshold()\n\n\nThe fitted model here involved preprocessing the data to appropriately generate lagged predictors, estimating a linear model with stats::lm() and then postprocessing the results to be meaningful for epidemiological tasks. We can also examine the predictions.\n\ntwo_week_ahead$predictions\n\n#&gt; # A tibble: 56 × 5\n#&gt;   geo_value .pred         .pred_distn forecast_date target_date\n#&gt;   &lt;chr&gt;     &lt;dbl&gt;              &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n#&gt; 1 ak        0.449 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-14 \n#&gt; 2 al        0.574 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-14 \n#&gt; 3 ar        0.673 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-14 \n#&gt; 4 as        0     [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-14 \n#&gt; 5 az        0.679 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-14 \n#&gt; 6 ca        0.575 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-14 \n#&gt; # ℹ 50 more rows\n\n\nThe results above show a distributional forecast produced using data through the end of 2021 for the 14th of January 2022. A prediction for the death rate per 100K inhabitants is available for every state (geo_value) along with a 90% predictive interval. The figure below displays the forecast for a small handful of states. The vertical black line is the forecast date. The forecast doesn’t appear to be particularly good, but our choices above were intended to be illustrative of the functionality rather than optimized for accuracy.\n\n\nCode\nsamp_geos &lt;- c(\"ca\", \"co\", \"ny\", \"pa\")\n\nhist &lt;- jhu %&gt;%\n  filter(\n    geo_value %in% samp_geos,\n    time_value &gt;= max(time_value) - 90L\n  )\npreds &lt;- two_week_ahead$predictions %&gt;%\n  filter(geo_value %in% samp_geos) %&gt;%\n  mutate(q = nested_quantiles(.pred_distn)) %&gt;%\n  unnest(q) %&gt;%\n  pivot_wider(names_from = tau, values_from = q)\n\nggplot(hist, aes(color = geo_value)) +\n  geom_line(aes(time_value, death_rate)) +\n  theme_bw() +\n  geom_errorbar(data = preds, aes(x = target_date, ymin = `0.05`, ymax = `0.95`)) +\n  geom_point(data = preds, aes(target_date, .pred)) +\n  geom_vline(data = preds, aes(xintercept = forecast_date)) +\n  scale_colour_viridis_d(name = \"\") +\n  scale_x_date(date_labels = \"%b %Y\") +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"\", y = \"Incident deaths per 100K\\n inhabitants\")"
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "Contents",
    "text": "Contents\nThe remainder of this book examines this software in more detail, illustrating some of the flexibility that is available.\n\n\n\nSession Information.\n\nSee also Section 1.\n\nsessioninfo::session_info()\n\n#&gt; ─ Session info ────────────────────────────────────────────────────────────\n#&gt;  setting  value\n#&gt;  version  R version 4.3.0 (2023-04-21)\n#&gt;  os       macOS Ventura 13.4\n#&gt;  system   aarch64, darwin20\n#&gt;  ui       X11\n#&gt;  language (EN)\n#&gt;  collate  en_US.UTF-8\n#&gt;  ctype    en_US.UTF-8\n#&gt;  tz       America/Vancouver\n#&gt;  date     2023-06-21\n#&gt;  pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n#&gt; \n#&gt; ─ Packages ────────────────────────────────────────────────────────────────\n#&gt;  ! package        * version    date (UTC) lib source\n#&gt;  P anytime          0.3.9      2020-08-27 [?] CRAN (R 4.3.0)\n#&gt;  P backports        1.4.1      2021-12-13 [?] CRAN (R 4.3.0)\n#&gt;  P checkmate        2.2.0      2023-04-27 [?] CRAN (R 4.3.0)\n#&gt;  P class            7.3-22     2023-05-03 [?] CRAN (R 4.3.0)\n#&gt;  P cli              3.6.1      2023-03-23 [?] CRAN (R 4.3.0)\n#&gt;  P codetools        0.2-19     2023-02-01 [?] CRAN (R 4.3.0)\n#&gt;  P colorspace       2.1-0      2023-01-23 [?] CRAN (R 4.3.0)\n#&gt;  P crayon           1.5.2      2022-09-29 [?] CRAN (R 4.3.0)\n#&gt;  P data.table       1.14.8     2023-02-17 [?] CRAN (R 4.3.0)\n#&gt;  P digest           0.6.31     2022-12-11 [?] CRAN (R 4.3.0)\n#&gt;  P distributional   0.3.2      2023-03-22 [?] CRAN (R 4.3.0)\n#&gt;  P dplyr          * 1.1.2      2023-04-20 [?] CRAN (R 4.3.0)\n#&gt;  P ellipsis         0.3.2      2021-04-29 [?] CRAN (R 4.3.0)\n#&gt;  P epidatasets    * 0.0.1      2023-06-17 [?] Github (cmu-delphi/epidatasets@cc8f2a0)\n#&gt;  P epidatr        * 0.6.0      2023-06-16 [?] Github (cmu-delphi/epidatr@46d2d54)\n#&gt;  P epipredict     * 0.0.5      2023-06-17 [?] Github (cmu-delphi/epipredict@206f0ef)\n#&gt;  P epiprocess     * 0.6.0.9999 2023-06-16 [?] Github (cmu-delphi/epiprocess@572f6e6)\n#&gt;  P evaluate         0.21       2023-05-05 [?] CRAN (R 4.3.0)\n#&gt;  P fansi            1.0.4      2023-01-22 [?] CRAN (R 4.3.0)\n#&gt;  P farver           2.1.1      2022-07-06 [?] CRAN (R 4.3.0)\n#&gt;  P fastmap          1.1.1      2023-02-24 [?] CRAN (R 4.3.0)\n#&gt;  P forcats        * 1.0.0      2023-01-29 [?] CRAN (R 4.3.0)\n#&gt;  P fs               1.6.2      2023-04-25 [?] CRAN (R 4.3.0)\n#&gt;  P future           1.32.0     2023-03-07 [?] CRAN (R 4.3.0)\n#&gt;  P future.apply     1.11.0     2023-05-21 [?] CRAN (R 4.3.0)\n#&gt;  P generics         0.1.3      2022-07-05 [?] CRAN (R 4.3.0)\n#&gt;  P ggplot2        * 3.4.2      2023-04-03 [?] CRAN (R 4.3.0)\n#&gt;  P globals          0.16.2     2022-11-21 [?] CRAN (R 4.3.0)\n#&gt;  P glue             1.6.2      2022-02-24 [?] CRAN (R 4.3.0)\n#&gt;  P gower            1.0.1      2022-12-22 [?] CRAN (R 4.3.0)\n#&gt;  P gtable           0.3.3      2023-03-21 [?] CRAN (R 4.3.0)\n#&gt;  P hardhat          1.3.0      2023-03-30 [?] CRAN (R 4.3.0)\n#&gt;  P hms              1.1.3      2023-03-21 [?] CRAN (R 4.3.0)\n#&gt;  P htmltools        0.5.5      2023-03-23 [?] CRAN (R 4.3.0)\n#&gt;  P httr             1.4.6      2023-05-08 [?] CRAN (R 4.3.0)\n#&gt;  P ipred            0.9-14     2023-03-09 [?] CRAN (R 4.3.0)\n#&gt;  P jsonlite         1.8.5      2023-06-05 [?] CRAN (R 4.3.0)\n#&gt;  P knitr            1.43       2023-05-25 [?] CRAN (R 4.3.0)\n#&gt;  P labeling         0.4.2      2020-10-20 [?] CRAN (R 4.3.0)\n#&gt;  P lattice          0.21-8     2023-04-05 [?] CRAN (R 4.3.0)\n#&gt;  P lava             1.7.2.1    2023-02-27 [?] CRAN (R 4.3.0)\n#&gt;  P lifecycle        1.0.3      2022-10-07 [?] CRAN (R 4.3.0)\n#&gt;  P listenv          0.9.0      2022-12-16 [?] CRAN (R 4.3.0)\n#&gt;  P lubridate      * 1.9.2      2023-02-10 [?] CRAN (R 4.3.0)\n#&gt;  P magrittr         2.0.3      2022-03-30 [?] CRAN (R 4.3.0)\n#&gt;  P MASS             7.3-60     2023-05-04 [?] CRAN (R 4.3.0)\n#&gt;  P Matrix           1.5-4      2023-04-04 [?] CRAN (R 4.3.0)\n#&gt;  P MatrixModels     0.5-1      2022-09-11 [?] CRAN (R 4.3.0)\n#&gt;  P MMWRweek         0.1.3      2020-04-22 [?] CRAN (R 4.3.0)\n#&gt;  P munsell          0.5.0      2018-06-12 [?] CRAN (R 4.3.0)\n#&gt;  P nnet             7.3-19     2023-05-03 [?] CRAN (R 4.3.0)\n#&gt;  P parallelly       1.36.0     2023-05-26 [?] CRAN (R 4.3.0)\n#&gt;  P parsnip        * 1.1.0      2023-04-12 [?] CRAN (R 4.3.0)\n#&gt;  P pillar           1.9.0      2023-03-22 [?] CRAN (R 4.3.0)\n#&gt;  P pkgconfig        2.0.3      2019-09-22 [?] CRAN (R 4.3.0)\n#&gt;  P prodlim          2023.03.31 2023-04-02 [?] CRAN (R 4.3.0)\n#&gt;  P purrr          * 1.0.1      2023-01-10 [?] CRAN (R 4.3.0)\n#&gt;  P quantreg         5.95       2023-04-08 [?] CRAN (R 4.3.0)\n#&gt;  P R.cache          0.16.0     2022-07-21 [?] CRAN (R 4.3.0)\n#&gt;  P R.methodsS3      1.8.2      2022-06-13 [?] CRAN (R 4.3.0)\n#&gt;  P R.oo             1.25.0     2022-06-12 [?] CRAN (R 4.3.0)\n#&gt;  P R.utils          2.12.2     2022-11-11 [?] CRAN (R 4.3.0)\n#&gt;  P R6               2.5.1      2021-08-19 [?] CRAN (R 4.3.0)\n#&gt;  P Rcpp             1.0.10     2023-01-22 [?] CRAN (R 4.3.0)\n#&gt;  P readr          * 2.1.4      2023-02-10 [?] CRAN (R 4.3.0)\n#&gt;  P recipes          1.0.6      2023-04-25 [?] CRAN (R 4.3.0)\n#&gt;    renv             0.17.3     2023-04-06 [1] CRAN (R 4.2.2)\n#&gt;  P rlang            1.1.1      2023-04-28 [?] CRAN (R 4.3.0)\n#&gt;  P rmarkdown        2.22       2023-06-01 [?] CRAN (R 4.3.0)\n#&gt;  P rpart            4.1.19     2022-10-21 [?] CRAN (R 4.3.0)\n#&gt;  P rstudioapi       0.14       2022-08-22 [?] CRAN (R 4.3.0)\n#&gt;  P scales           1.2.1      2022-08-20 [?] CRAN (R 4.3.0)\n#&gt;  P sessioninfo      1.2.2      2021-12-06 [?] CRAN (R 4.3.0)\n#&gt;  P SparseM          1.81       2021-02-18 [?] CRAN (R 4.3.0)\n#&gt;  P stringi          1.7.12     2023-01-11 [?] CRAN (R 4.3.0)\n#&gt;  P stringr        * 1.5.0      2022-12-02 [?] CRAN (R 4.3.0)\n#&gt;  P styler           1.10.1     2023-06-05 [?] CRAN (R 4.3.0)\n#&gt;  P survival         3.5-5      2023-03-12 [?] CRAN (R 4.3.0)\n#&gt;  P tibble         * 3.2.1      2023-03-20 [?] CRAN (R 4.3.0)\n#&gt;  P tidyr          * 1.3.0      2023-01-24 [?] CRAN (R 4.3.0)\n#&gt;  P tidyselect       1.2.0      2022-10-10 [?] CRAN (R 4.3.0)\n#&gt;  P tidyverse      * 2.0.0      2023-02-22 [?] CRAN (R 4.3.0)\n#&gt;  P timechange       0.2.0      2023-01-11 [?] CRAN (R 4.3.0)\n#&gt;  P timeDate         4022.108   2023-01-07 [?] CRAN (R 4.3.0)\n#&gt;  P tsibble          1.1.3      2022-10-09 [?] CRAN (R 4.3.0)\n#&gt;  P tzdb             0.4.0      2023-05-12 [?] CRAN (R 4.3.0)\n#&gt;  P usethis          2.2.0      2023-06-06 [?] CRAN (R 4.3.0)\n#&gt;  P utf8             1.2.3      2023-01-31 [?] CRAN (R 4.3.0)\n#&gt;  P vctrs            0.6.2      2023-04-19 [?] CRAN (R 4.3.0)\n#&gt;  P viridisLite      0.4.2      2023-05-02 [?] CRAN (R 4.3.0)\n#&gt;  P withr            2.5.0      2022-03-03 [?] CRAN (R 4.3.0)\n#&gt;  P workflows        1.1.3      2023-02-22 [?] CRAN (R 4.3.0)\n#&gt;  P xfun             0.39       2023-04-20 [?] CRAN (R 4.3.0)\n#&gt;  P xml2             1.3.4      2023-04-27 [?] CRAN (R 4.3.0)\n#&gt;  P yaml             2.3.7      2023-01-23 [?] CRAN (R 4.3.0)\n#&gt; \n#&gt;  [1] /Users/dajmcdon/Library/Caches/org.R-project.R/R/renv/library/delphi-tooling-book-d37e2426/R-4.3/aarch64-apple-darwin20\n#&gt;  [2] /Users/dajmcdon/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/aarch64-apple-darwin20/84ba8b13\n#&gt; \n#&gt;  P ── Loaded and on-disk path mismatch.\n#&gt; \n#&gt; ───────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\nBien, Jacob, Logan Brooks, David Farrow, Pedrito Maynard-Zhang, Alex Reinhart, Ryan Tibshirani, and Samuel Gratzl. 2023. Epidatr: Client for Delphi’s Epidata API. https://github.com/cmu-delphi/epidatr.\n\n\nBrooks, Logan, Daniel McDonald, Evan Ray, and Ryan Tibshirani. 2023. Epiprocess: Tools for Basic Signal Processing in Epidemiology. https://cmu-delphi.github.io/epiprocess/.\n\n\nCramer, Estee Y., Evan L. Ray, Velma K. Lopez, Johannes Bracher, Andrea Brennen, Alvaro J. Castro Rivadeneira, Aaron Gerding, et al. 2022. “Evaluation of Individual and Ensemble Probabilistic Forecasts of COVID-19 Mortality in the United States.” Proceedings of the National Academy of Sciences 119 (15): e2113561119. https://doi.org/10.1073/pnas.2113561119.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.\n\n\nKuhn, Max, and Davis Vaughan. 2023. Parsnip: A Common API to Modeling and Analysis Functions. https://CRAN.R-project.org/package=parsnip.\n\n\nLutz, Chelsea S, Mimi P Huynh, Monica Schroeder, Sophia Anyatonwu, F Scott Dahlgren, Gregory Danyluk, Danielle Fernandez, et al. 2019. “Applying Infectious Disease Forecasting to Public Health: A Path Forward Using Influenza Forecasting Examples.” BMC Public Health 19 (1): 1–12. https://doi.org/10.1186/s12889-019-7966-8.\n\n\nMcDonald, Daniel J. 2023. Epidatasets: Epidemiological Data for Delphi Tooling Examples. https://cmu-delphi.github.io/epidatasets/.\n\n\nMcDonald, Daniel J, Jacob Bien, Alden Green, Addison J Hu, Nat DeFries, Sangwon Hyun, Natalia L Oliveira, et al. 2021. “Can Auxiliary Indicators Improve COVID-19 Forecasting and Hotspot Prediction?” Proceedings of the National Academy of Sciences 118: e2111453118. https://doi.org/10.1073/pnas.2111453118.\n\n\nMcDonald, Daniel, Ryan Tibshirani, Logan Brooks, Rachel Lobay, Maggie Liu, Ken Mawer, and Chloe You. 2023. Epipredict: Basic Epidemiology Forecasting Methods.\n\n\nMüller, Kirill, and Hadley Wickham. 2023. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nReinhart, Alex, Logan Brooks, Maria Jahja, Aaron Rumack, Jingjing Tang, Wael Al Saeed, Taylor Arnold, et al. 2021. “An Open Repository of Real-Time COVID-19 Indicators.” Proceedings of the National Academy of Sciences 118: e2111452118. https://doi.org/10.1073/pnas.2111452118.\n\n\nSpinu, Vitalie, Garrett Grolemund, and Hadley Wickham. 2023. Lubridate: Make Dealing with Dates a Little Easier. https://CRAN.R-project.org/package=lubridate.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2022. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2023a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2023b. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2023. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Lionel Henry. 2023. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "",
    "text": "COVIDcast data and other epidemiological signals for non-Covid related illnesses are available with {epidatr}, which interfaces directly to Delphi’s Epidata API.↩︎"
  },
  {
    "objectID": "why-this-package.html#a-brief-overview-of-the-epi-packages",
    "href": "why-this-package.html#a-brief-overview-of-the-epi-packages",
    "title": "Motivation",
    "section": "A brief overview of the epi* packages",
    "text": "A brief overview of the epi* packages\nSince early 2020, the CMU’s Delphi Research Group has worked with data partners to publicly disseminate information about numerous COVID-19 indicators, which we and others then use for nowcasting and short-term forecasting. Prior to that we worked mostly on influenza, dengue and norovirus modelling.\nThe overarching goal of the effort described here is to describe a set of interworking, community-driven packages built specifically for epidemiologic tracking and forcasting. To that end, the epi. universe is currently comprised of three main packages: {epidatr} (in Python {epidatpy}), {epiprocess}, and {epipredict}. Each has their own role in the forecasting process: The {epidatr} package is used to fetch data from the API, {epiprocess} is used to standardize, clean and process data, and finally the {epipredict} package is used for building (and to a lesser degree, evaluating) predictive models. Here is a diagram depicting the basic workflow for using these packages:\n\nHere’s a brief overview about what each package offers…\nepidatr - Obtain epidemiological surveillance data from Delphi’s Epidata API\n\nQuick access to data on the spread and impact of the COVID-19 pandemic\nAccess to data about other diseases, including influenza, dengue, and more\nProvide geographic and temporal detail\nTracked through several data streams\n\nepiprocess - Basic processing operations and data structures\n\nCalculate rolling statistics\nFill / impute gaps\nExamine correlations\nStore revision history smartly\nInspect revision patterns\nDetect / correct outliers\n\nepipredict - A forecasting framework\n\nFlatline forecaster\nAR-type models\nBacktest using the versioned data\nEasily create features\nQuickly pivot to new tasks\nHighly customizable for advanced users\n\nThe focus of this book is on {epiprocess} and {epipredict}. By the end of this book, you should be equipped with the tools and knowledge to build your own basic and customizable forecasting models. At that point, our hope is that some may take things a step further and leverage this package to build on it and advance the frontier of predictive modelling.\n\n\n\n\nCramer, Estee Y., Evan L. Ray, Velma K. Lopez, Johannes Bracher, Andrea Brennen, Alvaro J. Castro Rivadeneira, Aaron Gerding, et al. 2022. “Evaluation of Individual and Ensemble Probabilistic Forecasts of COVID-19 Mortality in the United States.” Proceedings of the National Academy of Sciences 119 (15): e2113561119. https://doi.org/10.1073/pnas.2113561119.\n\n\nLutz, Chelsea S, Mimi P Huynh, Monica Schroeder, Sophia Anyatonwu, F Scott Dahlgren, Gregory Danyluk, Danielle Fernandez, et al. 2019. “Applying Infectious Disease Forecasting to Public Health: A Path Forward Using Influenza Forecasting Examples.” BMC Public Health 19 (1): 1–12. https://doi.org/10.1186/s12889-019-7966-8.\n\n\nMcDonald, Daniel J, Jacob Bien, Alden Green, Addison J Hu, Nat DeFries, Sangwon Hyun, Natalia L Oliveira, et al. 2021. “Can Auxiliary Indicators Improve COVID-19 Forecasting and Hotspot Prediction?” Proceedings of the National Academy of Sciences 118: e2111453118. https://doi.org/10.1073/pnas.2111453118."
  },
  {
    "objectID": "epiprocess.html#epi_df-snapshot-of-a-data-set",
    "href": "epiprocess.html#epi_df-snapshot-of-a-data-set",
    "title": "1  Overview",
    "section": "1.1 epi_df: snapshot of a data set",
    "text": "1.1 epi_df: snapshot of a data set\nThe first main data structure in the epiprocess package is called [epi_df]. This is simply a tibble with a couple of required columns, geo_value and time_value. It can have any other number of columns, which can be seen as measured variables, which we also call signal variables. In brief, an epi_df object represents a snapshot of a data set that contains the most up-to-date values of the signals variables, as of a given time.\nBy convention, functions in the epiprocess package that operate on epi_df objects begin with epi. For example:\n\nepi_slide(), for iteratively applying a custom computation to a variable in an epi_df object over sliding windows in time;\nepi_cor(), for computing lagged correlations between variables in an epi_df object, (allowing for grouping by geo value, time value, or any other variables).\n\nFunctions in the package that operate directly on given variables do not begin with epi. For example:\n\ngrowth_rate(), for estimating the growth rate of a given signal at given time values, using various methodologies;\ndetect_outlr(), for detecting outliers in a given signal over time, using either built-in or custom methodologies."
  },
  {
    "objectID": "epiprocess.html#epi_archive-full-version-history-of-a-data-set",
    "href": "epiprocess.html#epi_archive-full-version-history-of-a-data-set",
    "title": "1  Overview",
    "section": "1.2 epi_archive: full version history of a data set",
    "text": "1.2 epi_archive: full version history of a data set\nThe second main data structure in the package is called [epi_archive]. This is a special class (R6 format) wrapped around a data table that stores the archive (version history) of some signal variables of interest.\nBy convention, functions in the epiprocess package that operate on epi_archive objects begin with epix (the “x” is meant to remind you of “archive”). These are just wrapper functions around the public methods for the epi_archive R6 class. For example:\n\nepix_as_of(), for generating a snapshot in epi_df format from the data archive, which represents the most up-to-date values of the signal variables, as of the specified version;\nepix_fill_through_version(), for filling in some fake version data following simple rules, for use when downstream methods expect an archive that is more up-to-date (e.g., if it is a forecasting deadline date and one of our data sources cannot be accessed to provide the latest versions of its data)\nepix_merge(), for merging two data archives with each other, with support for various approaches to handling when one of the archives is more up-to-date version-wise than the other;\nepix_slide(), for sliding a custom computation to a data archive over local windows in time, much like epi_slide for an epi_df object, but with one key difference: the sliding computation at any given reference time \\(t\\) is performed only on the data that would have been available as of \\(t\\)."
  },
  {
    "objectID": "epidf.html#some-details-on-metadata",
    "href": "epidf.html#some-details-on-metadata",
    "title": "2  Getting data into epi_df format",
    "section": "2.1 Some details on metadata",
    "text": "2.1 Some details on metadata\nIn general, an epi_df object has the following fields in its metadata:\n\ngeo_type: the type for the geo values.\ntime_type: the type for the time values.\nas_of: the time value at which the given data were available.\n\nMetadata for an epi_df object x can be accessed (and altered) via attributes(x)$metadata. The first two fields here, geo_type and time_type, are not currently used by any downstream functions in the epiprocess package, and serve only as useful bits of information to convey about the data set at hand. The last field here, as_of, is one of the most unique aspects of an epi_df object.\nIn brief, we can think of an epi_df object as a single snapshot of a data set that contains the most up-to-date values of some signals of interest, as of the time specified as_of. For example, if as_of is January 31, 2022, then the epi_df object has the most up-to-date version of the data available as of January 31, 2022. The epiprocess package also provides a companion data structure called epi_archive, which stores the full version history of a given data set. See the archive vignette for more.\nIf any of the geo_type, time_type, or as_of arguments are missing in a call to as_epi_df(), then this function will try to infer them from the passed object. Usually, geo_type and time_type can be inferred from the geo_value and time_value columns, respectively, but inferring the as_of field is not as easy. See the documentation for as_epi_df() more details.\n\nx &lt;- as_epi_df(cases) %&gt;%\n  select(geo_value, time_value, total_cases = value)\n\nattributes(x)$metadata\n\n#&gt; $geo_type\n#&gt; [1] \"state\"\n#&gt; \n#&gt; $time_type\n#&gt; [1] \"day\"\n#&gt; \n#&gt; $as_of\n#&gt; [1] \"2023-03-10\""
  },
  {
    "objectID": "epidf.html#sec-additional-keys",
    "href": "epidf.html#sec-additional-keys",
    "title": "2  Getting data into epi_df format",
    "section": "2.2 Using additional key columns in epi_df",
    "text": "2.2 Using additional key columns in epi_df\nIn the following examples we will show how to create an epi_df with additional keys.\n\n2.2.1 Converting a tsibble that has county code as an extra key\n\nset.seed(12345)\nex1 &lt;- tibble(\n  geo_value = rep(c(\"ca\", \"fl\", \"pa\"), each = 3),\n  county_code = c(\n    \"06059\", \"06061\", \"06067\", \"12111\", \"12113\", \"12117\",\n    \"42101\", \"42103\", \"42105\"\n  ),\n  time_value = rep(\n    seq(as.Date(\"2020-06-01\"), as.Date(\"2020-06-03\"), by = \"1 day\"),\n    length.out = 9\n  ),\n  value = rpois(9, 5)\n) %&gt;%\n  as_tsibble(index = time_value, key = c(geo_value, county_code))\n\nex1 &lt;- as_epi_df(x = ex1, geo_type = \"state\", time_type = \"day\", as_of = \"2020-06-03\")\n\nThe metadata now includes county_code as an extra key.\n\nattr(ex1, \"metadata\")\n\n#&gt; $geo_type\n#&gt; [1] \"state\"\n#&gt; \n#&gt; $time_type\n#&gt; [1] \"day\"\n#&gt; \n#&gt; $as_of\n#&gt; [1] \"2020-06-03\"\n#&gt; \n#&gt; $other_keys\n#&gt; [1] \"county_code\"\n\n\n\n\n2.2.2 Dealing with misspecified column names\nepi_df requires there to be columns geo_value and time_value, if they do not exist then as_epi_df() throws an error.\n\nex2 &lt;- data.frame(\n  state = rep(c(\"ca\", \"fl\", \"pa\"), each = 3), # misnamed\n  pol = rep(c(\"blue\", \"swing\", \"swing\"), each = 3), # extra key\n  reported_date = rep(\n    seq(as.Date(\"2020-06-01\"), as.Date(\"2020-06-03\"), by = \"day\"),\n    length.out = 9\n  ), # misnamed\n  value = rpois(9, 5)\n)\nex2 %&gt;% as_epi_df()\n\n#&gt; Error in `Abort()`:\n#&gt; ! `x` must contain a `geo_value` column.\n\n\nThe columns should be renamed to match epi_df format.\n\nex2 &lt;- ex2 %&gt;%\n  rename(geo_value = state, time_value = reported_date) %&gt;%\n  as_epi_df(\n    geo_type = \"state\",\n    as_of = \"2020-06-03\",\n    additional_metadata = list(other_keys = \"pol\")\n  )\n\nattr(ex2, \"metadata\")\n\n#&gt; $geo_type\n#&gt; [1] \"state\"\n#&gt; \n#&gt; $time_type\n#&gt; [1] \"day\"\n#&gt; \n#&gt; $as_of\n#&gt; [1] \"2020-06-03\"\n#&gt; \n#&gt; $other_keys\n#&gt; [1] \"pol\"\n\n\n\n\n2.2.3 Adding additional keys to an epi_df object\nIn the above examples, all the keys are added to objects prior to conversion to epi_df objects. But this can also be accomplished afterward. We’ll look at an included dataset and filter to a single state for simplicity.\n\nex3 &lt;- jhu_csse_county_level_subset %&gt;%\n  filter(time_value &gt; \"2021-12-01\", state_name == \"Massachusetts\") %&gt;%\n  slice_tail(n = 6)\n\nattr(ex3, \"metadata\") # geo_type is county currently\n\n#&gt; $geo_type\n#&gt; [1] \"county\"\n#&gt; \n#&gt; $time_type\n#&gt; [1] \"day\"\n#&gt; \n#&gt; $as_of\n#&gt; [1] \"2022-05-23 14:35:45 PDT\"\n\n\nNow we add state (MA) and pol as new columns to the data and as new keys to the metadata. The “state” geo_type anticipates lower-case abbreviations, so we’ll match that.\n\nex3 &lt;- ex3 %&gt;%\n  as_tibble() %&gt;% # drop the `epi_df` class before adding additional metadata\n  mutate(\n    state = rep(tolower(\"MA\"), 6),\n    pol = rep(c(\"blue\", \"swing\", \"swing\"), each = 2)\n  ) %&gt;%\n  as_epi_df(additional_metadata = list(other_keys = c(\"state\", \"pol\")))\n\nattr(ex3, \"metadata\")\n\n#&gt; $geo_type\n#&gt; [1] \"county\"\n#&gt; \n#&gt; $time_type\n#&gt; [1] \"day\"\n#&gt; \n#&gt; $as_of\n#&gt; [1] \"2023-06-19 20:29:48 PDT\"\n#&gt; \n#&gt; $other_keys\n#&gt; [1] \"state\" \"pol\"\n\n\nNote that the two additional keys we added, state and pol, are specified as a character vector in the other_keys component of the additional_metadata list. They must be specified in this manner so that downstream actions on the epi_df, like model fitting and prediction, can recognize and use these keys."
  },
  {
    "objectID": "epidf.html#working-with-epi_df-objects-downstream",
    "href": "epidf.html#working-with-epi_df-objects-downstream",
    "title": "2  Getting data into epi_df format",
    "section": "2.3 Working with epi_df objects downstream",
    "text": "2.3 Working with epi_df objects downstream\nData in epi_df format should be easy to work with downstream, since it is a very standard tabular data format; in the other vignettes, we’ll walk through some basic signal processing tasks using functions provided in the epiprocess package. Of course, we can also write custom code for other downstream uses, like plotting, which is pretty easy to do ggplot2.\n\nggplot(x, aes(x = time_value, y = total_cases, color = geo_value)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(x = \"Date\", y = \"Cumulative COVID-19 cases\", color = \"State\")\n\n\n\n\n\n\n\n\nFinally, we’ll examine some data from other packages just to show how we might get them into epi_df format. The first is data on daily new (not cumulative) SARS cases in Canada in 2003, from the outbreaks package. New cases are broken into a few categories by provenance.\n\nx &lt;- outbreaks::sars_canada_2003 %&gt;%\n  mutate(geo_value = \"ca\") %&gt;%\n  select(geo_value, time_value = date, starts_with(\"cases\")) %&gt;%\n  as_epi_df(geo_type = \"nation\")\n\nhead(x)\n\n#&gt; An `epi_df` object, 6 x 6 with metadata:\n#&gt; * geo_type  = nation\n#&gt; * time_type = day\n#&gt; * as_of     = 2023-06-19 20:29:48.463959\n#&gt; \n#&gt; # A tibble: 6 × 6\n#&gt;   geo_value time_value cases_travel cases_household cases_healthcare\n#&gt; * &lt;chr&gt;     &lt;date&gt;            &lt;int&gt;           &lt;int&gt;            &lt;int&gt;\n#&gt; 1 ca        2003-02-23            1               0                0\n#&gt; 2 ca        2003-02-24            0               0                0\n#&gt; 3 ca        2003-02-25            0               0                0\n#&gt; 4 ca        2003-02-26            0               1                0\n#&gt; 5 ca        2003-02-27            0               0                0\n#&gt; 6 ca        2003-02-28            1               0                0\n#&gt; # ℹ 1 more variable: cases_other &lt;int&gt;\n\n\n\n\nCode\nx &lt;- x %&gt;%\n  pivot_longer(starts_with(\"cases\"), names_to = \"type\") %&gt;%\n  mutate(type = substring(type, 7))\n\nggplot(x, aes(x = time_value, y = value)) +\n  geom_col(aes(fill = type), just = 0.5) +\n  scale_y_continuous(breaks = 0:4 * 2, expand = expansion(c(0, 0.05))) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(x = \"Date\", y = \"SARS cases in Canada\", fill = \"Type\")\n\n\n\n\n\n\n\n\n\nThis next example examines data on new cases of Ebola in Sierra Leone in 2014 (from the same package).\n\nx &lt;- outbreaks::ebola_sierraleone_2014 %&gt;%\n  mutate(\n    cases = ifelse(status == \"confirmed\", 1, 0),\n    province = case_when(\n      district %in% c(\"Kailahun\", \"Kenema\", \"Kono\") ~ \"Eastern\",\n      district %in% c(\n        \"Bombali\", \"Kambia\", \"Koinadugu\",\n        \"Port Loko\", \"Tonkolili\"\n      ) ~ \"Northern\",\n      district %in% c(\"Bo\", \"Bonthe\", \"Moyamba\", \"Pujehun\") ~ \"Sourthern\",\n      district %in% c(\"Western Rural\", \"Western Urban\") ~ \"Western\"\n    )\n  ) %&gt;%\n  select(geo_value = province, time_value = date_of_onset, cases) %&gt;%\n  filter(cases == 1) %&gt;%\n  group_by(geo_value, time_value) %&gt;%\n  summarise(cases = sum(cases)) %&gt;%\n  as_epi_df(geo_type = \"province\")\n\n\n\nCode\nggplot(x, aes(x = time_value, y = cases)) +\n  geom_col(aes(fill = geo_value), show.legend = FALSE) +\n  facet_wrap(~geo_value, scales = \"free_y\") +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(x = \"Date\", y = \"Confirmed cases of Ebola in Sierra Leone\")"
  },
  {
    "objectID": "slide.html#slide-with-a-formula",
    "href": "slide.html#slide-with-a-formula",
    "title": "3  Sliding computations",
    "section": "3.1 Slide with a formula",
    "text": "3.1 Slide with a formula\nWe first demonstrate how to apply a 7-day trailing average to the daily cases in order to smooth the signal, by passing in a formula for the first argument of epi_slide(). To do this computation per state, we first call group_by().\n\nx %&gt;%\n  group_by(geo_value) %&gt;%\n  epi_slide(~ mean(.x$cases), before = 6) %&gt;%\n  ungroup()\n\n#&gt; An `epi_df` object, 4,026 x 4 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-23 13:17:07.044666\n#&gt; \n#&gt; # A tibble: 4,026 × 4\n#&gt;   geo_value time_value cases slide_value\n#&gt; * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 ca        2020-03-01     6        6   \n#&gt; 2 ca        2020-03-02     4        5   \n#&gt; 3 ca        2020-03-03     6        5.33\n#&gt; 4 ca        2020-03-04    11        6.75\n#&gt; 5 ca        2020-03-05    10        7.4 \n#&gt; 6 ca        2020-03-06    18        9.17\n#&gt; # ℹ 4,020 more rows\n\n\nThe formula specified has access to all non-grouping columns present in the original epi_df object (and must refer to them with the prefix .x$). As we can see, the function epi_slide() returns an epi_df object with a new column appended that contains the results (from sliding), named slide_value as the default. We can of course change this post hoc, or we can instead specify a new name up front using the new_col_name argument:\n\nx %&gt;%\n  group_by(geo_value) %&gt;%\n  epi_slide(~ mean(.x$cases), before = 6, new_col_name = \"cases_7dav\") %&gt;%\n  ungroup()\n\n#&gt; An `epi_df` object, 4,026 x 4 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-23 13:17:07.044666\n#&gt; \n#&gt; # A tibble: 4,026 × 4\n#&gt;   geo_value time_value cases cases_7dav\n#&gt; * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 ca        2020-03-01     6       6   \n#&gt; 2 ca        2020-03-02     4       5   \n#&gt; 3 ca        2020-03-03     6       5.33\n#&gt; 4 ca        2020-03-04    11       6.75\n#&gt; 5 ca        2020-03-05    10       7.4 \n#&gt; 6 ca        2020-03-06    18       9.17\n#&gt; # ℹ 4,020 more rows\n\n\nSome other information is available in additional variables:\n\n.group_key is a one-row tibble containing the values of the grouping variables for the associated group\n.ref_time_value is the reference time value the time window was based on\n\nLike in group_modify(), there are alternative names for these variables as well: . can be used instead of .x, .y instead of .group_key, and .z instead of .ref_time_value."
  },
  {
    "objectID": "slide.html#slide-with-a-function",
    "href": "slide.html#slide-with-a-function",
    "title": "3  Sliding computations",
    "section": "3.2 Slide with a function",
    "text": "3.2 Slide with a function\nWe can also pass a function for the first argument in epi_slide(). In this case, the passed function must accept the following arguments:\nIn this case, the passed function f must accept the following arguments: a data frame with the same column names as the original object, minus any grouping variables, containing the time window data for one group-ref_time_value combination; followed by a one-row tibble containing the values of the grouping variables for the associated group; followed by the associated ref_time_value. It can accept additional arguments; epi_slide() will forward any ... args it receives to f.\nRecreating the last example of a 7-day trailing average:\n\nx %&gt;%\n  group_by(geo_value) %&gt;%\n  epi_slide(function(x, gk, rtv) mean(x$cases),\n    before = 6, new_col_name = \"cases_7dav\"\n  ) %&gt;%\n  ungroup()\n\n#&gt; An `epi_df` object, 4,026 x 4 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-23 13:17:07.044666\n#&gt; \n#&gt; # A tibble: 4,026 × 4\n#&gt;   geo_value time_value cases cases_7dav\n#&gt; * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 ca        2020-03-01     6       6   \n#&gt; 2 ca        2020-03-02     4       5   \n#&gt; 3 ca        2020-03-03     6       5.33\n#&gt; 4 ca        2020-03-04    11       6.75\n#&gt; 5 ca        2020-03-05    10       7.4 \n#&gt; 6 ca        2020-03-06    18       9.17\n#&gt; # ℹ 4,020 more rows"
  },
  {
    "objectID": "slide.html#slide-the-tidy-way",
    "href": "slide.html#slide-the-tidy-way",
    "title": "3  Sliding computations",
    "section": "3.3 Slide the tidy way",
    "text": "3.3 Slide the tidy way\nPerhaps the most convenient way to setup a computation in epi_slide() is to pass in an expression for tidy evaluation. In this case, we can simply define the name of the new column directly as part of the expression, setting it equal to a computation in which we can access any columns of x by name, just as we would in a call to dplyr::mutate(), or any of the dplyr verbs. For example:\n\nx &lt;- x %&gt;%\n  group_by(geo_value) %&gt;%\n  epi_slide(cases_7dav = mean(cases), before = 6) %&gt;%\n  ungroup()\n\nIn addition to referring to individual columns by name, you can refer to the time window data as an epi_df or tibble using .x. Similarly, the other arguments of the function format are available through the magic names .group_key and .ref_time_value, and the tidyverse “pronouns” .data and .env can also be used.\nAs a simple sanity check, we visualize the 7-day trailing averages computed on top of the original counts.\n\n\nCode\ncols &lt;- RColorBrewer::brewer.pal(7, \"Set1\")[-6]\nggplot(x, aes(x = time_value)) +\n  geom_col(aes(y = cases, fill = geo_value),\n    alpha = 0.5,\n    show.legend = FALSE\n  ) +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  geom_line(aes(y = cases_7dav, col = geo_value), show.legend = FALSE) +\n  scale_fill_manual(values = cols) +\n  scale_color_manual(values = cols) +\n  facet_wrap(~geo_value, scales = \"free_y\") +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(x = \"Date\", y = \"Reported COVID-19 cases\")\n\n\n\n\n\n\n\n\n\nAs we can see from the center top panel, it looks like Florida moved to weekly reporting of COVID-19 cases in summer of 2021, while California occasionally reported negative cases counts!"
  },
  {
    "objectID": "slide.html#sec-local-forecaster",
    "href": "slide.html#sec-local-forecaster",
    "title": "3  Sliding computations",
    "section": "3.4 Running a local forecaster",
    "text": "3.4 Running a local forecaster\nAs a more complex example, we preview some of the functionality of {epipredict} described in future chapters, and use a forecaster based on a local (in time) autoregression or “AR model”. AR models can be fit in numerous ways (using base R functions and various packages), but here we the arx_forecaster(), implemented in {epipredict} both provides a more advanced example of sliding a function over an epi_df object, and it allows us to be a bit more flexible in defining a probabilistic forecaster: one that outputs not just a point prediction, but a notion of uncertainty around this. In particular, our forecaster will output a point prediction along with an 90% uncertainty band, represented by a predictive quantiles at the 5% and 95% levels (lower and upper endpoints of the uncertainty band).\nThe function signature below, is a probabilistic AR forecaster. The lags argument indicates which lags to use in the model, and ahead indicates how far ahead in the future to make forecasts (both are encoded in terms of the units of the time_value column; so, days, in the working epi_df being considered in this vignette).\n\narx_forecaster &lt;- function(\n    epi_df, \n    outcome, # the outcome column name in `epi_df`\n    predictors, # a character vector, containing 1 or more predictors in `epi_df`\n    trainer = quantile_reg(), \n    args_list = arx_args_list(\n      lags = c(0, 7, 14), \n      ahead = 7,\n      levels = c(0.05, 0.95)\n    )\n)\n\nWe go ahead and slide this AR forecaster over the working epi_df of COVID-19 cases. Note that we actually model the cases_7dav column, to operate on the scale of smoothed COVID-19 cases. This is clearly equivalent, up to a constant, to modeling weekly sums of COVID-19 cases.\n\nfc_time_values &lt;- seq(\n  from = as.Date(\"2020-06-01\"),\n  to = as.Date(\"2021-12-01\"),\n  by = \"1 months\"\n)\n\nfcasts &lt;- epi_slide(\n  x,\n  ~ arx_forecaster(\n    epi_data = .x,\n    outcome = \"cases_7dav\",\n    predictors = \"cases_7dav\",\n    trainer = quantile_reg(),\n    args_list = arx_args_list(ahead = 7)\n  )$predictions,\n  before = 119,\n  ref_time_values = fc_time_values,\n  new_col_name = \"fc\"\n)\n\n# grab just the relevant columns, and make them easier to plot\nfcasts &lt;- fcasts %&gt;%\n  select(\n    geo_value, time_value, cases_7dav,\n    contains(\"_distn\"), fc_target_date\n  ) %&gt;%\n  pivot_quantiles(contains(\"_distn\"))\nfcasts\n\n#&gt; # A tibble: 114 × 7\n#&gt;   geo_value time_value cases_7dav fc_target_date `0.05` `0.5` `0.95`\n#&gt;   &lt;chr&gt;     &lt;date&gt;          &lt;dbl&gt; &lt;date&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 ca        2020-06-01      2655. 2020-06-08      1940. 2694.  3840.\n#&gt; 2 fl        2020-06-01       726. 2020-06-08       558.  747.  1290.\n#&gt; 3 ga        2020-06-01       643. 2020-06-08       520.  638.  1083.\n#&gt; 4 ny        2020-06-01      1278. 2020-06-08       821. 1044.  1864.\n#&gt; 5 pa        2020-06-01       603. 2020-06-08       450.  570.  1080.\n#&gt; 6 tx        2020-06-01      1002. 2020-06-08       716. 1134.  1950.\n#&gt; # ℹ 108 more rows\n\n\nNote that here we have used an argument ref_time_values to perform the sliding computation (here, compute a forecast) at a specific subset of reference time values. We get out 4 new columns: fc_target_date, 0.05, 0.5, 0.95 that correspond to the date the forecast is for (rather than the date it was made on, the point forecast, and the lower and upper endpoints of the 95% prediction band.1\nTo finish off, we plot the forecasts at some times (spaced out by a few months) over the last year, at multiple horizons: 7, 14, 21, and 28 days ahead. To do so, we encapsulate the process of generating forecasts into a simple function, so that we can call it a few times.\n\nk_week_ahead &lt;- function(ahead = 7) {\n  epi_slide(\n    x,\n    ~ arx_forecaster(\n      epi_data = .x,\n      outcome = \"cases_7dav\",\n      predictors = \"cases_7dav\",\n      trainer = quantile_reg(),\n      args_list = arx_args_list(ahead = ahead)\n    )$predictions,\n    before = 119,\n    ref_time_values = fc_time_values,\n    new_col_name = \"fc\"\n  ) %&gt;%\n    select(\n      geo_value, time_value, cases_7dav, contains(\"_distn\"),\n      fc_target_date\n    ) %&gt;%\n    pivot_quantiles(contains(\"_distn\"))\n}\n\n# First generate the forecasts, and bind them together\nz &lt;- map(c(7, 14, 21, 28), k_week_ahead) %&gt;% list_rbind()\n\nThen we can plot the on top of the observed data\n\n\nCode\nggplot(z) +\n  geom_line(data = x, aes(x = time_value, y = cases_7dav), color = \"gray50\") +\n  geom_ribbon(aes(\n    x = fc_target_date, ymin = `0.05`, ymax = `0.95`,\n    group = time_value, fill = geo_value\n  ), alpha = 0.4) +\n  geom_line(aes(x = fc_target_date, y = `0.5`, group = time_value)) +\n  geom_point(aes(x = fc_target_date, y = `0.5`, group = time_value), size = 0.5) +\n  # geom_vline(data = tibble(x = fc_time_values), aes(xintercept = x),\n  #           linetype = 2, alpha = 0.5) +\n  facet_wrap(vars(geo_value), scales = \"free_y\", nrow = 3) +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  scale_x_date(minor_breaks = \"1 months\", date_labels = \"%b %Y\") +\n  scale_fill_viridis_d(guide = \"none\", end = .9) +\n  labs(x = \"Date\", y = \"Reported COVID-19 cases\")\n\n\n\n\n\n\n\n\n\nTwo points are worth making. First, the AR model’s performance here is pretty spotty. At various points in time, we can see that its forecasts are volatile (its point predictions are all over the place), or overconfident (its bands are too narrow), or both at the same time. This is only meant as a simple demo and not entirely unexpected given the way the AR model is set up. The epipredict package, offers a suite of predictive modeling tools that improve on many of the shortcomings of the above simple AR model (simply using all states for training rather than 6 is a huge improvement).\nSecond, the AR forecaster here is using finalized data, meaning, it uses the latest versions of signal values (reported COVID-19 cases) available, for both training models and making predictions historically. However, this is not reflective of the provisional nature of the data that it must cope with in a true forecast task. Training and making predictions on finalized data can lead to an overly optimistic sense of accuracy; see, for example, (McDonald et al. 2021) and references therein. Fortunately, the epiprocess package provides a data structure called epi_archive that can be used to store all data revisions, and furthermore, an epi_archive object knows how to slide computations in the correct version-aware sense (for the computation at each reference time \\(t\\), it uses only data that would have been available as of \\(t\\)). We will revisit this example in the archive vignette.\n\n\n\n\nMcDonald, Daniel J, Jacob Bien, Alden Green, Addison J Hu, Nat DeFries, Sangwon Hyun, Natalia L Oliveira, et al. 2021. “Can Auxiliary Indicators Improve COVID-19 Forecasting and Hotspot Prediction?” Proceedings of the National Academy of Sciences 118: e2111453118. https://doi.org/10.1073/pnas.2111453118."
  },
  {
    "objectID": "slide.html#footnotes",
    "href": "slide.html#footnotes",
    "title": "3  Sliding computations",
    "section": "",
    "text": "If instead we had set as_list_col = TRUE in the call to epi_slide(), then we would have gotten a list column fc, where each element of fc contains these results.↩︎"
  },
  {
    "objectID": "growth-rates.html#growth-rate-basics",
    "href": "growth-rates.html#growth-rate-basics",
    "title": "4  Estimate growth rates in signals",
    "section": "4.1 Growth rate basics",
    "text": "4.1 Growth rate basics\nThe growth rate of a function \\(f\\) defined over a continuously-valued parameter \\(t\\) is defined as \\(f'(t)/f(t)\\), where \\(f'(t)\\) is the derivative of \\(f\\) at \\(t\\). To estimate the growth rate of a signal in discrete-time (which can be thought of as evaluations or discretizations of an underlying function in continuous-time), we can estimate the derivative and divide by the signal value itself (or possibly a smoothed version of the signal value).\nThe growth_rate() function takes a sequence of underlying design points x and corresponding sequence y of signal values, and allows us to choose from the following methods for estimating the growth rate at a given reference point x0, by setting the method argument:\n\n“rel_change”: uses \\((\\bar B/\\bar A - 1) / h\\), where \\(\\bar B\\) is the average of y over the second half of a sliding window of bandwidth h centered at the reference point x0, and \\(\\bar A\\) the average over the first half. This can be seen as using a first-difference approximation to the derivative.\n“linear_reg”: uses the slope from a linear regression of y on x over a sliding window centered at the reference point x0, divided by the fitted value from this linear regression at x0.\n“smooth_spline”: uses the estimated derivative at x0 from a smoothing spline fit to x and y, via stats::smooth.spline(), divided by the fitted value of the spline at x0.\n“trend_filter”: uses the estimated derivative at x0 from polynomial trend filtering (a discrete spline) fit to x and y, via genlasso::trendfilter(), divided by the fitted value of the discrete spline at x0.\n\nThe default in growth_rate() is x0 = x, so that it returns an estimate of the growth rate at each underlying design point."
  },
  {
    "objectID": "growth-rates.html#relative-change",
    "href": "growth-rates.html#relative-change",
    "title": "4  Estimate growth rates in signals",
    "section": "4.2 Relative change",
    "text": "4.2 Relative change\nThe default method is “rel_change”, which is the simplest way to estimate growth rates. The default bandwidth is h = 7, which for daily data, considers the relative change in a signal over adjacent weeks. We can wrap growth_rate() in a call to dplyr::mutate() to append a new column to our epi_df object with the computed growth rates.\n\nx &lt;- x %&gt;%\n  group_by(geo_value) %&gt;%\n  mutate(cases_gr1 = growth_rate(time_value, cases))\n\nx\n\n#&gt; An `epi_df` object, 1,158 x 4 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-23 13:17:07.044666\n#&gt; \n#&gt; # A tibble: 1,158 × 4\n#&gt; # Groups:   geo_value [2]\n#&gt;   geo_value time_value cases cases_gr1\n#&gt; * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 ga        2020-06-01  643.   0.00601\n#&gt; 2 ga        2020-06-02  603.   0.0185 \n#&gt; 3 ga        2020-06-03  608    0.0240 \n#&gt; 4 ga        2020-06-04  656.   0.0218 \n#&gt; 5 ga        2020-06-05  677.   0.0193 \n#&gt; 6 ga        2020-06-06  718.   0.0163 \n#&gt; # ℹ 1,152 more rows\n\n\nWe can visualize these growth rate estimates by plotting the signal values and highlighting the periods in time for which the relative change is above 1% (in red) and below -1% (in blue), faceting by geo value.\n\n\nCode\nupper &lt;- 0.01\nlower &lt;- -0.01\n\nggplot(x, aes(x = time_value, y = cases)) +\n  geom_tile(\n    data = x %&gt;% filter(cases_gr1 &gt;= upper),\n    aes(x = time_value, y = 0, width = 7, height = Inf),\n    fill = 2, alpha = 0.08\n  ) +\n  geom_tile(\n    data = x %&gt;% filter(cases_gr1 &lt;= lower),\n    aes(x = time_value, y = 0, width = 7, height = Inf),\n    fill = 4, alpha = 0.08\n  ) +\n  geom_line() +\n  facet_wrap(vars(geo_value), scales = \"free_y\") +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  labs(x = \"Date\", y = \"Reported COVID-19 cases\")\n\n\n\n\n\n\n\n\n\nAs a more direct visualization, we plot the estimated growth rates themselves, overlaying the curves for the two states on one plot.\n\n\nCode\nggplot(x, aes(x = time_value, y = cases_gr1)) +\n  geom_line(aes(col = geo_value)) +\n  geom_hline(yintercept = upper, linetype = 2, col = 2) +\n  geom_hline(yintercept = lower, linetype = 2, col = 4) +\n  scale_color_manual(values = c(3, 6)) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(x = \"Date\", y = \"Growth rate\", col = \"State\")\n\n\n\n\n\n\n\n\n\nWe can see that the estimated growth rates from the relative change method are somewhat volatile, and there appears to be some bias towards towards the right boundary of the time span—look at the estimated growth rate for Georgia in late December 2021, which takes a potentially suspicious dip. In general, estimation of derivatives will be difficult near the boundary, but relative changes can suffer from particularly noticeable boundary bias because they are based on a difference in averages over two halves of a local window, and with this simplistic approach, one of these halves will be truncated near a boundary."
  },
  {
    "objectID": "growth-rates.html#linear-regression",
    "href": "growth-rates.html#linear-regression",
    "title": "4  Estimate growth rates in signals",
    "section": "4.3 Linear regression",
    "text": "4.3 Linear regression\nThe second simplest method available is “linear_reg”, whose default bandwidth is again h = 7. Compared to “rel_change”, it appears to behave similarly overall, but thankfully avoids some of the troublesome spikes:\n\nx &lt;- x %&gt;%\n  group_by(geo_value) %&gt;%\n  mutate(cases_gr2 = growth_rate(time_value, cases, method = \"linear_reg\"))\n\n\n\nCode\nx %&gt;%\n  pivot_longer(\n    cols = starts_with(\"cases_gr\"),\n    names_to = \"method\",\n    values_to = \"gr\"\n  ) %&gt;%\n  mutate(\n    method = recode(method,\n      cases_gr1 = \"rel_change\",\n      cases_gr2 = \"linear_reg\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = time_value, y = gr)) +\n  geom_hline(yintercept = 0) +\n  geom_line(aes(col = method)) +\n  scale_color_manual(values = c(2, 4)) +\n  facet_wrap(vars(geo_value), scales = \"free_y\", ncol = 1) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(x = \"Date\", y = \"Growth rate\", col = \"Method\")"
  },
  {
    "objectID": "growth-rates.html#nonparametric-estimation",
    "href": "growth-rates.html#nonparametric-estimation",
    "title": "4  Estimate growth rates in signals",
    "section": "4.4 Nonparametric estimation",
    "text": "4.4 Nonparametric estimation\nWe can also use a nonparametric method to estimate the derivative, through “smooth_spline” or “trend_filter”. The latter is going to be generally more computationally expensive, but it is also able to adapt better to the local level of smoothness. (The apparent efficiency is actually compounded by the particular implementations and default settings for these methods: “trend_filter” is based on a full solution path algorithm provided in the genlasso package, and performs cross-validation by default in order to pick the level of regularization; read the documentation for growth_rate() more details.)\n\nx &lt;- x %&gt;%\n  group_by(geo_value) %&gt;%\n  mutate(\n    cases_gr3 = growth_rate(time_value, cases, method = \"smooth_spline\"),\n    cases_gr4 = growth_rate(time_value, cases, method = \"trend_filter\")\n  )\n\n\n\nCode\nx %&gt;%\n  select(geo_value, time_value, cases_gr3, cases_gr4) %&gt;%\n  pivot_longer(\n    cols = starts_with(\"cases_gr\"),\n    names_to = \"method\",\n    values_to = \"gr\"\n  ) %&gt;%\n  mutate(method = recode(method,\n    cases_gr3 = \"smooth_spline\",\n    cases_gr4 = \"trend_filter\"\n  )) %&gt;%\n  ggplot(aes(x = time_value, y = gr)) +\n  geom_hline(yintercept = 0) +\n  geom_line(aes(col = method)) +\n  scale_color_manual(values = c(3, 6)) +\n  facet_wrap(vars(geo_value), scales = \"free_y\", ncol = 1) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(x = \"Date\", y = \"Growth rate\", col = \"Method\")\n\n\n\n\n\n\n\n\n\nIn this particular example, the trend filtering estimates of growth rate appear to be much more stable than those from the smoothing spline, and also much more stable than the estimates from local relative changes and linear regressions.\nThe smoothing spline growth rate estimates are based on the default settings in stats::smooth.spline(), and appear severely under-regularized here. Any of the arguments to stats::smooth.spline() can be customized by passing them as additional arguments ... in the call to growth_rate(); similarly, we can also use additional arguments to customize the settings in the underlying trend filtering functions genlasso::trendfilter(), genlasso::cv.trendfilter(), and the documentation for growth_rate() gives the full details."
  },
  {
    "objectID": "growth-rates.html#log-scale-estimation",
    "href": "growth-rates.html#log-scale-estimation",
    "title": "4  Estimate growth rates in signals",
    "section": "4.5 Log scale estimation",
    "text": "4.5 Log scale estimation\nIn general, and alternative view for the growth rate of a function \\(f\\) is given by defining \\(g(t) = \\log(f(t))\\), and then observing that \\(g'(t) = f'(t)/f(t)\\). Therefore, any method that estimates the derivative can be simply applied to the log of the signal of interest, and in this light, each method above (“rel_change”, “linear_reg”, “smooth_spline”, and “trend_filter”) has a log scale analog, which can be used by setting the argument log_scale = TRUE in the call to growth_rate().\n\nx &lt;- x %&gt;%\n  group_by(geo_value) %&gt;%\n  mutate(\n    cases_gr5 = growth_rate(time_value, cases,\n      method = \"rel_change\",\n      log_scale = TRUE\n    ),\n    cases_gr6 = growth_rate(time_value, cases,\n      method = \"linear_reg\",\n      log_scale = TRUE\n    ),\n    cases_gr7 = growth_rate(time_value, cases,\n      method = \"smooth_spline\",\n      log_scale = TRUE\n    ),\n    cases_gr8 = growth_rate(time_value, cases,\n      method = \"trend_filter\",\n      log_scale = TRUE\n    )\n  )\n\n\n\nCode\nx %&gt;%\n  select(geo_value, time_value, cases_gr5, cases_gr6) %&gt;%\n  pivot_longer(\n    cols = starts_with(\"cases_gr\"),\n    names_to = \"method\",\n    values_to = \"gr\"\n  ) %&gt;%\n  mutate(method = recode(method,\n    cases_gr5 = \"rel_change_log\",\n    cases_gr6 = \"linear_reg_log\"\n  )) %&gt;%\n  ggplot(aes(x = time_value, y = gr)) +\n  geom_line(aes(col = method)) +\n  geom_hline(yintercept = 0) +\n  scale_color_manual(values = c(2, 4)) +\n  facet_wrap(vars(geo_value), scales = \"free_y\", ncol = 1) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(x = \"Date\", y = \"Growth rate\", col = \"Method\")\n\n\n\n\n\n\n\n\n\nCode\nx %&gt;%\n  select(geo_value, time_value, cases_gr7, cases_gr8) %&gt;%\n  pivot_longer(\n    cols = starts_with(\"cases_gr\"),\n    names_to = \"method\",\n    values_to = \"gr\"\n  ) %&gt;%\n  mutate(method = recode(method,\n    cases_gr7 = \"smooth_spline_log\",\n    cases_gr8 = \"trend_filter_log\"\n  )) %&gt;%\n  ggplot(aes(x = time_value, y = gr)) +\n  geom_hline(yintercept = 0) +\n  geom_line(aes(col = method)) +\n  scale_color_manual(values = c(3, 6)) +\n  facet_wrap(vars(geo_value), scales = \"free_y\", ncol = 1) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(x = \"Date\", y = \"Growth rate\", col = \"Method\")\n\n\n\n\n\n\n\n\n\nComparing the rel_change_log curves with their rel_change counterparts (shown in earlier figures), we see that the former curves appear less volatile and match the linear regression estimates much more closely. In particular, when rel_change has upward spikes, rel_change_log has less pronounced spikes. Why does this occur? The estimate of \\(g'(t)\\) here can be expressed as \\(\\mathbb E[\\log(B)-\\log(A)]/h = \\mathbb E[\\log(1+hR)]/h\\), where \\(R = ((B-A)/h) / A\\), and the expectation refers to averaging over the \\(h\\) observations in each window. Consider the following two relevant inequalities, both due to concavity of the logarithm function:\n\\[\n\\mathbb E[\\log(1+hR)]/h \\leq \\log(1+h\\mathbb E[R])/h \\leq \\mathbb E[R].\n\\]\nThe first inequality is Jensen’s; the second inequality is because the tangent line of a concave function lies above it. Finally, we observe that \\(\\mathbb E[R] \\approx ((\\bar B-\\bar A)/h) / \\bar A\\), which the rel_change estimate.\nThis explains why the rel_change_log curve often lies below the rel_change curve."
  },
  {
    "objectID": "correlations.html#correlations-grouped-by-time",
    "href": "correlations.html#correlations-grouped-by-time",
    "title": "5  Correlate signals over space and time",
    "section": "5.1 Correlations grouped by time",
    "text": "5.1 Correlations grouped by time\nThe epi_cor() function operates on an epi_df object, and it requires further specification of the variables to correlate, in its next two arguments (var1 and var2).\nIn general, we can specify any grouping variable (or combination of variables) for the correlation computations in a call to epi_cor(), via the cor_by argument. This potentially leads to many ways to compute correlations. There are always at least two ways to compute correlations in an epi_df: grouping by time value, and by geo value. The former is obtained via cor_by = time_value.\n\nz1 &lt;- epi_cor(x, case_rate, death_rate, cor_by = \"time_value\")\n\n\n\nCode\nggplot(z1, aes(x = time_value, y = cor)) +\n  geom_hline(yintercept = 0) +\n  geom_line(color = 4) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(x = \"Date\", y = \"Correlation\")\n\n\n\n\n\n\n\n\n\nThe above plot addresses the question: “on any given day, are case and death rates linearly associated, across the U.S. states?”. We might be interested in broadening this question, instead asking: “on any given day, do higher case rates tend to associate with higher death rates?”, removing the dependence on a linear relationship. The latter can be addressed using Spearman correlation, accomplished by setting method = \"spearman\" in the call to epi_cor(). Spearman correlation is highly robust and invariant to monotone transformations."
  },
  {
    "objectID": "correlations.html#lagged-correlations",
    "href": "correlations.html#lagged-correlations",
    "title": "5  Correlate signals over space and time",
    "section": "5.2 Lagged correlations",
    "text": "5.2 Lagged correlations\nWe might also be interested in how case rates associate with death rates in the future. Using the dt1 parameter in epi_cor(), we can lag case rates back any number of days we want, before calculating correlations. Below, we set dt1 = -10. This means that var1 = case_rate will be lagged by 10 days, so that case rates on June 1st will be correlated with death rates on June 11th. (It might also help to think of it this way: death rates on a certain day will be correlated with case rates at an offset of -10 days.)\n\nz2 &lt;- epi_cor(x, case_rate, death_rate, cor_by = time_value, dt1 = -10)\n\n\n\nCode\nz &lt;- rbind(\n  z1 %&gt;% mutate(lag = 0),\n  z2 %&gt;% mutate(lag = 10)\n) %&gt;%\n  mutate(lag = as.factor(lag))\n\nggplot(z, aes(x = time_value, y = cor)) +\n  geom_hline(yintercept = 0) +\n  geom_line(aes(color = lag)) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(x = \"Date\", y = \"Correlation\", col = \"Lag\")\n\n\n\n\n\n\n\n\n\nNote that epi_cor() takes an argument shift_by that determines the grouping to use for the time shifts. The default is geo_value, which makes sense in our problem at hand (but in another setting, we may want to group by geo value and another variable—say, age—before time shifting).\nWe can see that, generally, lagging the case rates back by 10 days improves the correlations, confirming case rates are better correlated with death rates 10 days from now."
  },
  {
    "objectID": "correlations.html#correlations-grouped-by-state",
    "href": "correlations.html#correlations-grouped-by-state",
    "title": "5  Correlate signals over space and time",
    "section": "5.3 Correlations grouped by state",
    "text": "5.3 Correlations grouped by state\nThe second option we have is to group by geo value, obtained by setting cor_by = geo_value. We’ll again look at correlations for both 0- and 10-day lagged case rates.\n\nz1 &lt;- epi_cor(x, case_rate, death_rate, cor_by = geo_value)\nz2 &lt;- epi_cor(x, case_rate, death_rate, cor_by = geo_value, dt1 = -10)\n\n\n\nCode\nz &lt;- rbind(\n  z1 %&gt;% mutate(lag = 0),\n  z2 %&gt;% mutate(lag = 10)\n) %&gt;%\n  mutate(lag = as.factor(lag))\n\nggplot(z, aes(cor)) +\n  geom_density(aes(fill = lag, col = lag), alpha = 0.5, bounds = c(-1, 1)) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(x = \"Correlation\", y = \"Density\", fill = \"Lag\", col = \"Lag\")\n\n\n\n\n\n\n\n\n\nWe can again see that, generally speaking, lagging the case rates back by 10 days improves the correlations."
  },
  {
    "objectID": "correlations.html#more-systematic-lag-analysis",
    "href": "correlations.html#more-systematic-lag-analysis",
    "title": "5  Correlate signals over space and time",
    "section": "5.4 More systematic lag analysis",
    "text": "5.4 More systematic lag analysis\nNext we perform a more systematic investigation of the correlations over a broad range of lag values.\n\nlags &lt;- 0:35\n\nz &lt;- map(\n  .x = lags,\n  ~ epi_cor(x, case_rate, death_rate, cor_by = geo_value, dt1 = -.x) %&gt;%\n    mutate(lag = .x)\n) %&gt;% list_rbind()\n\n\n\nCode\nz %&gt;%\n  group_by(lag) %&gt;%\n  summarize(mean = mean(cor, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = lag, y = mean)) +\n  geom_line(color = 4) +\n  geom_point(color = 4) +\n  labs(x = \"Lag\", y = \"Mean correlation\")\n\n\n\n\n\n\n\n\n\nWe can see pronounced curvature in the average correlation between case and death rates (where the correlations come from grouping by geo_value) as a function of lag. The maximum occurs at a lag of somewhere around 17 days."
  },
  {
    "objectID": "outliers.html#outlier-detection",
    "href": "outliers.html#outlier-detection",
    "title": "6  Detect and correct outliers in signals",
    "section": "6.1 Outlier detection",
    "text": "6.1 Outlier detection\nThe detect_outlr() function allows us to run multiple outlier detection methods on a given signal, and then (optionally) combine the results from those methods. Here, we’ll investigate outlier detection results from the following methods.\n\nDetection based on a rolling median, using detect_outlr_rm(), which computes a rolling median on with a default window size of n time points centered at the time point under consideration, and then computes thresholds based on a multiplier times a rolling IQR computed on the residuals.\nDetection based on a seasonal-trend decomposition using LOESS (STL), using detect_outlr_stl(), which is similar to the rolling median method but replaces the rolling median with fitted values from STL.\nDetection based on an STL decomposition, but without seasonality term, which amounts to smoothing using LOESS.\n\nThe outlier detection methods are specified using a tibble that is passed to detect_outlr(), with one row per method, and whose columms specify the outlier detection function, any input arguments (only nondefault values need to be supplied), and an abbreviated name for the method used in tracking results. Abbreviations “rm” and “stl” can be used for the built-in detection functions detect_outlr_rm() and detect_outlr_stl(), respectively.\n\ndetection_methods &lt;- bind_rows(\n  tibble(\n    method = \"rm\",\n    args = list(list(\n      detect_negatives = TRUE,\n      detection_multiplier = 2.5\n    )),\n    abbr = \"rm\"\n  ),\n  tibble(\n    method = \"stl\",\n    args = list(list(\n      detect_negatives = TRUE,\n      detection_multiplier = 2.5,\n      seasonal_period = 7\n    )),\n    abbr = \"stl_seasonal\"\n  ),\n  tibble(\n    method = \"stl\",\n    args = list(list(\n      detect_negatives = TRUE,\n      detection_multiplier = 2.5,\n      seasonal_period = NULL\n    )),\n    abbr = \"stl_nonseasonal\"\n  )\n)\n\ndetection_methods\n\n#&gt; # A tibble: 3 × 3\n#&gt;   method args             abbr           \n#&gt;   &lt;chr&gt;  &lt;list&gt;           &lt;chr&gt;          \n#&gt; 1 rm     &lt;named list [2]&gt; rm             \n#&gt; 2 stl    &lt;named list [3]&gt; stl_seasonal   \n#&gt; 3 stl    &lt;named list [3]&gt; stl_nonseasonal\n\n\nAdditionally, we’ll form combined lower and upper thresholds, calculated as the median of the lower and upper thresholds from the methods at each time point. Note that using this combined median threshold is equivalent to using a majority vote across the base methods to determine whether a value is an outlier.\n\nx &lt;- x %&gt;%\n  group_by(geo_value) %&gt;%\n  mutate(\n    outlier_info = detect_outlr(\n      x = time_value, y = cases,\n      methods = detection_methods,\n      combiner = \"median\"\n    )\n  ) %&gt;%\n  ungroup() %&gt;%\n  unnest(outlier_info)\n\nx\n\n#&gt; An `epi_df` object, 730 x 15 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-21 15:17:14.962335\n#&gt; \n#&gt; # A tibble: 730 × 15\n#&gt;   geo_value time_value cases rm_lower rm_upper rm_replacement\n#&gt; * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1 fl        2020-06-01   667    345      2195             667\n#&gt; 2 nj        2020-06-01   486     64.4     926.            486\n#&gt; 3 fl        2020-06-02   617    406.     2169.            617\n#&gt; 4 nj        2020-06-02   658    140.      841.            658\n#&gt; 5 fl        2020-06-03  1317    468.     2142.           1317\n#&gt; 6 nj        2020-06-03   541    216       756             541\n#&gt; # ℹ 724 more rows\n#&gt; # ℹ 9 more variables: stl_seasonal_lower &lt;dbl&gt;, stl_seasonal_upper &lt;dbl&gt;, …\n\n\nTo visualize the results, we define a convenience function for and call it on each state separately (hidden below the fold).\n\n\nCode\n# Plot outlier detection bands and/or points identified as outliers\nplot_outlr &lt;- function(\n    x, signal, method_abbr, bands = TRUE, points = TRUE,\n    facet_vars = vars(geo_value), nrow = NULL, ncol = NULL,\n    scales = \"fixed\") {\n  # Convert outlier detection results to long format\n  signal &lt;- rlang::enquo(signal)\n  x_long &lt;- x %&gt;%\n    pivot_longer(\n      cols = starts_with(method_abbr),\n      names_to = c(\"method\", \".value\"),\n      names_pattern = \"(.+)_(.+)\"\n    )\n\n  # Start of plot with observed data\n  p &lt;- ggplot() +\n    geom_line(data = x, mapping = aes(x = time_value, y = !!signal))\n\n  # If requested, add bands\n  if (bands) {\n    p &lt;- p + geom_ribbon(\n      data = x_long,\n      aes(\n        x = time_value, ymin = lower, ymax = upper,\n        color = method\n      ), fill = NA\n    )\n  }\n\n  # If requested, add points\n  if (points) {\n    x_detected &lt;- x_long %&gt;% filter((!!signal &lt; lower) | (!!signal &gt; upper))\n    p &lt;- p + geom_point(\n      data = x_detected,\n      aes(\n        x = time_value, y = !!signal, color = method,\n        shape = method\n      )\n    )\n  }\n\n  # If requested, add faceting\n  if (!is.null(facet_vars)) {\n    p &lt;- p + facet_wrap(facet_vars, nrow = nrow, ncol = ncol, scales = scales)\n  }\n\n  return(p)\n}\n\n\nNow we produce plots for each state at a time, faceting by the detection method.\n\n\nCode\nmethod_abbr &lt;- c(detection_methods$abbr, \"combined\")\n\nplot_outlr(x %&gt;% filter(geo_value == \"fl\"), cases, method_abbr,\n  facet_vars = vars(method), scales = \"free_y\", ncol = 2\n) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(\n    x = \"Date\", y = \"Reported COVID-19 counts\", color = \"Method\",\n    shape = \"Method\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  ggtitle(\"Florida\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nCode\nplot_outlr(x %&gt;% filter(geo_value == \"nj\"), cases, method_abbr,\n  facet_vars = vars(method), scales = \"free_y\", ncol = 2\n) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(\n    x = \"Date\", y = \"Reported COVID-19 counts\", color = \"Method\",\n    shape = \"Method\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  ggtitle(\"New Jersey\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "outliers.html#outlier-correction",
    "href": "outliers.html#outlier-correction",
    "title": "6  Detect and correct outliers in signals",
    "section": "6.2 Outlier correction",
    "text": "6.2 Outlier correction\nFinally, in order to correct outliers, we can use the posited replacement values returned by each outlier detection method. Below we use the replacement value from the combined method, which is defined by the median of replacement values from the base methods at each time point.\n\ny &lt;- x %&gt;%\n  mutate(cases_corrected = combined_replacement) %&gt;%\n  select(geo_value, time_value, cases, cases_corrected)\n\ny %&gt;% filter(cases != cases_corrected)\n\n#&gt; An `epi_df` object, 22 x 4 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-21 15:17:14.962335\n#&gt; \n#&gt; # A tibble: 22 × 4\n#&gt;   geo_value time_value cases cases_corrected\n#&gt; * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1 fl        2020-07-12 15300          10181 \n#&gt; 2 nj        2020-07-19    -8            320.\n#&gt; 3 nj        2020-08-13   694            404.\n#&gt; 4 nj        2020-08-14   619            397.\n#&gt; 5 nj        2020-08-16    40            366 \n#&gt; 6 nj        2020-08-22   555            360 \n#&gt; # ℹ 16 more rows\n\n\n\n\nCode\ny %&gt;%\n  pivot_longer(starts_with(\"cases\")) %&gt;%\n  ggplot(aes(x = time_value)) +\n  geom_line(aes(y = value, color = name, linetype = name)) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_linetype_manual(values = c(2, 1)) +\n  geom_hline(yintercept = 0) +\n  facet_wrap(vars(geo_value), scales = \"free_y\", ncol = 1) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  labs(x = \"Date\", y = \"Reported COVID-19 counts\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nMore advanced correction functionality will be coming at some point in the future."
  },
  {
    "objectID": "archive.html#getting-data-into-epi_archive-format",
    "href": "archive.html#getting-data-into-epi_archive-format",
    "title": "7  Work with archive objects and data revisions",
    "section": "7.1 Getting data into epi_archive format",
    "text": "7.1 Getting data into epi_archive format\nAn epi_archive object can be constructed from a data frame, data table, or tibble, provided that it has (at least) the following columns:\n\ngeo_value: the geographic value associated with each row of measurements.\ntime_value: the time value associated with each row of measurements.\nversion: the time value specifying the version for each row of measurements. For example, if in a given row the version is January 15, 2022 and time_value is January 14, 2022, then this row contains the measurements of the data for January 14, 2022 that were available one day later.\n\nAs we can see from the above, the data frame returned by epidatr::covidcast() has the columns required for the epi_archive format, so we use as_epi_archive() to cast it into epi_archive format.1\n\nx &lt;- archive_cases_dv_subset_dt %&gt;%\n  select(geo_value, time_value, version, percent_cli) %&gt;%\n  as_epi_archive(compactify = TRUE)\n\nclass(x)\n\n#&gt; [1] \"epi_archive\" \"R6\"\n\nprint(x)\n\n#&gt; An `epi_archive` object, with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; ----------\n#&gt; * min time value = 2020-06-01\n#&gt; * max time value = 2021-11-30\n#&gt; * first version with update = 2020-06-02\n#&gt; * last version with update = 2021-12-01\n#&gt; * No clobberable versions\n#&gt; * versions end   = 2021-12-01\n#&gt; ----------\n#&gt; Data archive (stored in DT field): 119316 x 4\n#&gt; Columns in DT: geo_value, time_value, version, percent_cli\n#&gt; ----------\n#&gt; Public R6 methods: initialize, print, as_of, fill_through_version, \n#&gt;                    truncate_versions_after, merge, group_by, slide, clone\n\n\nAn epi_archive is special kind of class called an R6 class. Its primary field is a data table DT, which is of class data.table (from the data.table package), and has columns geo_value, time_value, version, as well as any number of additional columns.\n\nclass(x$DT)\n\n#&gt; [1] \"data.table\" \"data.frame\"\n\nhead(x$DT)\n\n#&gt;    geo_value time_value    version percent_cli\n#&gt; 1:        ca 2020-06-01 2020-06-02          NA\n#&gt; 2:        ca 2020-06-01 2020-06-06    2.140116\n#&gt; 3:        ca 2020-06-01 2020-06-08    2.140379\n#&gt; 4:        ca 2020-06-01 2020-06-09    2.114430\n#&gt; 5:        ca 2020-06-01 2020-06-10    2.133677\n#&gt; 6:        ca 2020-06-01 2020-06-11    2.197207\n\n\nThe variables geo_value, time_value, version serve as key variables for the data table, as well as any other specified in the metadata (described below). There can only be a single row per unique combination of key variables, and therefore the key variables are critical for figuring out how to generate a snapshot of data from the archive, as of a given version (also described below).\n\nkey(x$DT)\n\n#&gt; Error in key(x$DT): could not find function \"key\"\n\n\nIn general, the last version of each observation is carried forward (LOCF) to fill in data between recorded versions. A word of caution: R6 objects, unlike most other objects in R, have reference semantics. An important consequence of this is that objects are not copied when modified.\n\noriginal_value &lt;- x$DT$percent_cli[1]\ny &lt;- x # This DOES NOT make a copy of x\ny$DT$percent_cli[1] &lt;- 0\nhead(y$DT)\n\n#&gt;    geo_value time_value    version percent_cli\n#&gt; 1:        ca 2020-06-01 2020-06-02    0.000000\n#&gt; 2:        ca 2020-06-01 2020-06-06    2.140116\n#&gt; 3:        ca 2020-06-01 2020-06-08    2.140379\n#&gt; 4:        ca 2020-06-01 2020-06-09    2.114430\n#&gt; 5:        ca 2020-06-01 2020-06-10    2.133677\n#&gt; 6:        ca 2020-06-01 2020-06-11    2.197207\n\nhead(x$DT)\n\n#&gt;    geo_value time_value    version percent_cli\n#&gt; 1:        ca 2020-06-01 2020-06-02    0.000000\n#&gt; 2:        ca 2020-06-01 2020-06-06    2.140116\n#&gt; 3:        ca 2020-06-01 2020-06-08    2.140379\n#&gt; 4:        ca 2020-06-01 2020-06-09    2.114430\n#&gt; 5:        ca 2020-06-01 2020-06-10    2.133677\n#&gt; 6:        ca 2020-06-01 2020-06-11    2.197207\n\nx$DT$percent_cli[1] &lt;- original_value\n\nTo make a copy, we can use the clone() method for an R6 class, as in y &lt;- x$clone(). You can read more about reference semantics in Hadley Wickham’s Advanced R book."
  },
  {
    "objectID": "archive.html#some-details-on-metadata",
    "href": "archive.html#some-details-on-metadata",
    "title": "7  Work with archive objects and data revisions",
    "section": "7.2 Some details on metadata",
    "text": "7.2 Some details on metadata\nThe following pieces of metadata are included as fields in an epi_archive object:\n\ngeo_type: the type for the geo values.\ntime_type: the type for the time values.\nadditional_metadata: list of additional metadata for the data archive.\n\nMetadata for an epi_archive object x can be accessed (and altered) directly, as in x$geo_type or x$time_type, etc. Just like as_epi_df(), the function as_epi_archive() attempts to guess metadata fields when an epi_archive object is instantiated, if they are not explicitly specified in the function call (as it did in the case above)."
  },
  {
    "objectID": "archive.html#producing-snapshots-in-epi_df-form",
    "href": "archive.html#producing-snapshots-in-epi_df-form",
    "title": "7  Work with archive objects and data revisions",
    "section": "7.3 Producing snapshots in epi_df form",
    "text": "7.3 Producing snapshots in epi_df form\nA key method of an epi_archive class is as_of(), which generates a snapshot of the archive in epi_df format. This represents the most up-to-date values of the signal variables as of a given version. This can be accessed via x$as_of() for an epi_archive object x, but the package also provides a simple wrapper function epix_as_of() since this is likely a more familiar interface for users not familiar with R6 (or object-oriented programming).\n\nx_snapshot &lt;- epix_as_of(x, max_version = as.Date(\"2021-06-01\"))\nclass(x_snapshot)\n\n#&gt; [1] \"epi_df\"     \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nx_snapshot\n\n#&gt; An `epi_df` object, 1,460 x 3 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2021-06-01\n#&gt; \n#&gt; # A tibble: 1,460 × 3\n#&gt;   geo_value time_value percent_cli\n#&gt; * &lt;chr&gt;     &lt;date&gt;           &lt;dbl&gt;\n#&gt; 1 ca        2020-06-01        2.75\n#&gt; 2 ca        2020-06-02        2.57\n#&gt; 3 ca        2020-06-03        2.48\n#&gt; 4 ca        2020-06-04        2.41\n#&gt; 5 ca        2020-06-05        2.57\n#&gt; 6 ca        2020-06-06        2.63\n#&gt; # ℹ 1,454 more rows\n\nmax(x_snapshot$time_value)\n\n#&gt; [1] \"2021-05-31\"\n\nattributes(x_snapshot)$metadata$as_of\n\n#&gt; [1] \"2021-06-01\"\n\n\nWe can see that the max time value in the epi_df object x_snapshot that was generated from the archive is May 29, 2021, even though the specified version date was June 1, 2021. From this we can infer that the doctor’s visits signal was 2 days latent on June 1. Also, we can see that the metadata in the epi_df object has the version date recorded in the as_of field.\nBy default, using the maximum of the version column in the underlying data table in an epi_archive object itself generates a snapshot of the latest values of signal variables in the entire archive. The epix_as_of() function issues a warning in this case, since updates to the current version may still come in at a later point in time, due to various reasons, such as synchronization issues.\n\nx_latest &lt;- epix_as_of(x, max_version = max(x$DT$version))\n\nBelow, we pull several snapshots from the archive, spaced one month apart. We overlay the corresponding signal curves as colored lines, with the version dates marked by dotted vertical lines, and draw the latest curve in black (from the latest snapshot x_latest that the archive can provide).\n\nself_max &lt;- max(x$DT$version)\nversions &lt;- seq(as.Date(\"2020-06-01\"), self_max - 1, by = \"1 month\")\nsnapshots &lt;- map(\n  versions,\n  function(v) {\n    epix_as_of(x, max_version = v) %&gt;% mutate(version = v)\n  }\n) %&gt;%\n  list_rbind() %&gt;%\n  bind_rows(x_latest %&gt;% mutate(version = self_max)) %&gt;%\n  mutate(latest = version == self_max)\n\n\n\nCode\nggplot(\n  snapshots %&gt;% filter(!latest),\n  aes(x = time_value, y = percent_cli)\n) +\n  geom_line(aes(color = factor(version)), na.rm = TRUE) +\n  geom_vline(aes(color = factor(version), xintercept = version), lty = 2) +\n  facet_wrap(~geo_value, scales = \"free_y\", ncol = 1) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %Y\") +\n  scale_color_viridis_d(option = \"A\", end = .9) +\n  labs(x = \"Date\", y = \"% of doctor's visits with CLI\") +\n  theme(legend.position = \"none\") +\n  geom_line(\n    data = snapshots %&gt;% filter(latest),\n    aes(x = time_value, y = percent_cli),\n    inherit.aes = FALSE, color = \"black\", na.rm = TRUE\n  )\n\n\n\n\n\n\n\n\n\nWe can see some interesting and highly nontrivial revision behavior: at some points in time the provisional data snapshots grossly underestimate the latest curve (look in particular at Florida close to the end of 2021), and at others they overestimate it (both states towards the beginning of 2021), though not quite as dramatically. Modeling the revision process, which is often called backfill modeling, is an important statistical problem in it of itself."
  },
  {
    "objectID": "archive.html#merging-epi_archive-objects",
    "href": "archive.html#merging-epi_archive-objects",
    "title": "7  Work with archive objects and data revisions",
    "section": "7.4 Merging epi_archive objects",
    "text": "7.4 Merging epi_archive objects\nNow we demonstrate how to merge two epi_archive objects together, e.g., so that grabbing data from multiple sources as of a particular version can be performed with a single as_of call. The epi_archive class provides a method merge() precisely for this purpose. The wrapper function is called epix_merge(); this wrapper avoids mutating its inputs, while x$merge will mutate x. Below we merge the working epi_archive of versioned percentage CLI from outpatient visits to another one of versioned COVID-19 case reporting data, which we fetch the from the COVIDcast API, on the rate scale (counts per 100,000 people in the population).\nWhen merging archives, unless the archives have identical data release patterns, NAs can be introduced in the non-key variables for a few reasons: - to represent the “value” of an observation before its initial release (when we need to pair it with additional observations from the other archive that have been released) - to represent the “value” of an observation that has no recorded versions at all (in the same sort of situation) - if requested via sync = \"na\", to represent potential update data that we do not yet have access to (e.g., due to encountering issues while attempting to download the currently available version data for one of the archives, but not the other).\n\n# This code is for illustration and doesn't run.\n# The result is saved/loaded in the (hidden) next chunk from `{epidatasets}`\ny &lt;- covidcast(\n  data_source = \"jhu-csse\",\n  signals = \"confirmed_7dav_incidence_prop\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20200601, 20211201),\n  geo_values = \"ca,fl,ny,tx\",\n  issues = epirange(20200601, 20211201)\n) %&gt;%\n  fetch() %&gt;%\n  select(geo_value, time_value, version = issue, case_rate_7d_av = value) %&gt;%\n  as_epi_archive(compactify = TRUE)\n\nx$merge(y, sync = \"locf\", compactify = FALSE)\nprint(x)\nhead(x$DT)\n\n\n\n#&gt; An `epi_archive` object, with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; ----------\n#&gt; * min time value = 2020-06-01\n#&gt; * max time value = 2021-11-30\n#&gt; * first version with update = 2020-06-02\n#&gt; * last version with update = 2021-12-01\n#&gt; * No clobberable versions\n#&gt; * versions end   = 2021-12-01\n#&gt; ----------\n#&gt; Data archive (stored in DT field): 129638 x 5\n#&gt; Columns in DT: geo_value, time_value, version, percent_cli and 1 more columns\n#&gt; ----------\n#&gt; Public R6 methods: initialize, print, as_of, fill_through_version, \n#&gt;                    truncate_versions_after, merge, group_by, slide, clone\n\n\n#&gt;    geo_value time_value    version percent_cli case_rate_7d_av\n#&gt; 1:        ca 2020-06-01 2020-06-02          NA        6.628329\n#&gt; 2:        ca 2020-06-01 2020-06-06    2.140116        6.628329\n#&gt; 3:        ca 2020-06-01 2020-06-07    2.140116        6.628329\n#&gt; 4:        ca 2020-06-01 2020-06-08    2.140379        6.628329\n#&gt; 5:        ca 2020-06-01 2020-06-09    2.114430        6.628329\n#&gt; 6:        ca 2020-06-01 2020-06-10    2.133677        6.628329\n\n\nImportantly, see that x$merge mutated x to hold the result of the merge. We could also have used xy = epix_merge(x, y) to avoid mutating x. See the documentation for either for more detailed descriptions of what mutation, pointer aliasing, and pointer reseating is possible."
  },
  {
    "objectID": "archive.html#sliding-version-aware-computations",
    "href": "archive.html#sliding-version-aware-computations",
    "title": "7  Work with archive objects and data revisions",
    "section": "7.5 Sliding version-aware computations",
    "text": "7.5 Sliding version-aware computations\n\n\n\n\n\n\nNote\n\n\n\nTODO: need a simple example here."
  },
  {
    "objectID": "archive.html#footnotes",
    "href": "archive.html#footnotes",
    "title": "7  Work with archive objects and data revisions",
    "section": "",
    "text": "For a discussion of the removal of redundant version updates in as_epi_archive using compactify, please refer to the compactify vignette.↩︎"
  },
  {
    "objectID": "epipredict.html#baseline-models",
    "href": "epipredict.html#baseline-models",
    "title": "8  Overview",
    "section": "8.1 Baseline models",
    "text": "8.1 Baseline models\nWe provide a set of basic, easy-to-use forecasters that work out of the box. You should be able to do a reasonably limited amount of customization on them. Any serious customization happens with the framework discussed below).\nFor the basic forecasters, we provide:\n\nFlatline (basic) forecaster\nAutoregressive forecaster\nAutoregressive classifier\nSmooth AR forecaster\n\nAll the forcasters we provide are built on our framework. So we will use these basic models to illustrate its flexibility."
  },
  {
    "objectID": "epipredict.html#forecasting-framework",
    "href": "epipredict.html#forecasting-framework",
    "title": "8  Overview",
    "section": "8.2 Forecasting framework",
    "text": "8.2 Forecasting framework\nAt its core, {epipredict} is a framework for creating custom forecasters. By that we mean that we view the process of creating custom forecasters as a collection of modular components. All of them should be easy to swap out or exchange for others, and massive variety should be available by fairly simple modifications through the addition of steps or layers. There are four types of components:\n\nPreprocessor: make transformations to the data before model training\nTrainer: train a model on data, resulting in a fitted model object\nPredictor: make predictions, using a fitted model object and processed test data\nPostprocessor: manipulate or transform the predictions before returning\n\nUsers familiar with {tidymodels} and especially the {workflows} package will notice a lot of overlap. This is by design, and is in fact a feature. The truth is that {epipredict} is a wrapper around much that is contained in these packages. Therefore, if you want something from this -verse, it should “just work” (we hope).\nThe reason for the overlap is that {workflows} already implements the first three steps. And it does this very well. However, it is missing the postprocessing stage and currently has no plans for such an implementation. And this feature is important. The baseline forecaster we provide requires postprocessing. Anything more complicated (which is nearly everything) needs this as well.\nThe second omission from {tidymodels} is support for panel data. Besides epidemiological data, economics, psychology, sociology, and many other areas frequently deal with data of this type. So the framework of behind {epipredict} implements this. In principle, this has nothing to do with epidemiology, and one could simply use this package as a solution for the missing functionality in {tidymodels}. Again, this should “just work” (we hope).\nAll of the panel data functionality is implemented through the epi_df data type described in the previous part. If you have different panel data, just force it into an epi_df as described in Section 2.2."
  },
  {
    "objectID": "epipredict.html#why-doesnt-this-package-already-exist",
    "href": "epipredict.html#why-doesnt-this-package-already-exist",
    "title": "8  Overview",
    "section": "8.3 Why doesn’t this package already exist?",
    "text": "8.3 Why doesn’t this package already exist?\n\nParts of it actually DO exist. There’s a universe called tidymodels. It handles pre-processing, training, and prediction, bound together, through a package called workflows. We built epipredict on top of that setup. In this way, you CAN use almost everything they provide.\nHowever, workflows doesn’t do post-processing to the extent envisioned here. And nothing in tidymodels handles panel data.\nThe tidy-team doesn’t have plans to do either of these things. (We checked).\nThere are two packages that do time series built on tidymodels, but it’s “basic” time series: 1-step AR models, exponential smoothing, STL decomposition, etc.1"
  },
  {
    "objectID": "epipredict.html#show-me-the-basics",
    "href": "epipredict.html#show-me-the-basics",
    "title": "8  Overview",
    "section": "8.4 Show me the basics",
    "text": "8.4 Show me the basics\nFor now, we’ll just demonstrate one of the “canned” forecasters we provide: an autoregressive forecaster with (or without) covariates that directly trains on the response. This is in contrast to a typical “iterative” AR model that trains to predict one-step-ahead, and then plugs in the predictions to “leverage up” to longer horizons. You saw this function in Section 3.4, but now we’ll explain the arguments a bit more thoroughly. Below, you’ll see how to make a number of modifications to this forecaster, but understanding the inner workings, and why you would want something like this (as well as how to do elaborate customizations) will be the subject of the rest of this book.\nWe’ll use some of the same data we’ve examined earlier and estimate a model jointly across all locations using only the most recent 30 days of data (available in the built-in data frame).\n\njhu &lt;- case_death_rate_subset %&gt;%\n  filter(time_value &gt;= max(time_value) - 30)\n\nout &lt;- arx_forecaster(\n  jhu,\n  outcome = \"death_rate\",\n  predictors = c(\"case_rate\", \"death_rate\")\n)\n\n#&gt; Warning: The forecast_date is less than the most recent update date of the\n#&gt; data.forecast_date = 2021-12-31 while data is from 2022-05-31.\n\n\nThis call produces a warning, which we’ll ignore for now. But essentially, it’s telling us that our data comes from May 2022 but we’re trying to do a forecast for January 2022. The result is likely not an accurate measure of real-time forecast performance, because the data have been revised over time.\n\nout\n\n#&gt; \n#&gt; ══ A basic forecaster of type ARX Forecaster ════════════════════════════════\n#&gt; \n#&gt; This forecaster was fit on 2023-06-19 20:31:07\n#&gt; \n#&gt; Training data was an `epi_df` with\n#&gt; • Geography: state,\n#&gt; • Time type: day,\n#&gt; • Using data up-to-date as of: 2022-05-31 12:08:25.\n#&gt; \n#&gt; ── Predictions ──────────────────────────────────────────────────────────────\n#&gt; \n#&gt; A total of 56 predictions are available for\n#&gt; • 56 unique geographic regions,\n#&gt; • At forecast dates: 2021-12-31,\n#&gt; • For target dates: 2022-01-07.\n\n\nPrinting the S3 object provides a bunch of summary information describing the original training data used to estimate the model as well as some information of what the predictions are for. It contains three main components:\n\nMetadata about the training data and when the forecast was created\n\n\nstr(out$metadata)\n\n#&gt; List of 2\n#&gt;  $ training        :List of 3\n#&gt;   ..$ geo_type : chr \"state\"\n#&gt;   ..$ time_type: chr \"day\"\n#&gt;   ..$ as_of    : POSIXct[1:1], format: \"2022-05-31 12:08:25\"\n#&gt;  $ forecast_created: POSIXct[1:1], format: \"2023-06-19 20:31:07\"\n\n\n\nThe predictions in a tibble. The columns give the predictions for each location along with additional columns. By default, these are a 90% predictive interval, the forecast_date (the date on which the forecast was putatively made) and the target_date (the date for which the forecast is being made).\n\n\nout$predictions\n\n#&gt; # A tibble: 56 × 5\n#&gt;   geo_value  .pred         .pred_distn forecast_date target_date\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;              &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n#&gt; 1 ak        0.355  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 2 al        0.325  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 3 ar        0.496  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 4 as        0.0836 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 5 az        0.614  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 6 ca        0.327  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; # ℹ 50 more rows\n\n\n\nAn S3 object of class epi_workflow. This object encapsulates all the instructions necessary to create the prediction. More details on this below.\n\n\nout$epi_workflow\n\n#&gt; ══ Epi Workflow [trained] ═══════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; Postprocessor: Frosting\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 6 Recipe Steps\n#&gt; \n#&gt; • step_epi_lag()\n#&gt; • step_epi_lag()\n#&gt; • step_epi_ahead()\n#&gt; • step_naomit()\n#&gt; • step_naomit()\n#&gt; • step_training_window()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)    lag_0_case_rate    lag_7_case_rate   lag_14_case_rate  \n#&gt;         0.0829475          0.0009830          0.0027035         -0.0005651  \n#&gt;  lag_0_death_rate   lag_7_death_rate  lag_14_death_rate  \n#&gt;         0.2466110          0.1964921          0.0752998  \n#&gt; \n#&gt; ── Postprocessor ────────────────────────────────────────────────────────────\n#&gt; 5 Frosting Layers\n#&gt; \n#&gt; • layer_predict()\n#&gt; • layer_residual_quantiles()\n#&gt; • layer_add_forecast_date()\n#&gt; • layer_add_target_date()\n#&gt; • layer_threshold()\n\n\nBy default, the forecaster predicts the outcome (death_rate) 1-week ahead, using 3 lags of each predictor (case_rate and death_rate) at 0 (today), 1 week back and 2 weeks back. The predictors and outcome can be changed directly. The rest of the defaults are encapsulated into a list of arguments. This list is produced by arx_args_list()."
  },
  {
    "objectID": "epipredict.html#simple-adjustments",
    "href": "epipredict.html#simple-adjustments",
    "title": "8  Overview",
    "section": "8.5 Simple adjustments",
    "text": "8.5 Simple adjustments\nBasic adjustments can be made through the args_list.\n\nout2week &lt;- arx_forecaster(\n  epi_data = jhu,\n  outcome = \"death_rate\",\n  predictors = c(\"case_rate\", \"death_rate\"),\n  args_list = arx_args_list(\n    lags = list(case_rate = c(0, 1, 2, 3, 7, 14), death_rate = c(0, 7, 14)),\n    ahead = 14\n  )\n)\n\nHere, we’ve used different lags on the case_rate and are now predicting 2 weeks ahead. Note that lags and aheads are in the same units as the time_value of the epi_df used for training (same as the epi_slide() arguments discussed in Chapter 3). This example also illustrates a major difficulty with the “iterative” versions of AR models. This model doesn’t produce forecasts for case_rate, and so, would not have data to “plug in” for the necessary lags.2\nAnother property of the basic model is the predictive interval. We describe this in more detail in a coming chapter, but it is easy to request multiple quantiles.\n\nout_q &lt;- arx_forecaster(jhu, \"death_rate\", c(\"case_rate\", \"death_rate\"),\n  args_list = arx_args_list(\n    levels = c(.01, .025, seq(.05, .95, by = .05), .975, .99)\n  )\n)\n\nThe column .pred_dstn in the predictions object is actually a “distribution” here parameterized by its quantiles. For this default forecaster, these are created using the quantiles of the residuals of the predictive model (possibly symmetrized). Here, we used 23 quantiles, but one can grab a particular quantile,\n\nhead(quantile(out_q$predictions$.pred_distn, p = .4))\n\n#&gt;        40%        40%        40%        40%        40%        40% \n#&gt; 0.30277798 0.27213225 0.44345734 0.03120647 0.56121844 0.27492711\n\n\nor extract the entire distribution into a “long” epi_df with tau being the probability and q being the value associated to that quantile.\n\nout_q$predictions %&gt;%\n  # first create a \"nested\" list-column\n  mutate(.pred_distn = nested_quantiles(.pred_distn)) %&gt;%\n  unnest(.pred_distn) # then unnest it\n\n#&gt; # A tibble: 1,288 × 6\n#&gt;   geo_value .pred      q   tau forecast_date target_date\n#&gt;   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;        &lt;date&gt;     \n#&gt; 1 ak        0.355 0      0.01  2021-12-31    2022-01-07 \n#&gt; 2 ak        0.355 0      0.025 2021-12-31    2022-01-07 \n#&gt; 3 ak        0.355 0.0371 0.05  2021-12-31    2022-01-07 \n#&gt; 4 ak        0.355 0.123  0.1   2021-12-31    2022-01-07 \n#&gt; 5 ak        0.355 0.174  0.15  2021-12-31    2022-01-07 \n#&gt; 6 ak        0.355 0.211  0.2   2021-12-31    2022-01-07 \n#&gt; # ℹ 1,282 more rows\n\n\nAdditional simple adjustments to the basic forecaster can be made using the function:\n\narx_args_list(\n  lags = c(0L, 7L, 14L), ahead = 7L, n_training = Inf,\n  forecast_date = NULL, target_date = NULL, levels = c(0.05, 0.95),\n  symmetrize = TRUE, nonneg = TRUE, quantile_by_key = \"geo_value\"\n)"
  },
  {
    "objectID": "epipredict.html#changing-the-engine",
    "href": "epipredict.html#changing-the-engine",
    "title": "8  Overview",
    "section": "8.6 Changing the engine",
    "text": "8.6 Changing the engine\nSo far, our forecasts have been produced using simple linear regression. But this is not the only way to estimate such a model. The trainer argument determines the type of model we want. This takes a {parsnip} model. The default is linear regression, but we could instead use a random forest with the {ranger} package:\n\nout_rf &lt;- arx_forecaster(\n  jhu, \"death_rate\", c(\"case_rate\", \"death_rate\"),\n  rand_forest(mode = \"regression\")\n)\n\nOr boosted regression trees with {xgboost}:\n\nout_gb &lt;- arx_forecaster(\n  jhu, \"death_rate\", c(\"case_rate\", \"death_rate\"),\n  boost_tree(mode = \"regression\", trees = 20)\n)\n\nOr quantile regression, using our custom forecasting engine quantile_reg():\n\nout_gb &lt;- arx_forecaster(\n  jhu, \"death_rate\", c(\"case_rate\", \"death_rate\"),\n  quantile_reg()\n)\n\nFWIW, this last case (using quantile regression), is not far from what the Delphi production forecast team used for its Covid forecasts over the past few years."
  },
  {
    "objectID": "epipredict.html#footnotes",
    "href": "epipredict.html#footnotes",
    "title": "8  Overview",
    "section": "",
    "text": "Our group has not prioritized these sorts of models for epidemic forecasting, but one could also integrate these methods into our framework.↩︎\nAn obvious fix is to instead use a VAR and predict both, but this would likely increase the variance of the model, and therefore, may lead to less accurate forecasts for the variable of interest.↩︎"
  },
  {
    "objectID": "forecast-framework.html#preprocessing",
    "href": "forecast-framework.html#preprocessing",
    "title": "9  Inner workings of the framework",
    "section": "9.1 Preprocessing",
    "text": "9.1 Preprocessing\nPreprocessing is accomplished through a recipe (imagine baking a cake) as provided in the {recipes} package. We’ve made a few modifications (to handle panel data) as well as added some additional options. The recipe gives a specification of how to handle training data. Think of it like a fancified formula that you would pass to lm(): y ~ x1 + log(x2). In general, there are 2 extensions to the formula that {recipes} handles:\n\nDoing transformations of both training and test data that can always be applied. These are things like taking the log of a variable, leading or lagging, filtering out rows, handling dummy variables, etc.\nUsing statistics from the training data to eventually process test data. This is a major benefit of {recipes}. It prevents what the tidy team calls “data leakage”. A simple example is centering a predictor by its mean. We need to store the mean of the predictor from the training data and use that value on the test data rather than accidentally calculating the mean of the test predictor for centering.\n\nA recipe is processed in 2 steps, first it is “prepped”. This calculates and stores any intermediate statistics necessary for use on the test data. Then it is “baked” resulting in training data ready for passing into a statistical model (like lm).\nWe have introduced an epi_recipe. It’s just a recipe that knows how to handle the time_value, geo_value, and any additional keys so that these are available when necessary.\nThe epi_recipe from out_gb can be extracted from the result:\n\nlibrary(workflows)\nlibrary(recipes)\nextract_recipe(out_gb$epi_workflow)\n\n\n#&gt; \n#&gt; ── Recipe ─────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; raw:        2\n#&gt; geo_value:  1\n#&gt; time_value: 1\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 1736 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Lagging: case_rate by 0, 7, 14 | Trained\n#&gt; • Lagging: death_rate by 0, 7, 14 | Trained\n#&gt; • Leading: death_rate by 7 | Trained\n#&gt; • Removing rows with NA values in: lag_0_case_rate, ... | Trained\n#&gt; • Removing rows with NA values in: ahead_7_death_rate | Trained\n#&gt; • # of recent observations per key limited to:: Inf | Trained\n\nThe “Inputs” are the original epi_df and the “roles” that these are assigned. None of these are predictors or outcomes. Those will be created by the recipe when it is prepped. The “Operations” are the sequence of instructions to create the cake (baked training data). Here we create lagged predictors, lead the outcome, and then remove NAs. Some models like lm internally handle NAs, but not everything does, so we deal with them explicitly. The code to do this (inside the forecaster) is\n\ner &lt;- epi_recipe(jhu) %&gt;%\n  step_epi_lag(case_rate, death_rate, lag = c(0, 7, 14)) %&gt;%\n  step_epi_ahead(death_rate, ahead = 7) %&gt;%\n  step_epi_naomit()\n\nWhile {recipes} provides a function step_lag(), it assumes that the data have no breaks in the sequence of time_values. This is a bit dangerous, so we avoid that behaviour. Our lag/ahead functions also appropriately adjust the amount of data to avoid accidentally dropping recent predictors from the test data."
  },
  {
    "objectID": "forecast-framework.html#the-model-specification",
    "href": "forecast-framework.html#the-model-specification",
    "title": "9  Inner workings of the framework",
    "section": "9.2 The model specification",
    "text": "9.2 The model specification\nUsers familiar with the {parsnip} package will have no trouble here. Basically, {parsnip} unifies the function signature across statistical models. For example, lm() “likes” to work with formulas, but glmnet::glmnet() uses x and y for predictors and response. {parsnip} is agnostic. Both of these do “linear regression”. Above we switched from lm() to xgboost() without any issue despite the fact that these functions couldn’t be more different.\n\nlm(\n  formula, data, subset, weights, na.action,\n  method = \"qr\",\n  model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE,\n  contrasts = NULL, offset, ...\n)\n\nxgboost(\n  data = NULL, label = NULL, missing = NA, weight = NULL,\n  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(),\n  ...\n)\n\n{epipredict} provides a few engines/modules like flatline() and quantile_reg() to power the flatline_forecaster() and provide quantile regression, but you should be able to use almost any available models listed here.\nTo estimate (fit) a preprocessed model, one calls fit() on the epi_workflow.\n\newf &lt;- epi_workflow(er, linear_reg()) %&gt;% fit(jhu)"
  },
  {
    "objectID": "forecast-framework.html#predicting-and-postprocessing-bound-together",
    "href": "forecast-framework.html#predicting-and-postprocessing-bound-together",
    "title": "9  Inner workings of the framework",
    "section": "9.3 Predicting and Postprocessing (bound together)",
    "text": "9.3 Predicting and Postprocessing (bound together)\nTo stretch the metaphor of preparing a cake to its natural limits, we have created postprocessing functionality called “frosting”. Much like the recipe, each postprocessing operation is a “layer” and we “slather” these onto our baked cake. To fix ideas, below is the postprocessing frosting for arx_forecaster()\n\nextract_frosting(out_gb$epi_workflow)\n\n\n#&gt; \n#&gt; ── Frosting ─────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Layers\n#&gt; • Creating predictions: \"&lt;calculated&gt;\"\n#&gt; • Resampling residuals for predictive quantiles: \"&lt;calculated&gt;\" levels 0.05,\n#&gt;   0.95\n#&gt; • Adding forecast date: \"2021-12-31\"\n#&gt; • Adding target date: \"2022-01-07\"\n#&gt; • Thresholding predictions: dplyr::starts_with(\".pred\") to ]0, Inf)\n\nHere we have 5 layers of frosting. The first generates the forecasts from the test data. The second uses quantiles of the residuals to create distributional forecasts. The next two add columns for the date the forecast was made and the date for which it is intended to occur. Because we are predicting rates, they should be non-negative, so the last layer thresholds both predicted values and intervals at 0. The code to do this (inside the forecaster) is\n\nf &lt;- frosting() %&gt;%\n  layer_predict() %&gt;%\n  layer_residual_quantiles(\n    probs = c(.01, .025, seq(.05, .95, by = .05), .975, .99),\n    symmetrize = TRUE\n  ) %&gt;%\n  layer_add_forecast_date() %&gt;%\n  layer_add_target_date() %&gt;%\n  layer_threshold(starts_with(\".pred\"))\n\nAt predict time, we add this object onto the epi_workflow and call predict()\n\ntest_data &lt;- get_test_data(er, jhu)\newf %&gt;%\n  add_frosting(f) %&gt;%\n  predict(test_data)\n\n#&gt; An `epi_df` object, 56 x 6 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25.791826\n#&gt; \n#&gt; # A tibble: 56 × 6\n#&gt;   geo_value time_value  .pred         .pred_distn forecast_date target_date\n#&gt; * &lt;chr&gt;     &lt;date&gt;      &lt;dbl&gt;              &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n#&gt; 1 ak        2021-12-31 0.355  [0.01, 0.99]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 2 al        2021-12-31 0.325  [0.01, 0.99]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 3 ar        2021-12-31 0.496  [0.01, 0.99]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 4 as        2021-12-31 0.0836 [0.01, 0.99]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 5 az        2021-12-31 0.614  [0.01, 0.99]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 6 ca        2021-12-31 0.327  [0.01, 0.99]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; # ℹ 50 more rows\n\n\nThe above get_test_data() function examines the recipe and ensures that enough test data is available to create the necessary lags and produce a prediction for the desired future time point (after the end of the training data). This mimics what would happen if jhu contained the most recent available historical data and we wanted to actually predict the future. We could have instead used any test data that contained the necessary predictors.\n\n\n\n\n\n\nNote\n\n\n\nIn the predictions above, you’ll see a time_value column. That’s because we could use any training data. We happened to use training data corresponding to the most recent available, and it’s lags. But we could have instead used last week’s or we could use the data that arrives next year, or we could use multiple time_values for multiple locations. This is completely allowed, though not necessarily what you expect.\nIn production forecasting, you’d probably reestimate the model and produce new predictions whenever new data arrives. This is exactly what all the canned forecasters we provide do. So those strip out the time_value column.\nBut the next most likely procedure would be to feed your previously estimated model (without refitting) the new data. To do this, you’d just call get_test_data() on that new data. And the time_value would still be the same as your forecast_date.\nGetting many forecasts (multiple time_values) for each location, is not exactly a typical desire in this context. But it’s also not unheard of, so it is possible (and analogous to standard, non-time series forecasting)."
  },
  {
    "objectID": "forecast-framework.html#conclusion",
    "href": "forecast-framework.html#conclusion",
    "title": "9  Inner workings of the framework",
    "section": "9.4 Conclusion",
    "text": "9.4 Conclusion\nInternally, we provide some canned forecaster functions to create reasonable forecasts. But ideally, a user could create their own forecasters by building up the components we provide. In other chapters, we try to walk through some of these customizations.\nTo illustrate everything above, here is (roughly) the code for the arx_forecaster() to predict the death rate, 1 week ahead:\n\nr &lt;- epi_recipe(jhu) %&gt;%\n  step_epi_ahead(death_rate, ahead = 7) %&gt;%\n  step_epi_lag(case_rate, death_rate, lag = c(0, 7, 14)) %&gt;%\n  step_epi_naomit()\n\nlatest &lt;- get_test_data(r, jhu)\n\nf &lt;- frosting() %&gt;%\n  layer_predict() %&gt;%\n  layer_residual_quantiles() %&gt;%\n  layer_add_forecast_date() %&gt;%\n  layer_add_target_date() %&gt;%\n  layer_threshold(starts_with(\".pred\"))\n\neng &lt;- linear_reg()\nwf &lt;- epi_workflow(r, eng, f) %&gt;% fit(jhu)\npreds &lt;- predict(wf, latest)\n\nThe code for arx_forecaster() simply generalizes this, passing along arguments as needed.\n\npreds\n\n#&gt; An `epi_df` object, 56 x 6 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25.791826\n#&gt; \n#&gt; # A tibble: 56 × 6\n#&gt;   geo_value time_value .pred         .pred_distn forecast_date target_date\n#&gt; * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;              &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n#&gt; 1 ak        2021-12-31  36.4 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 2 al        2021-12-31  89.9 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 3 ar        2021-12-31  82.6 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 4 as        2021-12-31   0   [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 5 az        2021-12-31  58.3 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 6 ca        2021-12-31  84.4 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; # ℹ 50 more rows"
  },
  {
    "objectID": "flatline-forecaster.html#example-of-using-the-flatline-forecaster",
    "href": "flatline-forecaster.html#example-of-using-the-flatline-forecaster",
    "title": "10  Introducing the flatline forecaster",
    "section": "10.1 Example of using the flatline forecaster",
    "text": "10.1 Example of using the flatline forecaster\nWe will continue to use the case_death_rate_subset dataset that comes with the epipredict package. In brief, this is a subset of the JHU daily COVID-19 cases and deaths by state. While this dataset ranges from Dec 31, 2020 to Dec 31, 2021, we will only consider a small subset at the end of that range to keep our example relatively simple.\n\njhu &lt;- case_death_rate_subset %&gt;%\n  dplyr::filter(time_value &gt;= as.Date(\"2021-09-01\"))\n\njhu\n\n#&gt; An `epi_df` object, 6,832 x 4 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25.791826\n#&gt; \n#&gt; # A tibble: 6,832 × 4\n#&gt;   geo_value time_value case_rate death_rate\n#&gt; * &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 ak        2021-09-01      75.3      0.198\n#&gt; 2 al        2021-09-01     113.       0.845\n#&gt; 3 ar        2021-09-01      68.5      0.919\n#&gt; 4 as        2021-09-01       0        0    \n#&gt; 5 az        2021-09-01      48.8      0.414\n#&gt; 6 ca        2021-09-01      38.4      0.246\n#&gt; # ℹ 6,826 more rows\n\n\n\n10.1.1 The basic mechanics of the flatline forecaster\nThe simplest way to create and train a flatline forecaster to predict the d eath rate one week into the future, is to input the epi_df and the name of the column from it that we want to predict in the flatline_forecaster function.\n\none_week_ahead &lt;- flatline_forecaster(jhu, outcome = \"death_rate\")\none_week_ahead\n\n#&gt; \n#&gt; ══ A basic forecaster of type flatline ══════════════════════════════════════\n#&gt; \n#&gt; This forecaster was fit on 2023-06-19 20:31:13\n#&gt; \n#&gt; Training data was an `epi_df` with\n#&gt; • Geography: state,\n#&gt; • Time type: day,\n#&gt; • Using data up-to-date as of: 2022-05-31 12:08:25.\n#&gt; \n#&gt; ── Predictions ──────────────────────────────────────────────────────────────\n#&gt; \n#&gt; A total of 56 predictions are available for\n#&gt; • 56 unique geographic regions,\n#&gt; • At forecast dates: 2021-12-31,\n#&gt; • For target dates: 2022-01-07.\n\n\nThe result is both a fitted model object which could be used any time in the future to create different forecasts, as well as a set of predicted values and prediction intervals for each location 7 days after the last available time value in the data, which is Dec 31, 2021. Note that 7 days is the default number of time steps ahead of the forecast date in which forecasts should be produced. To change this, you must change the value of the ahead parameter in the list of additional arguments flatline_args_list(). Let’s change this to 5 days to get some practice.\n\nfive_days_ahead &lt;- flatline_forecaster(\n  jhu,\n  outcome = \"death_rate\",\n  flatline_args_list(ahead = 5L)\n)\n\nfive_days_ahead\n\n#&gt; \n#&gt; ══ A basic forecaster of type flatline ══════════════════════════════════════\n#&gt; \n#&gt; This forecaster was fit on 2023-06-19 20:31:14\n#&gt; \n#&gt; Training data was an `epi_df` with\n#&gt; • Geography: state,\n#&gt; • Time type: day,\n#&gt; • Using data up-to-date as of: 2022-05-31 12:08:25.\n#&gt; \n#&gt; ── Predictions ──────────────────────────────────────────────────────────────\n#&gt; \n#&gt; A total of 56 predictions are available for\n#&gt; • 56 unique geographic regions,\n#&gt; • At forecast dates: 2021-12-31,\n#&gt; • For target dates: 2022-01-05.\n\n\nWe could also specify that we want a 80% predictive interval by changing the levels. The default 0.05 and 0.95 levels/quantiles give us 90% predictive interval.\n\nfive_days_ahead &lt;- flatline_forecaster(\n  jhu,\n  outcome = \"death_rate\",\n  flatline_args_list(ahead = 5L, levels = c(0.1, 0.9))\n)\n\nfive_days_ahead\n\n#&gt; \n#&gt; ══ A basic forecaster of type flatline ══════════════════════════════════════\n#&gt; \n#&gt; This forecaster was fit on 2023-06-19 20:31:14\n#&gt; \n#&gt; Training data was an `epi_df` with\n#&gt; • Geography: state,\n#&gt; • Time type: day,\n#&gt; • Using data up-to-date as of: 2022-05-31 12:08:25.\n#&gt; \n#&gt; ── Predictions ──────────────────────────────────────────────────────────────\n#&gt; \n#&gt; A total of 56 predictions are available for\n#&gt; • 56 unique geographic regions,\n#&gt; • At forecast dates: 2021-12-31,\n#&gt; • For target dates: 2022-01-05.\n\n\nTo see the other arguments that you may modify, please see ?flatline_args_list(). For now, we will move on to looking at the workflow.\n\nfive_days_ahead$epi_workflow\n\n#&gt; ══ Epi Workflow [trained] ═══════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; Postprocessor: Frosting\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 2 Recipe Steps\n#&gt; \n#&gt; • step_epi_ahead()\n#&gt; • step_training_window()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Flatline forecaster\n#&gt; \n#&gt; Predictions produced by geo_value resulting in 56 total forecasts.\n#&gt; A total of 7112 residuals are available from the training set.\n#&gt; \n#&gt; ── Postprocessor ────────────────────────────────────────────────────────────\n#&gt; 5 Frosting Layers\n#&gt; \n#&gt; • layer_predict()\n#&gt; • layer_residual_quantiles()\n#&gt; • layer_add_forecast_date()\n#&gt; • layer_add_target_date()\n#&gt; • layer_threshold()\n\n\nThe fitted model here was based on minimal pre-processing of the data, estimating a flatline model, and then post-processing the results to be meaningful for epidemiological tasks. To look deeper into the pre-processing, model and processing parts individually, you may use the $ operator after epi_workflow. For example, let’s examine the pre-processing part in more detail.\n\nlibrary(workflows)\nextract_preprocessor(five_days_ahead$epi_workflow)\n\n\n#&gt; \n#&gt; ── Recipe ───────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; predictor:  3\n#&gt; geo_value:  1\n#&gt; raw:        1\n#&gt; time_value: 1\n#&gt; \n#&gt; ── Operations\n#&gt; • Leading: death_rate by 5\n#&gt; • # of recent observations per key limited to:: Inf\n\nUnder Operations, we can see that the pre-processing operations were to lead the death rate by 5 days (step_epi_ahead()) and that the # of recent observations used in the training window were not limited (in step_training_window() as n_training = Inf in flatline_args_list()). You should also see the molded/pre-processed training data.\nFor symmetry, let’s have a look at the post-processing.\n\nextract_frosting(five_days_ahead$epi_workflow)\n\n\n#&gt; \n#&gt; ── Frosting ─────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Layers\n#&gt; • Creating predictions: \"&lt;calculated&gt;\"\n#&gt; • Resampling residuals for predictive quantiles: \"&lt;calculated&gt;\" levels 0.1,\n#&gt;   0.9\n#&gt; • Adding forecast date: \"2021-12-31\"\n#&gt; • Adding target date: \"2022-01-05\"\n#&gt; • Thresholding predictions: dplyr::starts_with(\".pred\") to ]0, Inf)\n\nThe post-processing operations in the order the that were performed were to create the predictions and the predictive intervals, add the forecast and target dates and bound the predictions at zero.\nWe can also easily examine the predictions themselves.\n\nfive_days_ahead$predictions\n\n#&gt; # A tibble: 56 × 5\n#&gt;   geo_value  .pred       .pred_distn forecast_date target_date\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;            &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n#&gt; 1 ak        0.0395 [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 2 al        0.107  [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 3 ar        0.490  [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 4 as        0      [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 5 az        0.608  [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 6 ca        0.142  [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; # ℹ 50 more rows\n\n\nThe results above show a distributional forecast produced using data through the end of 2021 for the January 5, 2022. A prediction for the death rate per 100K inhabitants along with a 95% predictive interval is available for every state (geo_value).\nThe figure below displays the prediction and prediction interval for three sample states: Arizona, New York, and Florida.\n\n\nCode\nsamp_geos &lt;- c(\"az\", \"ny\", \"fl\")\n\nhist &lt;- jhu %&gt;%\n  filter(geo_value %in% samp_geos)\n\npreds &lt;- five_days_ahead$predictions %&gt;%\n  filter(geo_value %in% samp_geos) %&gt;%\n  mutate(q = nested_quantiles(.pred_distn)) %&gt;%\n  unnest(q) %&gt;%\n  pivot_wider(names_from = tau, values_from = q)\n\nggplot(hist, aes(color = geo_value)) +\n  geom_line(aes(time_value, death_rate)) +\n  theme_bw() +\n  geom_errorbar(data = preds, aes(x = target_date, ymin = `0.1`, ymax = `0.9`)) +\n  geom_point(data = preds, aes(target_date, .pred)) +\n  geom_vline(data = preds, aes(xintercept = forecast_date)) +\n  scale_colour_viridis_d(name = \"\") +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"1 month\") +\n  facet_grid(geo_value ~ ., scales = \"free_y\") +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"Incident deaths per 100K\\n inhabitants\")\n\n\n\n\n\n\n\n\n\nThe vertical black line is the forecast date. Here the forecast seems pretty reasonable based on the past observations shown. In cases where the recent past is highly predictive of the near future, a simple flatline forecast may be respectable, but in more complex situations where there is more uncertainty of what’s to come, the flatline forecaster may be best relegated to being a baseline model and nothing more.\nTake for example what happens when we consider a wider range of target dates. That is, we will now predict for several different horizons or ahead values - in our case, 5 to 25 days ahead, inclusive. Since the flatline forecaster function forecasts at a single unique ahead value, we can use the map() function from purrr to apply the forecaster to each ahead value we want to use. Then, we row bind the list of results.\n\nout_df &lt;- map(1:28, ~ flatline_forecaster(\n  epi_data = jhu,\n  outcome = \"death_rate\",\n  args_list = flatline_args_list(ahead = .x)\n)$predictions) %&gt;%\n  list_rbind()\n\nThen, we proceed as we did before. The only difference from before is that we’re using out_df where we had five_days_ahead$predictions.\n\n\nCode\npreds &lt;- out_df %&gt;%\n  filter(geo_value %in% samp_geos) %&gt;%\n  mutate(q = nested_quantiles(.pred_distn)) %&gt;%\n  unnest(q) %&gt;%\n  pivot_wider(names_from = tau, values_from = q)\n\nggplot(hist) +\n  geom_line(aes(time_value, death_rate)) +\n  geom_ribbon(\n    data = preds,\n    aes(x = target_date, ymin = `0.05`, ymax = `0.95`, fill = geo_value)\n  ) +\n  geom_point(data = preds, aes(target_date, .pred, colour = geo_value)) +\n  geom_vline(data = preds, aes(xintercept = forecast_date)) +\n  scale_colour_viridis_d() +\n  scale_fill_viridis_d(alpha = .4) +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"1 month\") +\n  scale_y_continuous(expand = expansion(c(0, .05))) +\n  facet_grid(geo_value ~ ., scales = \"free_y\") +\n  labs(x = \"\", y = \"Incident deaths per 100K\\n inhabitants\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nNow, you can really see the flat line trend in the predictions. And you may also observe that as we get further away from the forecast date, the more unnerving using a flatline prediction becomes. It feels increasingly unnatural.\nSo naturally the choice of forecaster relates to the time frame being considered. In general, using a flatline forecaster makes more sense for short-term forecasts than for long-term forecasts and for periods of great stability than in less stable times. Realistically, periods of great stability are rare. Moreover, in our model of choice we want to take into account more information about the past than just what happened at the most recent time point. So simple forecasters like the flatline forecaster don’t cut it as actual contenders in many real-life situations. However, they are not useless, just used for a different purpose. A simple model is often used to compare a more complex model to, which is why you may have seen such a model used as a baseline in the COVID Forecast Hub. The following blog post from Delphi explores the Hub’s ensemble accuracy relative to such a baseline model."
  },
  {
    "objectID": "flatline-forecaster.html#what-weve-learned-in-a-nutshell",
    "href": "flatline-forecaster.html#what-weve-learned-in-a-nutshell",
    "title": "10  Introducing the flatline forecaster",
    "section": "10.2 What we’ve learned in a nutshell",
    "text": "10.2 What we’ve learned in a nutshell\nThough the flatline forecaster is a very basic model with limited customization, it is about as steady and predictable as a model can get. So it provides a good reference or baseline to compare more complicated models to."
  },
  {
    "objectID": "tidymodels-intro.html#an-example-using-the-penguins-dataset",
    "href": "tidymodels-intro.html#an-example-using-the-penguins-dataset",
    "title": "11  Introduction to Tidymodels",
    "section": "11.1 An example using the penguins dataset",
    "text": "11.1 An example using the penguins dataset\nWe will now explore the tidymodels functions using the penguins dataset that we introduced and used in Regression in Tidymodels.\n\n11.1.1 Load packages\nNote that tidymodels automatically loads some very useful tidyverse packages for us, including fan favourites like dplyr and ggplot2.\n\nlibrary(tidymodels)\n\n\n\n11.1.2 Simplify dataset\nTo keep the focus on learning how to use tidymodels, we will work with a simplified version of the dataset in which we will only use the complete cases/rows in the penguins dataset\n\npenguins &lt;- penguins %&gt;%\n  filter(complete.cases(.))\n\nhead(penguins)\n\n#&gt; # A tibble: 6 × 7\n#&gt;   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#&gt;   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n#&gt; 1 Adelie  Torgersen           39.1          18.7               181\n#&gt; 2 Adelie  Torgersen           39.5          17.4               186\n#&gt; 3 Adelie  Torgersen           40.3          18                 195\n#&gt; 4 Adelie  Torgersen           36.7          19.3               193\n#&gt; 5 Adelie  Torgersen           39.3          20.6               190\n#&gt; 6 Adelie  Torgersen           38.9          17.8               181\n#&gt; # ℹ 2 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;\n\n\nand we will only use the species, bill_length_mm, bill_depth_mm, and flipper_length_mm variables.\n\npenguins &lt;- penguins %&gt;%\n  select(c(species, bill_length_mm, bill_depth_mm, flipper_length_mm))\n\n\n\n11.1.3 Data sampling\nAfter fitting a model, make sure it is a good model. That is, don’t forget to test how the model performs. For this reason, it is customary to split data into distinct training and test sets at the onset. The training data is used to fit the model and the test data is used to assess model performance.\nThe initial_split() function from the rsample package is what we will use to split our dataset into a training and test set. The function by default uses 3/4 of data for training and reserves the remaining 1/4 for testing. Use the prop argument to change the proportion used for training. Note that this function gives a rsplit object and not a data frame and the output of the object shows the number of rows used for testing, training and the grand total.\n\nset.seed(123) # For reproduciblity, as when we split the data below\npenguins_split &lt;- initial_split(penguins, prop = 0.7)\npenguins_split\n\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;233/100/333&gt;\n\n\nTo see what observations were used for training and testing, use the training() and testing() functions respectively.\n\npenguins_split %&gt;%\n  training() %&gt;%\n  glimpse()\n\n#&gt; Rows: 233\n#&gt; Columns: 4\n#&gt; $ species           &lt;fct&gt; Gentoo, Adelie, Gentoo, Chinstrap, Adelie, Chinst…\n#&gt; $ bill_length_mm    &lt;dbl&gt; 59.6, 34.4, 45.2, 49.0, 41.4, 51.0, 44.9, 51.1, 5…\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 17.0, 18.4, 15.8, 19.5, 18.5, 18.8, 13.8, 16.5, 1…\n#&gt; $ flipper_length_mm &lt;int&gt; 230, 184, 215, 210, 202, 203, 212, 225, 210, 211,…\n\n\nNow, we’ll create a data frame for each of the training and test set:\n\ntrain_data &lt;- training(penguins_split)\ntest_data &lt;- testing(penguins_split)\n\n\n\n11.1.4 Pre-processing\nThe main goal of this step is to use data transformations to make the data suitable for modeling. Most transformations that one required for standard data analysis tasks can be achieved by dplyr, or another tidyverse package.\n\n11.1.4.1 The pre-processing interface\nBefore training the model, a recipe can be used to carry out the pre-processing required by the model.\nThe recipe() has two main arguments: a formula (with the same format as when doing [LINK TO VIGNETTE]) and a data argument, which is usually the training set as that is the data used to create the model. Hence, we have data = train_data here.\nIn our example, suppose that our goal is to predict penguin species from bill length, bill depth and flipper length, then our recipe function would look as follows:\n\nrecipe(species ~ ., data = train_data)\n\nThe point of recipe is to be a more general purpose formula. A number of packages are not formula-based. The ever-popular glmnet() function is one example because it takes in matrices for the x and y variables instead of a formula. So a recipe is useful because you can use a package like glmnet by following the same standard formula-based recipe format and simply specify later on in the modelling stage that the you would like to use glmnet.\nNow, after saying that you are making a recipe by way of the recipe() function, simply specify the transformations that you want to apply to your data in the necessary steps. Each data transformation is one step and all of the available pre-processing transformations all have the prefix of step_. Now, while there are many step functions available (here’s a list), we will only use the following three in our example.\n\nstep_corr() to remove variables which have large absolute correlations with others\nstep_center() to normalize numeric data to have a mean of zero\nstep_scale() to normalize numeric data to have a standard deviation of one\n\nOne of the advantages of having these pre-processing steps is that they help to simplify concepts that are difficult or a pain to enforce in coding. For example, centering could be a nuisance to implement from scratch because we would first have to calculate statistics (variable averages) from the training data and then use them on both the training and on the test data. Note that centering should not be done on the test data, rather on the training data to avoid data leakage (contamination of the test data by using statistics from the test data). In a recipe, the the estimation of the variable means using the training data and the application of these to center new data sets is done automatically, under the hood, and so spares the coder from having to manually implement it. The situation is similar for scaling numeric data (step_scale()).\nAnother useful feature of the tidymodels pre-processing interface is that each step can be applied to one specified variable, a group of variables, or all variables. The all_predictors() and all_outcomes() functions are particularly convenient to help minimize the amount of typing you need to do. For instance, if you wanted to apply step_center() to only the predictor variables, simply type step_center(all_predictors()) instead of listing out each and every predictor in the step function.\nNow, let’s try this all out on our example.\n\npenguins_recipe &lt;- recipe(species ~ ., data = train_data) %&gt;%\n  step_corr(all_predictors()) %&gt;%\n  step_center(all_predictors(), -all_outcomes()) %&gt;%\n  step_scale(all_predictors(), -all_outcomes())\n\nTo summarize, we obtained a recipe object, penguins_recipe, by putting the recipe() and step functions together on our training data that we had ready to go from sampling.\nNow, to get the recipe details, simply call the recipe object. The operations section details what pre-processing steps we’ve applied to the data. Notice that the steps shown here are in the order that they were input into the recipe and they specify the variables used in each step.\n\npenguins_recipe\n\n\n\n\n11.1.5 Model Training\nRecall that in R, the same type of model could be fit using several different packages, and each such package typically has it’s own style of interface. Two popular packages to fit random forest models are ranger and randomForest. One way that their interfaces differ is in the parameter name for the number of trees - ranger() has the parameter num.trees, whereas in randomForest has parameter ntree. Such differences do not make it simple to run the model in the other package.\nTidymodels created an single interface that supports the usage of both models. Moreover, this general interface supports an even wider range of functions that use perform random forest. The key part that points to the function and package to be used is the engine.\nLet’s see how this works in practice. In the below example, we’ll use the general rand_forest() function from tidymodels. In there, we can specify the number of trees by using the trees argument. Then, in set_engine() we specify that we want to use ranger’s version of random forest. Notice this follows the model specification format introduced in the [Regression in Tidymodels] chapter.\n\npenguins_ranger &lt;- rand_forest(trees = 100, mode = \"classification\") %&gt;%\n  set_engine(\"ranger\")\n\nNow, if we wanted to use a different package’s version of random forest, we could easily do that by simply swapping out the engine. To try this out, let’s use randomForest instead of ranger.\n\npenguins_rf &lt;- rand_forest(trees = 100, mode = \"classification\") %&gt;%\n  set_engine(\"randomForest\")\n\nFor the remainder of this tutorial, we’ll stick with using ranger for simplify. At this stage, we’re ready to pre-process and model. The first task of those two is to apply our recipe before we train and test our model, in that we must\n\nProcess the recipe using the training set.\nUse the recipe on the training set to get the finalized predictor set.\nUse the recipe on the predictor set to get the test set.\n\nA workflow can be used to pair model and processing tasks together. When different recipes are needed for different models, this is very useful so that you don’t have to keep track of separate model and recipe objects in your workspace. Hence, training and testing different workflows becomes easier.\nFor our example, we’ll try tidy model’s workflows package to pair our model and our recipe together.\n\npenguins_wflow &lt;- workflow() %&gt;%\n  add_model(penguins_ranger) %&gt;%\n  add_recipe(penguins_recipe)\n\npenguins_wflow\n\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 3 Recipe Steps\n#&gt; \n#&gt; • step_corr()\n#&gt; • step_center()\n#&gt; • step_scale()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 100\n#&gt; \n#&gt; Computational engine: ranger\n\n\nAfter that, we’re ready to fit the model to our training data. The fit() function is what we will use to prepare the the recipe and train the model from the finalized predictors.\n\npenguins_fit &lt;- penguins_wflow %&gt;% fit(data = train_data)\n\nThe resulting object contains both the recipe and fitted model. To extract the model, use the helper function of extract_fit_parsnip(), and to extract the recipe object, use extract_recipe(). We extract the model object below.\n\nextract_fit_parsnip(penguins_fit)\n\n#&gt; parsnip model object\n#&gt; \n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~100,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  100 \n#&gt; Sample size:                      233 \n#&gt; Number of independent variables:  3 \n#&gt; Mtry:                             1 \n#&gt; Target node size:                 10 \n#&gt; Variable importance mode:         none \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.02954337\n\n\nOne important thing to notice is that that if we wanted to use the randomForest model instead of the ranger model, all we’d need to do is replace the engine in the model specification; the rest of the code remains the same. We shall leave it to the reader to try this on their own and marvel at the beauty of having such a unifying interface.\n\n\n11.1.6 Use a trained workflow to predict\nUp to this point we have\n\nBuilt the model (penguins_ranger)\nCreated a pre-processing recipe (penguins_recipe),\nPaired the model and recipe (penguins_wflow), and\nTrained our workflow using fit().\n\nSo the next step is to use the trained workflow, penguins_fit, to predict with the test data. This is easily done with a call to predict().\n\npredict(penguins_fit, test_data)\n\n#&gt; # A tibble: 100 × 1\n#&gt;   .pred_class\n#&gt;   &lt;fct&gt;      \n#&gt; 1 Adelie     \n#&gt; 2 Adelie     \n#&gt; 3 Adelie     \n#&gt; 4 Chinstrap  \n#&gt; 5 Adelie     \n#&gt; 6 Adelie     \n#&gt; # ℹ 94 more rows\n\n\nIf you wanted to obtain a probability for each predicted value, then simply set the type = prob in predict(). This will yield a tibble with one column per outcome type and the corresponding predicted probability for each value to be each type of outcome. Then, to add the predicted values as a new column on the test data, use the bind_cols() function from dplyr.\n\npenguins_predp &lt;- penguins_fit %&gt;%\n  predict(test_data, type = \"prob\")\n\nTo add the predicted values as a new column on the test data, you can use the bind_cols() function from dplyr.\n\nbind_cols(test_data, penguins_predp) %&gt;%\n  head() # View first six rows of output\n\n#&gt; # A tibble: 6 × 7\n#&gt;   species bill_length_mm bill_depth_mm flipper_length_mm .pred_Adelie\n#&gt;   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;        &lt;dbl&gt;\n#&gt; 1 Adelie            39.5          17.4               186        0.910\n#&gt; 2 Adelie            40.3          18                 195        0.960\n#&gt; 3 Adelie            38.7          19                 195        0.964\n#&gt; 4 Adelie            46            21.5               194        0.286\n#&gt; 5 Adelie            35.9          19.2               189        0.997\n#&gt; 6 Adelie            38.2          18.1               185        1    \n#&gt; # ℹ 2 more variables: .pred_Chinstrap &lt;dbl&gt;, .pred_Gentoo &lt;dbl&gt;\n\n\nAlternatively, we can use the augment() function to obtain the predicted probabilities and add them to the test data in a one-liner.\n\npenguins_aug &lt;- augment(penguins_fit, test_data)\n\npenguins_aug\n\n#&gt; # A tibble: 100 × 8\n#&gt;   species bill_length_mm bill_depth_mm flipper_length_mm .pred_class\n#&gt;   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt; &lt;fct&gt;      \n#&gt; 1 Adelie            39.5          17.4               186 Adelie     \n#&gt; 2 Adelie            40.3          18                 195 Adelie     \n#&gt; 3 Adelie            38.7          19                 195 Adelie     \n#&gt; 4 Adelie            46            21.5               194 Chinstrap  \n#&gt; 5 Adelie            35.9          19.2               189 Adelie     \n#&gt; 6 Adelie            38.2          18.1               185 Adelie     \n#&gt; # ℹ 94 more rows\n#&gt; # ℹ 3 more variables: .pred_Adelie &lt;dbl&gt;, .pred_Chinstrap &lt;dbl&gt;, …\n\n\nWe can see from the first couple of rows shown that our model predicted the species correctly to be Adelie (in the .pred_class column) because the .pred_Adelie probabilities are by far the largest of the three predicted probabilities for each row. So while we can informally say that our model is doing well for predicting, how can we formally assess this? We would like to calculate a metric (well, probably more than one) to tell us how well our model predicts the species of penguins.\n\n\n11.1.7 Model Validation\nThe metrics() function from the yardstick package is helps to assess model performance. As suggested by its name, it will output some metrics, and as an added bonus, these will be automatically selected for the type of model that you construct. The input for this function is a tibble that contains the actual values and the predicted values. This way we can compare how close the model estimates are to the truth. To serve this purpose, we can use penguins_aug.\n\npenguins_aug %&gt;%\n  metrics(truth = species, .pred_Adelie:.pred_Gentoo, estimate = .pred_class)\n\n#&gt; # A tibble: 4 × 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy    multiclass     0.97 \n#&gt; 2 kap         multiclass     0.954\n#&gt; 3 mn_log_loss multiclass     0.123\n#&gt; 4 roc_auc     hand_till      0.993\n\n\nLet’s briefly go through the metrics that were generated. Accuracy is simply the proportion of values that are predicted correctly, while kappa is similar to accuracy, but is normalized by the accuracy that would be expected by chance (you can think of it as a measure that compares observed accuracy to expected accuracy from random chance alone). For our example, both the accuracy and kappa value estimates are extremely high (near to the upper limit of 1) and similar in value, indicating that our model performs very well for prediction on the test data. Log loss is a measure of the performance of a classification model and a perfect model has a log loss of 0, so our model performs pretty well in that respect. Finally, roc_auc is the area under ROC curve and we’ll explain this very shortly so stay tuned (for now, just note that a value close to 1, like we have, is the goal). All in all, our model fairs very well.\nSince it is often not enough to rely purely on one number summaries of model performance, we’ll also look to graphical, curve-based metrics. We’ll walk through producing the classic ROC curve, which is computed using roc_curve() and roc_auc() from yardstick.\nTo get ourselves an ROC curve, we need to input the actual values and the columns of predicted class probabilities into roc_curve(). We finish off by piping into the autoplot() function.\n\npenguins_aug %&gt;%\n  roc_curve(truth = species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\nNotice that the x-axis displays 1 - specificity, which is otherwise known as the false positive rate. So on this plot, we can visualize the trade-off between the false positive (1 - specificity) and the true positive (sensitivity) rates. Since the best classification would be where all positives are correctly classified as positive (sensitivity = 1), and no negatives are incorrect classified as positive (specificity = 0), curves closer to the top left corner (and, hence, an area under the curve of about 1) is what we’re hoping for.\nSo, we can see that the curves for each of our species are looking pretty close to perfection (save for Adelie, which still does very well). To estimate the area under the curves, we can use roc_auc (or look to the summary of our metrics above for this very value).\n\npenguins_aug %&gt;%\n  roc_auc(truth = species, .pred_Adelie:.pred_Gentoo)\n\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc hand_till      0.993\n\n\nAs expected, the estimated area is very close to 1, indicating near-perfect discrimination.\nThe yardstick package also offers other standard tools for model assessment like a confusion matrix, from which we can inspect the counts of correct classifications and miclassifications.\n\npenguins_aug %&gt;%\n  conf_mat(truth = species, estimate = .pred_class)\n\n#&gt;            Truth\n#&gt; Prediction  Adelie Chinstrap Gentoo\n#&gt;   Adelie        40         0      0\n#&gt;   Chinstrap      3        24      0\n#&gt;   Gentoo         0         0     33\n\n\nWe could even combine this with autoplot() to get a nice heatmap visualization.\n\npenguins_aug %&gt;%\n  conf_mat(truth = species, estimate = .pred_class) %&gt;%\n  autoplot(\"heatmap\")\n\n\n\n\n\n\n\n\nThe diagonal shows the counts of correct predictions for each species, while the off-diagonal shows the counts of model misclassifications. As the metrics have indicated, our model performed magnificently on the test set as there was only three misclassifications of Adelie penguins as Chinstrap."
  },
  {
    "objectID": "tidymodels-intro.html#concluding-remarks",
    "href": "tidymodels-intro.html#concluding-remarks",
    "title": "11  Introduction to Tidymodels",
    "section": "11.2 Concluding remarks",
    "text": "11.2 Concluding remarks\nIn this vignette, we introduced tidymodels and illustrated how to its packages work together by way of example. Since this was an elementary example, so use this as a starting point and explore what more can be done with this wonderful set of packages. And yet, however wonderful they are, you may have already noticed that there are limitations like the glaring lack of a set of post-processing tools to refine the results. We fill this gap for epidemiological modelling with frosting. This will be formally introduced in a later chapter, so stay tuned!\n\n🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧"
  },
  {
    "objectID": "tidymodels-intro.html#attribution",
    "href": "tidymodels-intro.html#attribution",
    "title": "11  Introduction to Tidymodels",
    "section": "11.3 Attribution",
    "text": "11.3 Attribution\nThis Chapter was largely adapted from A Gentle Introduction to Tidymodels as well as Tidymodels - Getting Started and Tidymodels. The diagrams are from A Gentle Introduction to Tidymodels and based on R for Data Science.\n🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧"
  },
  {
    "objectID": "tidymodels-regression.html#libraries",
    "href": "tidymodels-regression.html#libraries",
    "title": "12  Regression in Tidymodels",
    "section": "12.1 Libraries",
    "text": "12.1 Libraries\n\nlibrary(tidymodels)\nlibrary(broom)\nlibrary(performance)"
  },
  {
    "objectID": "tidymodels-regression.html#simple-linear-regression",
    "href": "tidymodels-regression.html#simple-linear-regression",
    "title": "12  Regression in Tidymodels",
    "section": "12.2 Simple linear regression",
    "text": "12.2 Simple linear regression\nThe key steps to perform linear regression in tidymodels are to first specify the model type and then to specify the model form and the data to be used to construct it.\nTo illustrate, we shall look to penguins dataset from the tidymodels’ modeldata package. This dataset contains measurements for 344 penguins from three islands in Palmer Archipelago, Antarctica, and includes information on their species, island home, size (flipper length, body mass, bill dimensions), and sex.\n\n\n\n\n\n\n\n\n\n\n# Let's inspect the data\nhead(penguins)\n\n#&gt; # A tibble: 6 × 7\n#&gt;   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#&gt;   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n#&gt; 1 Adelie  Torgersen           39.1          18.7               181\n#&gt; 2 Adelie  Torgersen           39.5          17.4               186\n#&gt; 3 Adelie  Torgersen           40.3          18                 195\n#&gt; 4 Adelie  Torgersen           NA            NA                  NA\n#&gt; 5 Adelie  Torgersen           36.7          19.3               193\n#&gt; 6 Adelie  Torgersen           39.3          20.6               190\n#&gt; # ℹ 2 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;\n\n\nOne thing you may have spotted is that there’s missing data in this dataset in the fourth row. For simplicity, we will only work with the complete cases. This reduces the number of rows in our dataset to 333.\n\npenguins &lt;- penguins %&gt;%\n  filter(complete.cases(.))\n\nhead(penguins)\n\n#&gt; # A tibble: 6 × 7\n#&gt;   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#&gt;   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n#&gt; 1 Adelie  Torgersen           39.1          18.7               181\n#&gt; 2 Adelie  Torgersen           39.5          17.4               186\n#&gt; 3 Adelie  Torgersen           40.3          18                 195\n#&gt; 4 Adelie  Torgersen           36.7          19.3               193\n#&gt; 5 Adelie  Torgersen           39.3          20.6               190\n#&gt; 6 Adelie  Torgersen           38.9          17.8               181\n#&gt; # ℹ 2 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;\n\n\nMuch better! We will now build a simple linear regression model to model bill length as a function of bill depth.\n\n\n\n\n\n\n\n\n\nIn parsnip, the model specification is broken down into small functions such as set_mode() and set_engine() to make the interface more flexible and readable. The general structure is to first specify a mode (regression or classification) and then an engine to indicate what software (or implementation of the algorithm) will be used to fit the model. For our purposes, the mode is regression and the engine is lm for ordinary least squares. You may note that setting the mode is unnecessary for linear regression, but we include it here as it is a good practice.\n\nlm_spec &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"lm\")\n\nThe above specification does not actually carry out the regression, rather it just states what we would like to do.\n\nlm_spec\n\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\nOnce we have such a blueprint, we may fit a model by inputting data and a formula. Recall that in R, a formula takes the form y ~ x where y ix the response and x is the predictor variable. For our example, where the response of bill length and predictor of bill depth, we would write the formula as bill_length_mm ~ bill_depth_mm.\n\n\n\n\n\n\nNote\n\n\n\nUnlike with standard R formula() objects, the names used this a formula must be identical to the variable names in the dataset. No processing functions are allowed (processing is handled by the recipe()).\n\n\n\nlm_fit &lt;- lm_spec %&gt;%\n  fit(bill_length_mm ~ bill_depth_mm, data = penguins)\n\nlm_fit\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ bill_depth_mm, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;   (Intercept)  bill_depth_mm  \n#&gt;       54.8909        -0.6349\n\n\nThe resulting parsnip object includes basic information about the fit such as the model coefficients. To access the underlying fit object, we could use the standard lm_fit$fit or with purrr’s pluck() function.\n\nlm_fit %&gt;%\n  pluck(\"fit\")\n\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ bill_depth_mm, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;   (Intercept)  bill_depth_mm  \n#&gt;       54.8909        -0.6349\n\n\nTo get additional information about the fit (such as standard errors, and goodness-of-fit statistics), we can get a summary of the model fit as follows:\n\nlm_fit %&gt;%\n  pluck(\"fit\") %&gt;%\n  summary()\n\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ bill_depth_mm, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -12.9498  -3.9530  -0.3657   3.7327  15.5025 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)    54.8909     2.5673  21.380  &lt; 2e-16 ***\n#&gt; bill_depth_mm  -0.6349     0.1486  -4.273 2.53e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.332 on 331 degrees of freedom\n#&gt; Multiple R-squared:  0.05227,    Adjusted R-squared:  0.04941 \n#&gt; F-statistic: 18.26 on 1 and 331 DF,  p-value: 2.528e-05\n\n\nTo get a tidy summary of the model parameter estimates, simply use the tidy function from the broom package on the model fit. To extract model statistics, glance() can be used.\n\ntidy(lm_fit)\n\n#&gt; # A tibble: 2 × 5\n#&gt;   term          estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)     54.9       2.57      21.4  2.54e-64\n#&gt; 2 bill_depth_mm   -0.635     0.149     -4.27 2.53e- 5\n\nglance(lm_fit)\n\n#&gt; # A tibble: 1 × 12\n#&gt;   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n#&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    0.0523        0.0494  5.33      18.3 0.0000253     1 -1029. 2064. 2075.\n#&gt; # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nNow, to make predictions, we simply use predict() on the parnsip model object. In there, we must specify the dataset we want to predict on in the new_data argument. Note that this may be a different dataset than we used for fitting the model, but this input data must include all predictor variables that were used to fit the model.\n\npredict(lm_fit, new_data = penguins)\n\n#&gt; # A tibble: 333 × 1\n#&gt;   .pred\n#&gt;   &lt;dbl&gt;\n#&gt; 1  43.0\n#&gt; 2  43.8\n#&gt; 3  43.5\n#&gt; 4  42.6\n#&gt; 5  41.8\n#&gt; 6  43.6\n#&gt; # ℹ 327 more rows\n\n\nFor parnsip models, the predictions are always outputted in a tibble.\nTo specify the type of prediction made, modify type argument. If we set type = \"conf_int\", we get a 95% confidence interval.\n\npredict(lm_fit, new_data = penguins, type = \"conf_int\")\n\n#&gt; # A tibble: 333 × 2\n#&gt;   .pred_lower .pred_upper\n#&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1        42.3        43.7\n#&gt; 2        43.3        44.4\n#&gt; 3        42.8        44.1\n#&gt; 4        41.8        43.5\n#&gt; 5        40.7        43.0\n#&gt; 6        43.0        44.2\n#&gt; # ℹ 327 more rows\n\n\nTo evaluate model predictive performance, it is logical to compare the each of the observed and predicted values. To see these values side-by-side we simply bind the two vectors of interest.\n\nbind_cols(\n  predict(lm_fit, new_data = penguins),\n  penguins\n) %&gt;%\n  select(bill_length_mm, .pred)\n\n#&gt; # A tibble: 333 × 2\n#&gt;   bill_length_mm .pred\n#&gt;            &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1           39.1  43.0\n#&gt; 2           39.5  43.8\n#&gt; 3           40.3  43.5\n#&gt; 4           36.7  42.6\n#&gt; 5           39.3  41.8\n#&gt; 6           38.9  43.6\n#&gt; # ℹ 327 more rows\n\n\nA simpler way to do this is to use the nifty augment() function.\n\naugment(lm_fit, new_data = penguins) %&gt;%\n  select(bill_length_mm, .pred)\n\n#&gt; # A tibble: 333 × 2\n#&gt;   bill_length_mm .pred\n#&gt;            &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1           39.1  43.0\n#&gt; 2           39.5  43.8\n#&gt; 3           40.3  43.5\n#&gt; 4           36.7  42.6\n#&gt; 5           39.3  41.8\n#&gt; 6           38.9  43.6\n#&gt; # ℹ 327 more rows"
  },
  {
    "objectID": "tidymodels-regression.html#multiple-linear-regression",
    "href": "tidymodels-regression.html#multiple-linear-regression",
    "title": "12  Regression in Tidymodels",
    "section": "12.3 Multiple linear regression",
    "text": "12.3 Multiple linear regression\nThe only difference about fitting a multiple linear regression model in comparison to a simple linear regression model lies the formula. For multiple linear regression, the predictors are specified in the formula expression, separated by +. For example, if we have a response variable y and three predictors, x1, x2, and x3, we would write the formula as, y ~ x1 + x2 + x3.\n\nlm_fit2 &lt;- lm_spec %&gt;% fit(\n  formula = bill_length_mm ~ bill_depth_mm + flipper_length_mm + body_mass_g,\n  data = penguins\n)\nlm_fit2\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ bill_depth_mm + flipper_length_mm + \n#&gt;     body_mass_g, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)      bill_depth_mm  flipper_length_mm        body_mass_g  \n#&gt;        -2.571e+01          6.131e-01          2.872e-01          3.472e-04\n\n\nEverything else proceeds much the same as before. Such as obtaining parameter estimates\n\ntidy(lm_fit2)\n\n#&gt; # A tibble: 4 × 5\n#&gt;   term                estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)       -25.7       6.72        -3.83  1.55e- 4\n#&gt; 2 bill_depth_mm       0.613     0.138        4.43  1.26e- 5\n#&gt; 3 flipper_length_mm   0.287     0.0351       8.18  6.28e-15\n#&gt; 4 body_mass_g         0.000347  0.000566     0.614 5.40e- 1\n\n\nas well as predicting new values.\n\npredict(lm_fit2, new_data = penguins)\n\n#&gt; # A tibble: 333 × 1\n#&gt;   .pred\n#&gt;   &lt;dbl&gt;\n#&gt; 1  39.0\n#&gt; 2  39.7\n#&gt; 3  42.5\n#&gt; 4  42.8\n#&gt; 5  42.8\n#&gt; 6  38.4\n#&gt; # ℹ 327 more rows\n\n\nIf you would like to use all variables aside from your response as predictors, a shortcut is to use the formula form y ~ .\n\nlm_fit3 &lt;- lm_spec %&gt;% fit(bill_length_mm ~ ., data = penguins)\nlm_fit3\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)   speciesChinstrap      speciesGentoo        islandDream  \n#&gt;         15.343291           9.835502           6.117675          -0.503815  \n#&gt;   islandTorgersen      bill_depth_mm  flipper_length_mm        body_mass_g  \n#&gt;         -0.127431           0.300670           0.069257           0.001081  \n#&gt;           sexmale  \n#&gt;          2.047859"
  },
  {
    "objectID": "tidymodels-regression.html#checking-model-assumptions",
    "href": "tidymodels-regression.html#checking-model-assumptions",
    "title": "12  Regression in Tidymodels",
    "section": "12.4 Checking model assumptions",
    "text": "12.4 Checking model assumptions\nAfter fitting a model, it is good to check whether the assumptions of linear regression are met. For this, we will use the performance package, in particular the check_model() function to produce several helpful plots we may use to check the assumptions for our first multiple linear regression model.\n\nlm_fit2 %&gt;%\n  extract_fit_engine() %&gt;%\n  check_model()\n\n\n\n\n\n\n\n\nNotice that on each plot it says what we should expect to see if the model assumption is met.\nWe shall now briefly walk you through what each plot means.\nThe first two plots help us to examine the linearity of the errors versus the fitted values. Ideally, we want this error to be relatively flat and horizontal. The third plot is for checking homogeneity of the variance, where we want the points to be roughly the same distance from the line as this indicates similar dispersion. The fourth plot helps us to see if there are high leverage points - points that have command or influence over the model fit. As a result, these can have a great effect on the model predictions. So the removal of such points or modifications to the model may be necessary to deal with them. The fifth plot helps us to discern collinearity, which is when predictors are highly correlated. Since independent variables should be independent, this can throw off simple regression models (in standard error of coefficient estimates and the estimates themselves, which would likely be sensitive to changes in the predictors that are included in the model). The last plot enables us to check the normality of residuals. If the distribution of the model error is non-normal, then that suggests a linear model may not be appropriate. For a QQ plot, we want the points to fall along a straight diagonal line.\nFor our example, we observe that there’s a pretty high correlation between body_mass_g and flipper_length_mm (not quite in the red-zone of 10 and above, but close enough for concern). That is indicative of multicollinearity between them. Intuitively, it makes sense for the body mass and flipper length variables - we’d expect that as once increases, so should the other.\nWe can take a closer look at the correlation by whipping up a correlation matrix by using base R’s cor() function. Since for collinearity we’re only usually interested in the numerical predictors, we’ll only include the four numeric variables.\n\npenguins_corr &lt;- penguins %&gt;%\n  select(body_mass_g, ends_with(\"_mm\")) %&gt;%\n  cor()\npenguins_corr\n\n#&gt;                   body_mass_g bill_length_mm bill_depth_mm flipper_length_mm\n#&gt; body_mass_g         1.0000000      0.5894511    -0.4720157         0.8729789\n#&gt; bill_length_mm      0.5894511      1.0000000    -0.2286256         0.6530956\n#&gt; bill_depth_mm      -0.4720157     -0.2286256     1.0000000        -0.5777917\n#&gt; flipper_length_mm   0.8729789      0.6530956    -0.5777917         1.0000000\n\n\nIndeed body_mass_g and flipper_length_mm are highly positively correlated. To deal with this problem, we’ll re-fit the model without body_mass_g.\n\nlm_fit3 &lt;- lm_spec %&gt;% fit(\n  formula = bill_length_mm ~ bill_depth_mm + flipper_length_mm,\n  data = penguins\n)\nlm_fit3\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ bill_depth_mm + flipper_length_mm, \n#&gt;     data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)      bill_depth_mm  flipper_length_mm  \n#&gt;          -27.9762             0.6200             0.3052\n\n\nand then check again to see whether the assumptions are met.\n\nlm_fit3 %&gt;%\n  extract_fit_engine() %&gt;%\n  check_model()\n\n\n\n\n\n\n\n\nOverall, the plots look pretty good. For details on how to interpret each of these plots and more details about model assumptions please see here and here."
  },
  {
    "objectID": "tidymodels-regression.html#interaction-terms",
    "href": "tidymodels-regression.html#interaction-terms",
    "title": "12  Regression in Tidymodels",
    "section": "12.5 Interaction terms",
    "text": "12.5 Interaction terms\nIn general, the syntax to add an interaction term to a formula is as follows:\n\nx:y denotes an interaction term between x and y.\nx*y denotes the interaction between x and y as well as x and y; that is, x + y + x*y.\n\nIt is important to note that this syntax is not compatible with all engines. Thus, we shall explain how to bypass this issue by adding an interaction term in a recipe later on. For now, let’s start simple by adding an interaction term between species and bill_length_mm, which allows for a species-specific slope.\n\nlm_fit4 &lt;- lm_spec %&gt;% fit(\n  formula = bill_length_mm ~ species * bill_depth_mm,\n  data = penguins\n)\nlm_fit4\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ species * bill_depth_mm, \n#&gt;     data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                    (Intercept)                speciesChinstrap  \n#&gt;                        23.3668                         -9.9389  \n#&gt;                  speciesGentoo                   bill_depth_mm  \n#&gt;                        -6.6966                          0.8425  \n#&gt; speciesChinstrap:bill_depth_mm     speciesGentoo:bill_depth_mm  \n#&gt;                         1.0796                          1.2178\n\n\nUsing recipes, the interaction term is specified by using step_interact(). Then we construct a workflow object, where we add the linear regression model specification and recipe. Finally, we fit the model as we did for a parsnip model. Note that the workflow object does not need the variables that were specified in the recipe to be specified again.\n\nrec_spec_interact &lt;- recipe(\n  formula = bill_length_mm ~ species + bill_depth_mm,\n  data = penguins\n) %&gt;%\n  step_interact(~ species:bill_depth_mm)\n\nlm_wf_interact &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(rec_spec_interact)\n\nlm_wf_interact %&gt;% fit(penguins)\n\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; • step_interact()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                      (Intercept)                  speciesChinstrap  \n#&gt;                          23.3668                           -9.9389  \n#&gt;                    speciesGentoo                     bill_depth_mm  \n#&gt;                          -6.6966                            0.8425  \n#&gt; speciesChinstrap_x_bill_depth_mm     speciesGentoo_x_bill_depth_mm  \n#&gt;                           1.0796                            1.2178\n\n\nNotice the variable name for the interaction term is not the same as it is in base R (which is simply of the form x:y). In step_interact(), the default separator between the variable names is _x_. You can change this default by specifying the sep argument in the function.\nTo read more about formula syntax, see ?formula."
  },
  {
    "objectID": "tidymodels-regression.html#non-linear-transformations-of-the-predictors",
    "href": "tidymodels-regression.html#non-linear-transformations-of-the-predictors",
    "title": "12  Regression in Tidymodels",
    "section": "12.6 Non-linear transformations of the predictors",
    "text": "12.6 Non-linear transformations of the predictors\nSimilar to how we were able to add an interaction term using recipes, we can also perform a transformation as a pre-processing step. The function used for this is step_mutate() (which acts like dplyr’s mutate).\nNote that, in general, if you are specifying a recipe aim to keep as much of the pre-processing in your recipe specification as possible. This helps to ensure that the transformation will be applied to new data consistently.\n\nrec_spec_pow2 &lt;- recipe(bill_length_mm ~ bill_depth_mm, data = penguins) %&gt;%\n  step_mutate(bill_depth_mm2 = bill_depth_mm^2)\n\nlm_wf_pow2 &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(rec_spec_pow2)\n\nlm_wf_pow2 %&gt;% fit(penguins)\n\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; • step_mutate()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;    (Intercept)   bill_depth_mm  bill_depth_mm2  \n#&gt;        95.2558         -5.4431          0.1413\n\n\nThere are many transformations already built into recipes such as step_log(). So, for basic transformations, there’s often no need to make your own transformation from scratch. See here for a comprehensive list of the transformations that are offered in recipes.\n\nrec_spec_log &lt;- recipe(bill_length_mm ~ bill_depth_mm, data = penguins) %&gt;%\n  step_log(bill_depth_mm) # transforms the var in-place, keeps it's name\n\nlm_wf_log &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(rec_spec_log)\n\nlm_wf_log %&gt;% fit(penguins)\n\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; • step_log()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;   (Intercept)  bill_depth_mm  \n#&gt;         74.95         -10.91\n\n\n\n\n🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧"
  },
  {
    "objectID": "tidymodels-regression.html#attribution",
    "href": "tidymodels-regression.html#attribution",
    "title": "12  Regression in Tidymodels",
    "section": "12.7 Attribution",
    "text": "12.7 Attribution\nThis Chapter was largely adapted from Chapter 3 of ISLR tidymodels labs. Checking linear regression assumptions using the performance package is based on this article and this blog post on investigating model performance. The artwork used is by Allison Horst.Allison Horst.\n🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧"
  },
  {
    "objectID": "preprocessing-and-models.html#introduction",
    "href": "preprocessing-and-models.html#introduction",
    "title": "13  Examples of Preprocessing and Models",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nThe epipredict package uses the tidymodels framework, namely {recipes} for dplyr-like pipeable sequences of feature engineering and {parsnip} for a unified interface to a range of models.\nepipredict has additional customized feature engineering and preprocessing steps that specifically work with panel data in this context, for example, step_epi_lag(), step_population_scaling(), step_epi_naomit(). They can be used along with most steps from the {recipes} package for more feature engineering.\nIn this vignette, we will illustrate some examples of how to use epipredict with recipes and parsnip for different purposes of epidemiological forecasting. We will focus on basic autoregressive models, in which COVID cases and deaths in the near future are predicted using a linear combination of cases and deaths in the near past.\nThe remaining vignette will be split into three sections. In the first section, we will use a Poisson regression to predict death counts. In the second section, we will use a linear regression to predict death rates. Last but not least, we will create a classification model for hotspot predictions.\n\nlibrary(epidatr)\nlibrary(epipredict)\nlibrary(recipes)\nlibrary(workflows)\nlibrary(poissonreg)"
  },
  {
    "objectID": "preprocessing-and-models.html#poisson-regression",
    "href": "preprocessing-and-models.html#poisson-regression",
    "title": "13  Examples of Preprocessing and Models",
    "section": "13.2 Poisson Regression",
    "text": "13.2 Poisson Regression\nDuring COVID-19, the U.S. Centers for Disease Control and Prevention (CDC) collected models and forecasts to characterize the state of an outbreak and its course. They use it to inform public health decision makers on potential consequences of deploying control measures.\nOne of the outcomes that the CDC forecasts is death counts from COVID-19. Although there are many state-of-the-art models, we choose to use Poisson regression, the textbook example for modeling count data, as an illustration for using the epipredict package with other existing {tidymodels} packages.\nThe (folded) code below gives the necessary commands to download this data from the Delphi Epidata API, but it is also built into the {epidatasets} package.\n\n\nCode\ngeos &lt;- c(\"ca\", \"fl\", \"tx\", \"ny\", \"nj\")\nx &lt;- covidcast(\n  data_source = \"jhu-csse\",\n  signals = \"confirmed_incidence_num\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20210604, 20211231),\n  geo_values = geos\n) %&gt;%\n  fetch() %&gt;%\n  select(geo_value, time_value, cases = value)\n\ny &lt;- covidcast(\n  data_source = \"jhu-csse\",\n  signals = \"deaths_incidence_num\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20210604, 20211231),\n  geo_values = geos\n) %&gt;%\n  fetch() %&gt;%\n  select(geo_value, time_value, deaths = value)\n\ncounts_subset &lt;- full_join(x, y, by = c(\"geo_value\", \"time_value\")) %&gt;%\n  as_epi_df()\n\n\n\ndata(counts_subset, package = \"epidatasets\")\n\nThe counts_subset dataset contains the number of confirmed cases and deaths from June 4, 2021 to Dec 31, 2021 in some U.S. states.\nWe wish to predict the 7-day ahead death counts with lagged cases and deaths. Furthermore, we will let each state be a dummy variable. Using differential intercept coefficients, we can allow for an intercept shift between states.\nOne possible model takes the form\n\\[\\begin{aligned}\n\\log\\left( \\mu_{t+7} \\right) &{}= \\beta_0 + \\delta_1 s_{\\text{state}_1} +\n\\delta_2 s_{\\text{state}_2} + \\cdots +  \\nonumber \\\\ &\\quad\\beta_1 \\text{deaths}_{t} +\n\\beta_2 \\text{deaths}_{t-7}  + \\beta_3 \\text{cases}_{t} +\n\\beta_4 \\text{cases}_{t-7},\n\\end{aligned}\\]\nwhere \\(\\mu_{t+7} = \\mathbb{E}(\\text{deaths}_{t+7})\\), and \\(\\text{deaths}_{t+7}\\) is assumed to follow a Poisson distribution with mean \\(\\mu_{t+7}\\); \\(s_{\\text{state}}\\) are dummy variables for each state and take values of either 0 or 1.\nPreprocessing steps will be performed to prepare the data for model fitting. But before diving into them, it will be helpful to understand what roles are in the recipes framework.\n\n\nAside on recipes\nrecipes can assign one or more roles to each column in the data. The roles are not restricted to a predefined set; they can be anything. For most conventional situations, they are typically “predictor” and/or “outcome”. Additional roles enable targeted step_*() operations on specific variables or groups of variables.\nIn our case, the role predictor is given to explanatory variables on the right-hand side of the model (in the equation above). The role outcome is the response variable that we wish to predict. geo_value and time_value are predefined roles that are unique to the epipredict package. Since we work with epi_df objects, all datasets should have geo_value and time_value passed through automatically with these two roles assigned to the appropriate columns in the data.\nThe recipes package also allows manual alterations of roles in bulk. There are a few handy functions that can be used together to help us manipulate variable roles easily.\n\nupdate_role() alters an existing role in the recipe or assigns an initial role to variables that do not yet have a declared role.\nadd_role() adds an additional role to variables that already have a role in the recipe, without overwriting old roles.\nremove_role() eliminates a single existing role in the recipe.\n\n\n\nEnd aside\n\nNotice in the following preprocessing steps, we used add_role() on geo_value_factor since, currently, the default role for it is raw, but we would like to reuse this variable as a predictor.\n\ncounts_subset &lt;- counts_subset %&gt;%\n  mutate(geo_value_factor = as.factor(geo_value)) %&gt;%\n  as_epi_df()\n\nepi_recipe(counts_subset)\n\nr &lt;- epi_recipe(counts_subset) %&gt;%\n  add_role(geo_value_factor, new_role = \"predictor\") %&gt;%\n  step_dummy(geo_value_factor) %&gt;%\n  ## Occasionally, data reporting errors / corrections result in negative\n  ## cases / deaths\n  step_mutate(cases = pmax(cases, 0), deaths = pmax(deaths, 0)) %&gt;%\n  step_epi_lag(cases, deaths, lag = c(0, 7)) %&gt;%\n  step_epi_ahead(deaths, ahead = 7, role = \"outcome\") %&gt;%\n  step_epi_naomit()\n\nAfter specifying the preprocessing steps, we will use the parsnip package for modeling and producing the prediction for death count, 7 days after the latest available date in the dataset.\n\nlatest &lt;- get_test_data(r, counts_subset)\n\nwf &lt;- epi_workflow(r, parsnip::poisson_reg()) %&gt;%\n  fit(counts_subset)\n\npredict(wf, latest) %&gt;% filter(!is.na(.pred))\n\n#&gt; An `epi_df` object, 5 x 3 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2023-06-07 16:52:32.877214\n#&gt; \n#&gt; # A tibble: 5 × 3\n#&gt;   geo_value time_value .pred\n#&gt; * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;\n#&gt; 1 ca        2021-12-31 108. \n#&gt; 2 fl        2021-12-31 270. \n#&gt; 3 nj        2021-12-31  22.5\n#&gt; 4 ny        2021-12-31  94.8\n#&gt; 5 tx        2021-12-31  91.0\n\n\nNote that the time_value corresponds to the date(s) in the test set latest, NOT to the target date of the forecast (2022-01-07). Had we used different data for predictions, we would have gotten different time_value’s.\nLet’s take a look at the fit:\n\nextract_fit_engine(wf)\n\n#&gt; \n#&gt; Call:  stats::glm(formula = ..y ~ ., family = stats::poisson, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;         (Intercept)  geo_value_factor_fl  geo_value_factor_nj  \n#&gt;           3.970e+00           -1.487e-01           -1.425e+00  \n#&gt; geo_value_factor_ny  geo_value_factor_tx          lag_0_cases  \n#&gt;          -6.865e-01            3.025e-01            1.339e-05  \n#&gt;         lag_7_cases         lag_0_deaths         lag_7_deaths  \n#&gt;           1.717e-06            1.731e-03            8.566e-04  \n#&gt; \n#&gt; Degrees of Freedom: 984 Total (i.e. Null);  976 Residual\n#&gt; Null Deviance:       139600 \n#&gt; Residual Deviance: 58110     AIC: 62710\n\n\nAlternative forms of Poisson regression or particular computational approaches can be applied via arguments to parsnip::poisson_reg() for some common settings, and by using parsnip::set_engine() to use a specific Poisson regression engine and to provide additional engine-specific customization."
  },
  {
    "objectID": "preprocessing-and-models.html#linear-regression",
    "href": "preprocessing-and-models.html#linear-regression",
    "title": "13  Examples of Preprocessing and Models",
    "section": "13.3 Linear Regression",
    "text": "13.3 Linear Regression\nFor COVID-19, the CDC required submission of case and death count predictions. However, the Delphi Group preferred to train on rate data instead, because it puts different locations on a similar scale (eliminating the need for location-specific intercepts). We can use a linear regression to predict the death rates and use state population data to scale the rates to counts.1 We will do so using layer_population_scaling() from the epipredict package. (We could also use step_population_scaling() from the epipredict package to prepare rate data from count data in the preprocessing recipe.)\nAdditionally, when forecasts are submitted, prediction intervals should be provided along with the point estimates. This can be obtained via postprocessing using layer_residual_quantiles(). It is worth pointing out, however, that layer_residual_quantiles() should be used before population scaling or else the transformation will make the results uninterpretable.\nWe wish, now, to predict the 7-day ahead death counts with lagged case rates and death rates, along with some extra behaviourial predictors. Namely, we will use survey data from COVID-19 Trends and Impact Survey.\nThe survey data provides the estimated percentage of people who wore a mask for most or all of the time while in public in the past 7 days and the estimated percentage of respondents who reported that all or most people they encountered in public in the past 7 days maintained a distance of at least 6 feet.\n\n\nCode\n# Download the raw data as used in {epidatasets}\nbehav_ind_mask &lt;- covidcast(\n  data_source = \"fb-survey\",\n  signals = \"smoothed_wwearing_mask_7d\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20210604, 20211231),\n  geo_values = geos\n) %&gt;%\n  fetch() %&gt;%\n  select(geo_value, time_value, masking = value)\n\nbehav_ind_distancing &lt;- covidcast(\n  data_source = \"fb-survey\",\n  signals = \"smoothed_wothers_distanced_public\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20210604, 20211231),\n  geo_values = geos\n) %&gt;%\n  fetch() %&gt;%\n  select(geo_value, time_value, distancing = value)\n\nctis_covid_behaviours &lt;- behav_ind_mask %&gt;%\n  full_join(behav_ind_distancing, by = c(\"geo_value\", \"time_value\"))\n\n\n\ndata(ctis_covid_behaviours, package = \"epidatasets\")\npop_dat &lt;- state_census %&gt;% select(abbr, pop)\n\nState-wise population data from the 2019 U.S. Census is available from {epipredict} and will be used in layer_population_scaling().\nRather than using raw mask-wearing / social-distancing metrics, for the sake of illustration, we’ll convert both into categorical predictors.\n\n\n\n\n\n\n\n\n\nWe will take a subset of death rate and case rate data from the built-in dataset case_death_rate_subset.\n\njhu &lt;- filter(\n  case_death_rate_subset,\n  time_value &gt;= \"2021-06-04\",\n  time_value &lt;= \"2021-12-31\",\n  geo_value %in% c(\"ca\", \"fl\", \"tx\", \"ny\", \"nj\")\n)\n\nPreprocessing steps will again rely on functions from the epipredict package as well as the recipes package. There are also many functions in the recipes package that allow for scalar transformations, such as log transformations and data centering. In our case, we will center the numerical predictors to allow for a more meaningful interpretation of the intercept.\n\njhu &lt;- jhu %&gt;%\n  mutate(geo_value_factor = as.factor(geo_value)) %&gt;%\n  left_join(ctis_covid_behaviours, by = c(\"geo_value\", \"time_value\")) %&gt;%\n  as_epi_df()\n\nr &lt;- epi_recipe(jhu) %&gt;%\n  add_role(geo_value_factor, new_role = \"predictor\") %&gt;%\n  step_dummy(geo_value_factor) %&gt;%\n  step_epi_lag(case_rate, death_rate, lag = c(0, 7, 14)) %&gt;%\n  step_mutate(\n    masking = cut_number(masking, 5),\n    distancing = cut_number(distancing, 5)\n  ) %&gt;%\n  step_epi_ahead(death_rate, ahead = 7, role = \"outcome\") %&gt;%\n  step_center(contains(\"lag\"), role = \"predictor\") %&gt;%\n  step_epi_naomit()\n\nAs a sanity check we can examine the structure of the training data:\n\nglimpse(bake(prep(r, jhu), jhu))\n\n#&gt; Rows: 985\n#&gt; Columns: 17\n#&gt; $ time_value          &lt;date&gt; 2021-06-18, 2021-06-18, 2021-06-18, 2021-06-18…\n#&gt; $ geo_value           &lt;chr&gt; \"ca\", \"fl\", \"nj\", \"ny\", \"tx\", \"ca\", \"fl\", \"nj\",…\n#&gt; $ case_rate           &lt;dbl&gt; 2.382641, 6.635633, 2.771139, 1.959257, 3.50565…\n#&gt; $ death_rate          &lt;dbl&gt; 0.0373762, 0.1906224, 0.0707662, 0.0554089, 0.0…\n#&gt; $ masking             &lt;fct&gt; \"(69.7,85]\", \"(52.8,60.2]\", \"(60.2,63.9]\", \"(60…\n#&gt; $ distancing          &lt;fct&gt; \"(27,43]\", \"(21.1,27]\", \"(27,43]\", \"(27,43]\", \"…\n#&gt; $ geo_value_factor_fl &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n#&gt; $ geo_value_factor_nj &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,…\n#&gt; $ geo_value_factor_ny &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,…\n#&gt; $ geo_value_factor_tx &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,…\n#&gt; $ lag_0_case_rate     &lt;dbl&gt; -24.55902, -20.30603, -24.17052, -24.98241, -23…\n#&gt; $ lag_7_case_rate     &lt;dbl&gt; -24.28505, -17.44078, -23.74271, -24.00795, -19…\n#&gt; $ lag_14_case_rate    &lt;dbl&gt; -24.61817, -20.99358, -24.55491, -23.72352, -22…\n#&gt; $ lag_0_death_rate    &lt;dbl&gt; -0.2444974, -0.0912512, -0.2111074, -0.2264647,…\n#&gt; $ lag_7_death_rate    &lt;dbl&gt; -0.1875259, -0.0978243, -0.1869826, -0.2035624,…\n#&gt; $ lag_14_death_rate   &lt;dbl&gt; -0.1980493, -0.1431793, -0.1532078, -0.1651456,…\n#&gt; $ ahead_7_death_rate  &lt;dbl&gt; 0.1037824, 0.1426382, 0.0964993, 0.0347229, 0.0…\n\n\nBefore directly predicting the results, we need to add postprocessing layers to obtain the death counts instead of death rates. Note that the rates used so far are “per 100K people” rather than “per person”. We’ll also use quantile regression with the quantile_reg engine rather than ordinary least squares to create median predictions and a 90% prediction interval.\n\nf &lt;- frosting() %&gt;%\n  layer_predict() %&gt;%\n  layer_add_target_date(\"2022-01-07\") %&gt;%\n  layer_add_forecast_date() %&gt;%\n  layer_threshold(.pred, lower = 0) %&gt;%\n  layer_quantile_distn() %&gt;%\n  layer_point_from_distn() %&gt;%\n  layer_naomit(.pred) %&gt;%\n  layer_population_scaling(\n    contains(\".pred\"),\n    df = pop_dat,\n    rate_rescaling = 1e5,\n    by = c(\"geo_value\" = \"abbr\"),\n    df_pop_col = \"pop\"\n  )\n\nwf &lt;- epi_workflow(r, quantile_reg(tau = c(.05, .5, .95))) %&gt;%\n  fit(jhu) %&gt;%\n  add_frosting(f)\n\nlatest &lt;- get_test_data(recipe = r, x = jhu)\np &lt;- predict(wf, latest) %&gt;%\n  select(-time_value) %&gt;%\n  as_tibble()\np\n\n#&gt; # A tibble: 5 × 7\n#&gt;   geo_value .pred target_date forecast_date         .pred_distn .pred_scaled\n#&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;date&gt;      &lt;date&gt;                     &lt;dist&gt;        &lt;dbl&gt;\n#&gt; 1 ca        0.181 2022-01-07  2021-12-31    [0.25, 0.75]&lt;q-rng&gt;         71.6\n#&gt; 2 fl        0.348 2022-01-07  2021-12-31    [0.25, 0.75]&lt;q-rng&gt;         74.7\n#&gt; 3 nj        0.646 2022-01-07  2021-12-31    [0.25, 0.75]&lt;q-rng&gt;         57.4\n#&gt; 4 ny        0.698 2022-01-07  2021-12-31    [0.25, 0.75]&lt;q-rng&gt;        136. \n#&gt; 5 tx        0.299 2022-01-07  2021-12-31    [0.25, 0.75]&lt;q-rng&gt;         86.8\n#&gt; # ℹ 1 more variable: .pred_distn_scaled &lt;dist&gt;\n\n\nThe columns marked *_scaled (unfortunately, some of these are hidden above) have been rescaled to the correct units, in this case deaths rather than deaths per 100K people (these remain in .pred).\nTo look at the prediction intervals:\n\np %&gt;%\n  select(geo_value, target_date, .pred_scaled, .pred_distn_scaled) %&gt;%\n  pivot_quantiles(.pred_distn_scaled)\n\n#&gt; # A tibble: 5 × 5\n#&gt;   geo_value target_date .pred_scaled `0.25` `0.75`\n#&gt;   &lt;chr&gt;     &lt;date&gt;             &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 ca        2022-01-07          71.6   48.8   94.0\n#&gt; 2 fl        2022-01-07          74.7   48.4  104. \n#&gt; 3 nj        2022-01-07          57.4   45.5   68.7\n#&gt; 4 ny        2022-01-07         136.   108.   163. \n#&gt; 5 tx        2022-01-07          86.8   68.6  107.\n\n\nLast but not least, let’s take a look at the regression fit and check the coefficients:\n\n\n#&gt; Call:\n#&gt; quantreg::rq(formula = ..y ~ ., tau = ~c(0.05, 0.5, 0.95), data = data, \n#&gt;     na.action = stats::na.omit, method = \"br\", model = FALSE)\n#&gt; \n#&gt; Coefficients:\n#&gt;                        tau= 0.05     tau= 0.50    tau= 0.95\n#&gt; (Intercept)          0.210811625  0.2962574475  0.417583265\n#&gt; geo_value_factor_fl  0.032085820  0.0482361119  0.171126713\n#&gt; geo_value_factor_nj  0.007313762 -0.0033797953 -0.025251865\n#&gt; geo_value_factor_ny -0.001489163 -0.0199485947 -0.032635584\n#&gt; geo_value_factor_tx  0.029077485  0.0391980273  0.071961515\n#&gt; lag_0_case_rate     -0.001636588 -0.0011625693 -0.001430622\n#&gt; lag_7_case_rate      0.004700752  0.0057822095  0.006912655\n#&gt; lag_14_case_rate     0.001715816  0.0004224753  0.003448733\n#&gt; lag_0_death_rate     0.462341754  0.5274192012  0.164856372\n#&gt; lag_7_death_rate    -0.007368501  0.1132903956  0.172687438\n#&gt; lag_14_death_rate   -0.072500707 -0.0270474349  0.181279299\n#&gt; \n#&gt; Degrees of freedom: 950 total; 939 residual"
  },
  {
    "objectID": "preprocessing-and-models.html#classification",
    "href": "preprocessing-and-models.html#classification",
    "title": "13  Examples of Preprocessing and Models",
    "section": "13.4 Classification",
    "text": "13.4 Classification\nSometimes it is preferable to create a predictive model for surges or upswings rather than for raw values. In this case, the target is to predict if the future will have increased case rates (denoted up), decreased case rates (down), or flat case rates (flat) relative to the current level. Such models may be referred to as “hotspot prediction models”. We will follow the analysis in McDonald, Bien, Green, Hu, et al. but extend the application to predict three categories instead of two.\nHotspot prediction uses a categorical outcome variable defined in terms of the relative change of \\(Y_{\\ell, t+a}\\) compared to \\(Y_{\\ell, t}\\). Where \\(Y_{\\ell, t}\\) denotes the case rates in location \\(\\ell\\) at time \\(t\\). We define the response variables as follows:\n\\[\nZ_{\\ell, t}=\n    \\begin{cases}\n      \\text{up}, & \\text{if}\\ Y^{\\Delta}_{\\ell, t} &gt; 0.25 \\\\\n      \\text{down}, & \\text{if}\\  Y^{\\Delta}_{\\ell, t} &lt; -0.20\\\\\n      \\text{flat}, & \\text{otherwise}\n    \\end{cases}\n\\]\nwhere \\(Y^{\\Delta}_{\\ell, t} = (Y_{\\ell, t}- Y_{\\ell, t-7})\\ /\\ (Y_{\\ell, t-7})\\). We say location \\(\\ell\\) is a hotspot at time \\(t\\) when \\(Z_{\\ell,t}\\) is up, meaning the number of newly reported cases over the past 7 days has increased by at least 25% compared to the preceding week. When \\(Z_{\\ell,t}\\) is categorized as down, it suggests that there has been at least a 20% decrease in newly reported cases over the past 7 days (a 20% decrease is the inverse of a 25% increase). Otherwise, we will consider the trend to be flat.\nThe expression of the multinomial regression we will use is as follows: \\[\n\\pi_{j}(x) = \\text{Pr}(Z_{\\ell,t} = j|x) = \\frac{e^{g_j(x)}}{1 + \\sum_{k=0}^2 g_j(x) }\n\\] where \\(j\\) is either down, flat, or up\n\\[\n\\begin{aligned}\ng_{\\text{down}}(x) &= 0,\\\\\ng_{\\text{flat}}(x) &=\n\\log\\left(\\frac{Pr(Z_{\\ell,t}=\\text{flat}|x)}{Pr(Z_{\\ell,t}=\\text{down}|x)}\\right) =\n\\beta_{10} + \\beta_{11}t + \\delta_{10} s_{\\text{state}_1} +\n\\delta_{11} s_{\\text{state}_2} + \\cdots \\nonumber \\\\\n&\\quad +\\ \\beta_{12} Y^{\\Delta}_{\\ell, t} +\n\\beta_{13} Y^{\\Delta}_{\\ell, t-7}, \\\\\ng_{\\text{flat}}(x) &= \\log\\left(\\frac{Pr(Z_{\\ell,t}=\\text{up}|x)}{Pr(Z_{\\ell,t}=\\text{down}|x)}\\right) =\n\\beta_{20} + \\beta_{21}t + \\delta_{20} s_{\\text{state}_1} +\n\\delta_{21} s_{\\text{state}_2} + \\cdots \\nonumber \\\\\n&\\quad +\\ \\beta_{22} Y^{\\Delta}_{\\ell, t} +\n\\beta_{23} Y^{\\Delta}_{\\ell, t-7}.\n\\end{aligned}\n\\]\nPreprocessing steps are similar to the previous models with an additional step of categorizing the response variables. Again, we will use a subset of death rate and case rate data from our built-in dataset case_death_rate_subset.\n\njhu_rates &lt;- case_death_rate_subset %&gt;%\n  dplyr::filter(\n    time_value &gt;= \"2021-06-04\",\n    time_value &lt;= \"2021-12-31\",\n    geo_value %in% c(\"ca\", \"fl\", \"tx\", \"ny\", \"nj\")\n  ) %&gt;%\n  mutate(geo_value_factor = as.factor(geo_value))\n\nr &lt;- epi_recipe(jhu_rates) %&gt;%\n  add_role(time_value, new_role = \"predictor\") %&gt;%\n  step_dummy(geo_value_factor) %&gt;%\n  step_growth_rate(case_rate, role = \"none\", prefix = \"gr_\") %&gt;%\n  step_epi_lag(starts_with(\"gr_\"), lag = c(0, 7, 14)) %&gt;%\n  step_epi_ahead(starts_with(\"gr_\"), ahead = 7, role = \"none\") %&gt;%\n  # note recipes::step_cut() has a bug in it, or we could use that here\n  step_mutate(\n    response = cut(\n      ahead_7_gr_7_rel_change_case_rate,\n      breaks = c(-Inf, -0.2, 0.25, Inf) / 7, # division gives weekly not daily\n      labels = c(\"down\", \"flat\", \"up\")\n    ),\n    role = \"outcome\"\n  ) %&gt;%\n  step_rm(has_role(\"none\"), has_role(\"raw\")) %&gt;%\n  step_epi_naomit()\n\nWe will fit the multinomial regression and examine the predictions:\n\nwf &lt;- epi_workflow(r, parsnip::multinom_reg()) %&gt;%\n  fit(jhu_rates)\n\nlatest &lt;- get_test_data(recipe = r, x = jhu_rates)\npredict(wf, latest) %&gt;% filter(!is.na(.pred_class))\n\n#&gt; An `epi_df` object, 5 x 3 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25.791826\n#&gt; \n#&gt; # A tibble: 5 × 3\n#&gt;   geo_value time_value .pred_class\n#&gt; * &lt;chr&gt;     &lt;date&gt;     &lt;fct&gt;      \n#&gt; 1 ca        2021-12-31 up         \n#&gt; 2 fl        2021-12-31 up         \n#&gt; 3 nj        2021-12-31 up         \n#&gt; 4 ny        2021-12-31 up         \n#&gt; 5 tx        2021-12-31 up\n\n\nWe can also look at the estimated coefficients and model summary information:\n\nextract_fit_engine(wf)\n\n#&gt; Call:\n#&gt; nnet::multinom(formula = ..y ~ ., data = data, trace = FALSE)\n#&gt; \n#&gt; Coefficients:\n#&gt;      (Intercept)  time_value geo_value_factor_fl geo_value_factor_nj\n#&gt; flat   -144.2225 0.007754539          -1.3251332            1.137558\n#&gt; up     -133.1995 0.007082200          -0.5081323            1.562699\n#&gt;      geo_value_factor_ny geo_value_factor_tx lag_0_gr_7_rel_change_case_rate\n#&gt; flat            24.74419          -0.3345769                        18.96357\n#&gt; up              24.84975          -0.3176984                        33.79521\n#&gt;      lag_7_gr_7_rel_change_case_rate lag_14_gr_7_rel_change_case_rate\n#&gt; flat                        33.19050                         7.157027\n#&gt; up                          56.52376                         4.684422\n#&gt; \n#&gt; Residual Deviance: 1157.928 \n#&gt; AIC: 1193.928\n\n\nOne could also use a formula in epi_recipe() to achieve the same results as above. However, only one of add_formula(), add_recipe(), or workflow_variables() can be specified. For the purpose of demonstrating add_formula rather than add_recipe, we will prep and bake our recipe to return a data.frame that could be used for model fitting.\n\nb &lt;- bake(prep(r, jhu_rates), jhu_rates)\n\nepi_workflow() %&gt;%\n  add_formula(\n    response ~ geo_value + time_value + lag_0_gr_7_rel_change_case_rate +\n      lag_7_gr_7_rel_change_case_rate + lag_14_gr_7_rel_change_case_rate\n  ) %&gt;%\n  add_model(parsnip::multinom_reg()) %&gt;%\n  fit(data = b)\n\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: multinom_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; response ~ geo_value + time_value + lag_0_gr_7_rel_change_case_rate + \n#&gt;     lag_7_gr_7_rel_change_case_rate + lag_14_gr_7_rel_change_case_rate\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Call:\n#&gt; nnet::multinom(formula = ..y ~ ., data = data, trace = FALSE)\n#&gt; \n#&gt; Coefficients:\n#&gt;      (Intercept) geo_valuefl geo_valuenj geo_valueny geo_valuetx  time_value\n#&gt; flat   -144.2169  -1.3265567    1.133930    24.75059  -0.3335109 0.007754346\n#&gt; up     -133.3504  -0.5120227    1.559699    24.85666  -0.3158328 0.007090257\n#&gt;      lag_0_gr_7_rel_change_case_rate lag_7_gr_7_rel_change_case_rate\n#&gt; flat                        19.02258                        33.20795\n#&gt; up                          33.84665                        56.57066\n#&gt;      lag_14_gr_7_rel_change_case_rate\n#&gt; flat                         7.140357\n#&gt; up                           4.668902\n#&gt; \n#&gt; Residual Deviance: 1157.919 \n#&gt; AIC: 1193.919"
  },
  {
    "objectID": "preprocessing-and-models.html#footnotes",
    "href": "preprocessing-and-models.html#footnotes",
    "title": "13  Examples of Preprocessing and Models",
    "section": "",
    "text": "We could continue with the Poisson model, but we’ll switch to the Gaussian likelihood just for simplicity.↩︎"
  },
  {
    "objectID": "sliding-forecasters.html#comparing-different-forecasting-engines",
    "href": "sliding-forecasters.html#comparing-different-forecasting-engines",
    "title": "14  Pseudo-prospective forecast inspection",
    "section": "14.1 Comparing different forecasting engines",
    "text": "14.1 Comparing different forecasting engines\n\n14.1.1 Example using CLI and case data from US states\nFirst, we download the version history (i.e. archive) of the percentage of doctor’s visits with CLI (COVID-like illness) computed from medical insurance claims and the number of new confirmed COVID-19 cases per 100,000 population (daily) for all 50 states from the COVIDcast API. We process as before, with the modification that we use sync = \"locf\" in epix_merge() so that the last version of each observation can be carried forward to extrapolate unavailable versions for the less up-to-date input archive.\n\nus_raw_history_dfs &lt;-\n  readRDS(system.file(\"extdata\", \"all_states_covidcast_signals.rds\",\n    package = \"epipredict\", mustWork = TRUE\n  ))\n\nus_cli_archive &lt;- us_raw_history_dfs[[1]] %&gt;%\n  select(geo_value, time_value, version = issue, percent_cli = value) %&gt;%\n  as_epi_archive(compactify = TRUE)\nus_cases_archive &lt;- us_raw_history_dfs[[2]] %&gt;%\n  select(geo_value, time_value, version = issue, case_rate = value) %&gt;%\n  as_epi_archive(compactify = TRUE)\n\nus_archive &lt;- epix_merge(\n  us_cli_archive, us_cases_archive,\n  sync = \"locf\", compactify = TRUE\n)\n\nAfter obtaining the latest snapshot of the data, we produce forecasts on that data using the default engine of simple linear regression and compare to a random forest.\nNote that all of the warnings about the forecast date being less than the most recent update date of the data have been suppressed to avoid cluttering the output.\n\n# Latest snapshot of data, and forecast dates\nus_latest &lt;- epix_as_of(us_archive, max_version = max(us_archive$versions_end))\nfc_time_values &lt;- seq(\n  from = as.Date(\"2020-08-01\"),\n  to = as.Date(\"2021-11-01\"),\n  by = \"1 month\"\n)\naheads &lt;- c(7, 14, 21, 28)\n\nk_week_ahead &lt;- function(epi_df, outcome, predictors, ahead = 7, engine) {\n  epi_slide(epi_df, ~ arx_forecaster(\n    .x, outcome, predictors, engine,\n    args_list = arx_args_list(ahead = ahead)\n  )$predictions %&gt;%\n    select(-geo_value),\n  before = 120L - 1L,\n  ref_time_values = fc_time_values,\n  new_col_name = \"fc\"\n  ) %&gt;%\n    select(geo_value, time_value, starts_with(\"fc\")) %&gt;%\n    mutate(engine_type = engine$engine)\n}\n\n# Generate the forecasts and bind them together\nfc &lt;- bind_rows(\n  map(aheads, ~ k_week_ahead(\n    us_latest, \"case_rate\", c(\"case_rate\", \"percent_cli\"), .x,\n    engine = linear_reg()\n  )) %&gt;%\n    list_rbind(),\n  map(aheads, ~ k_week_ahead(\n    us_latest, \"case_rate\", c(\"case_rate\", \"percent_cli\"), .x,\n    engine = rand_forest(mode = \"regression\")\n  )) %&gt;%\n    list_rbind()\n) %&gt;%\n  pivot_quantiles(contains(\"_distn\"))\n\nHere, arx_forecaster() does all the heavy lifting. It creates leads of the target (respecting time stamps and locations) along with lags of the features (here, the response and doctors visits), estimates a forecasting model using the specified engine, creates predictions, and non-parametric confidence bands.\nTo see how the predictions compare, we plot them on top of the latest case rates. Note that even though we’ve fitted the model on all states, we’ll just display the results for two states, California (CA) and Florida (FL), to get a sense of the model performance while keeping the graphic simple.\n\n\nCode\nfc_cafl &lt;- fc %&gt;% filter(geo_value %in% c(\"ca\", \"fl\"))\nlatest_cafl &lt;- us_latest %&gt;% filter(geo_value %in% c(\"ca\", \"fl\"))\n\nggplot(fc_cafl, aes(fc_target_date, group = time_value, fill = engine_type)) +\n  geom_line(\n    data = latest_cafl, aes(x = time_value, y = case_rate),\n    inherit.aes = FALSE, color = \"gray50\"\n  ) +\n  geom_ribbon(aes(ymin = `0.05`, ymax = `0.95`), alpha = 0.4) +\n  geom_line(aes(y = fc_.pred)) +\n  geom_point(aes(y = fc_.pred), size = 0.5) +\n  geom_vline(aes(xintercept = time_value), linetype = 2, alpha = 0.5) +\n  facet_grid(engine_type ~ geo_value, scales = \"free\") +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %y\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  labs(x = \"Date\", y = \"Reported COVID-19 case rates\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFor the two states of interest, simple linear regression clearly performs better than random forest in terms of accuracy of the predictions and does not result in such in overconfident predictions (overly narrow confidence bands). Though, in general, neither approach produces amazingly accurate forecasts. This could be because the behaviour is rather different across states and the effects of other notable factors such as age and public health measures may be important to account for in such forecasting. Including such factors as well as making enhancements such as correcting for outliers are some improvements one could make to this simple model.1\n\n\n14.1.2 Example using case data from Canada\nBy leveraging the flexibility of epiprocess, we can apply the same techniques to data from other sources. Since some collaborators are in British Columbia, Canada, we’ll do essentially the same thing for Canada as we did above.\nThe COVID-19 Canada Open Data Working Group collects daily time series data on COVID-19 cases, deaths, recoveries, testing and vaccinations at the health region and province levels. Data are collected from publicly available sources such as government datasets and news releases. Unfortunately, there is no simple versioned source, so we have created our own from the Github commit history.\nFirst, we load versioned case rates at the provincial level. After converting these to 7-day averages (due to highly variable provincial reporting mismatches), we then convert the data to an epi_archive object, and extract the latest version from it. Finally, we run the same forcasting exercise as for the American data, but here we compare the forecasts produced from using simple linear regression with those from using boosted regression trees.\n\n# source(\"drafts/canada-case-rates.R)\ncan &lt;- readRDS(system.file(\n  \"extdata\", \"can_prov_cases.rds\",\n  package = \"epipredict\", mustWork = TRUE\n))\ncan &lt;- can %&gt;%\n  group_by(version, geo_value) %&gt;%\n  arrange(time_value) %&gt;%\n  mutate(cr_7dav = RcppRoll::roll_meanr(case_rate, n = 7L)) %&gt;%\n  as_epi_archive(compactify = TRUE)\n\ncan_latest &lt;- epix_as_of(can, max_version = max(can$DT$version))\n\n# Generate the forecasts, and bind them together\ncan_fc &lt;- bind_rows(\n  map(aheads, ~ k_week_ahead(\n    can_latest, \"cr_7dav\", \"cr_7dav\", .x, linear_reg()\n  )) %&gt;%\n    list_rbind(),\n  map(aheads, ~ k_week_ahead(\n    can_latest, \"cr_7dav\", \"cr_7dav\", .x,\n    boost_tree(mode = \"regression\", trees = 20)\n  )) %&gt;%\n    list_rbind()\n) %&gt;%\n  pivot_quantiles(contains(\"_distn\"))\n\nThe first figure shows the results for all of the provinces using linear regression.\n\n\nCode\nggplot(\n  can_fc %&gt;% filter(engine_type == \"lm\"),\n  aes(x = fc_target_date, group = time_value)\n) +\n  coord_cartesian(xlim = lubridate::ymd(c(\"2020-12-01\", NA))) +\n  geom_line(\n    data = can_latest, aes(x = time_value, y = cr_7dav),\n    inherit.aes = FALSE, color = \"gray50\"\n  ) +\n  geom_ribbon(aes(ymin = `0.05`, ymax = `0.95`, fill = geo_value),\n    alpha = 0.4\n  ) +\n  geom_line(aes(y = fc_.pred)) +\n  geom_point(aes(y = fc_.pred), size = 0.5) +\n  geom_vline(aes(xintercept = time_value), linetype = 2, alpha = 0.5) +\n  facet_wrap(~geo_value, scales = \"free_y\", ncol = 3) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %y\") +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  labs(\n    title = \"Using simple linear regression\", x = \"Date\",\n    y = \"Reported COVID-19 case rates\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCompare those forecasts with a related set using Gradient Boosting.\n\n\nCode\nggplot(\n  can_fc %&gt;% filter(engine_type == \"xgboost\"),\n  aes(x = fc_target_date, group = time_value)\n) +\n  coord_cartesian(xlim = lubridate::ymd(c(\"2020-12-01\", NA))) +\n  geom_line(\n    data = can_latest, aes(x = time_value, y = cr_7dav),\n    inherit.aes = FALSE, color = \"gray50\"\n  ) +\n  geom_ribbon(aes(ymin = `0.05`, ymax = `0.95`, fill = geo_value),\n    alpha = 0.4\n  ) +\n  geom_line(aes(y = fc_.pred)) +\n  geom_point(aes(y = fc_.pred), size = 0.5) +\n  geom_vline(aes(xintercept = time_value), linetype = 2, alpha = 0.5) +\n  facet_wrap(~geo_value, scales = \"free_y\", ncol = 3) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %y\") +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  labs(\n    title = \"Using boosted regression trees\", x = \"Date\",\n    y = \"Reported COVID-19 case rates\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nBoth approaches tend to produce quite volatile forecasts (point predictions) and/or are overly confident (very narrow bands), particularly when boosted regression trees are used. But as this is meant to be a simple demonstration of sliding with different engines in arx_forecaster, we may devote another vignette to work on improving the predictive modelling using the suite of tools available in epipredict."
  },
  {
    "objectID": "sliding-forecasters.html#pseudoprospective-vs.-unfaithful-retrospective-forecasting",
    "href": "sliding-forecasters.html#pseudoprospective-vs.-unfaithful-retrospective-forecasting",
    "title": "14  Pseudo-prospective forecast inspection",
    "section": "14.2 Pseudoprospective vs. unfaithful retrospective forecasting",
    "text": "14.2 Pseudoprospective vs. unfaithful retrospective forecasting\n\n14.2.1 Example using case data from US states\nWe will now run pseudoprospective forecasts based on properly-versioned data (that would have been available in real-time) to forecast future COVID-19 case rates from current and past COVID-19 case rates for all states. That is, we can make forecasts on the archive, us_archive, and compare those to forecasts on (time windows of) the latest data, us_latest, using the same general set-up as above. For pseudoprospective forecasting, note that us_archive is fed into epix_slide(), while for simpler (unfaithful) retrospective forecasting, us_latest is fed into epi_slide(). #%% update to include percent_cli after that issue is fixed?\n\nk_week_versioning &lt;- function(ahead, version = c(\"faithful\", \"unfaithful\")) {\n  version &lt;- match.arg(version)\n  if (version == \"faithful\") {\n    epix_slide(\n      us_archive,\n      ~ arx_forecaster(\n        .x, \"case_rate\", c(\"case_rate\", \"percent_cli\"),\n        args_list = arx_args_list(ahead = ahead)\n      )$predictions,\n      before = 120 - 1,\n      ref_time_values = fc_time_values,\n      new_col_name = \"fc\"\n    ) %&gt;%\n      mutate(version = \"version faithful\") %&gt;%\n      rename(geo_value = \"fc_geo_value\")\n  } else {\n    k_week_ahead(\n      us_latest, \"case_rate\", c(\"case_rate\", \"percent_cli\"),\n      ahead, linear_reg()\n    ) %&gt;% mutate(version = \"not version faithful\")\n  }\n}\n\n# Generate the forecasts, and bind them together\nfc &lt;- bind_rows(\n  map(aheads, ~ k_week_versioning(.x, \"faithful\")) %&gt;% list_rbind(),\n  map(aheads, ~ k_week_versioning(.x, \"unfaithful\")) %&gt;% list_rbind()\n) %&gt;% pivot_quantiles(fc_.pred_distn)\n\nNow we can plot the results on top of the latest case rates. As before, we will only display and focus on the results for FL and CA for simplicity.\n\n\nCode\nfc_cafl &lt;- fc %&gt;% filter(geo_value %in% c(\"ca\", \"fl\"))\nlatest_cafl &lt;- us_latest %&gt;% filter(geo_value %in% c(\"ca\", \"fl\"))\n\nggplot(fc_cafl, aes(x = fc_target_date, group = time_value)) +\n  geom_line(\n    data = latest_cafl, aes(x = time_value, y = case_rate),\n    inherit.aes = FALSE, color = \"gray50\"\n  ) +\n  geom_ribbon(aes(ymin = `0.05`, ymax = `0.95`, fill = version), alpha = 0.4) +\n  geom_line(aes(y = fc_.pred)) +\n  geom_point(aes(y = fc_.pred), size = 0.5) +\n  geom_vline(aes(xintercept = time_value), linetype = 2, alpha = 0.5) +\n  facet_grid(version ~ geo_value, scales = \"free\") +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %y\") +\n  scale_y_continuous(expand = expansion(c(0, 0.05))) +\n  labs(x = \"Date\", y = \"Reported COVID-19 case rates\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nAgain, we observe that the results are not great for these two states, but that’s likely due to the simplicity of the model (ex. the omission of key factors such as age and public health measures) and the quality of the data (ex. we have not personally corrected for anomalies in the data).\nWe shall leave it to the reader to try the above version aware and unaware forecasting exercise on the Canadian case rate data. The above code for the American state data should be readily adaptable for this purpose."
  },
  {
    "objectID": "sliding-forecasters.html#footnotes",
    "href": "sliding-forecasters.html#footnotes",
    "title": "14  Pseudo-prospective forecast inspection",
    "section": "",
    "text": "Note that, despite the above caveats, simple models like this tend to out-perform many far more complicated models in the online Covid forecasting due to those models high variance predictions.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bien, Jacob, Logan Brooks, David Farrow, Pedrito Maynard-Zhang, Alex\nReinhart, Ryan Tibshirani, and Samuel Gratzl. 2023. Epidatr: Client\nfor Delphi’s Epidata API. https://github.com/cmu-delphi/epidatr.\n\n\nBrooks, Logan, Daniel McDonald, Evan Ray, and Ryan Tibshirani. 2023.\nEpiprocess: Tools for Basic Signal Processing in Epidemiology.\nhttps://cmu-delphi.github.io/epiprocess/.\n\n\nCramer, Estee Y., Evan L. Ray, Velma K. Lopez, Johannes Bracher, Andrea\nBrennen, Alvaro J. Castro Rivadeneira, Aaron Gerding, et al. 2022.\n“Evaluation of Individual and Ensemble Probabilistic Forecasts of\nCOVID-19 Mortality in the United States.” Proceedings of the\nNational Academy of Sciences 119 (15): e2113561119. https://doi.org/10.1073/pnas.2113561119.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times\nMade Easy with lubridate.”\nJournal of Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.\n\n\nKuhn, Max, and Davis Vaughan. 2023. Parsnip: A Common API to\nModeling and Analysis Functions. https://CRAN.R-project.org/package=parsnip.\n\n\nLutz, Chelsea S, Mimi P Huynh, Monica Schroeder, Sophia Anyatonwu, F\nScott Dahlgren, Gregory Danyluk, Danielle Fernandez, et al. 2019.\n“Applying Infectious Disease Forecasting to Public Health: A Path\nForward Using Influenza Forecasting Examples.” BMC Public\nHealth 19 (1): 1–12. https://doi.org/10.1186/s12889-019-7966-8.\n\n\nMcDonald, Daniel J. 2023. Epidatasets: Epidemiological Data for\nDelphi Tooling Examples. https://cmu-delphi.github.io/epidatasets/.\n\n\nMcDonald, Daniel J, Jacob Bien, Alden Green, Addison J Hu, Nat DeFries,\nSangwon Hyun, Natalia L Oliveira, et al. 2021. “Can Auxiliary\nIndicators Improve COVID-19 Forecasting and Hotspot\nPrediction?” Proceedings of the National Academy of\nSciences 118: e2111453118. https://doi.org/10.1073/pnas.2111453118.\n\n\nMcDonald, Daniel, Ryan Tibshirani, Logan Brooks, Rachel Lobay, Maggie\nLiu, Ken Mawer, and Chloe You. 2023. Epipredict: Basic Epidemiology\nForecasting Methods.\n\n\nMüller, Kirill, and Hadley Wickham. 2023. Tibble: Simple Data\nFrames. https://CRAN.R-project.org/package=tibble.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nReinhart, Alex, Logan Brooks, Maria Jahja, Aaron Rumack, Jingjing Tang,\nWael Al Saeed, Taylor Arnold, et al. 2021. “An Open Repository of\nReal-Time COVID-19 Indicators.” Proceedings of the National\nAcademy of Sciences 118: e2111452118. https://doi.org/10.1073/pnas.2111452118.\n\n\nSpinu, Vitalie, Garrett Grolemund, and Hadley Wickham. 2023.\nLubridate: Make Dealing with Dates a Little Easier. https://CRAN.R-project.org/package=lubridate.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2022. Stringr: Simple, Consistent Wrappers for Common String\nOperations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2023a. Forcats: Tools for Working with Categorical Variables\n(Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2023b. Tidyverse: Easily Install and Load the Tidyverse.\nhttps://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey\nDunnington. 2023. Ggplot2: Create Elegant Data Visualisations Using\nthe Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis\nVaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Lionel Henry. 2023. Purrr: Functional\nProgramming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. Readr: Read\nRectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr:\nTidy Messy Data. https://CRAN.R-project.org/package=tidyr."
  }
]
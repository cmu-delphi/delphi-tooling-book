[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "",
    "text": "Preface\nThis long-form vignette describes some of the functionality of the epipredict R package, with an eye toward creating various types of forecasters, from simple baselines to more elaborate customizations."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "Installation",
    "text": "Installation\nYou can install the development version of epipredict from GitHub with:\n# install.packages(\"remotes\")\nremotes::install_github(\"cmu-delphi/epipredict\")"
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "Documentation",
    "text": "Documentation\nYou can view the complete documentation for the package at https://cmu-delphi.github.io/epipredict."
  },
  {
    "objectID": "index.html#goals-for-epipredict",
    "href": "index.html#goals-for-epipredict",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "Goals for epipredict",
    "text": "Goals for epipredict\nWe currently provide:\n\nA set of basic, easy-to-use forecasters that work out of the box. You can do a reasonable amount of customization on them. For the basic forecasters, we currently provide:\n\nBaseline flat-line forecaster\nAutoregressive forecaster\nAutoregressive (multinomial) classifier\n\nA framework for creating custom forecasters out of modular components. There are four types of components:\n\nPreprocessor: do things to the raw data before model training that should be reproduced at test time.\nTrainer: train a model on data, resulting in a fitted model object\nPredictor: make predictions, using a fitted model object\nPostprocessor: do things to the predictions before returning\n\n\nTarget audiences:\n\nBasic. Has data, calls forecaster with default arguments.\nIntermediate. Wants to examine changes to the arguments, take advantage of some built in flexibility.\nAdvanced. Wants to write their own forecasters. Maybe willing to build up from some components that we write.\n\nEven the Advanced user should find their task to be relatively easy. Examples of these tasks are illustrated in the vignettes and articles."
  },
  {
    "objectID": "index.html#intermediate-example",
    "href": "index.html#intermediate-example",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "Intermediate example",
    "text": "Intermediate example\nThe package comes with some built-in historical data for illustration, but up-to-date versions of this could be downloaded with the {covidcast} package and processed using {epiprocess}.1\n\nlibrary(epipredict)\njhu &lt;- case_death_rate_subset\njhu\n\n#&gt; An `epi_df` object, 20,496 x 4 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25\n#&gt; \n#&gt; # A tibble: 20,496 × 4\n#&gt;   geo_value time_value case_rate death_rate\n#&gt; * &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 ak        2020-12-31      35.9      0.158\n#&gt; 2 al        2020-12-31      65.1      0.438\n#&gt; 3 ar        2020-12-31      66.0      1.27 \n#&gt; 4 as        2020-12-31       0        0    \n#&gt; 5 az        2020-12-31      76.8      1.10 \n#&gt; 6 ca        2020-12-31      96.0      0.751\n#&gt; # ℹ 20,490 more rows\n\n\nTo create and train a simple auto-regressive forecaster to predict the death rate two weeks into the future using past (lagged) deaths and cases, we could use the following function.\n\ntwo_week_ahead &lt;- arx_forecaster(\n  jhu,\n  outcome = \"death_rate\",\n  predictors = c(\"case_rate\", \"death_rate\"),\n  args_list = arx_args_list(\n    lags = list(c(0, 1, 2, 3, 7, 14), c(0, 7, 14)),\n    ahead = 14\n  )\n)\n\nIn this case, we have used a number of different lags for the case rate, while only using 3 weekly lags for the death rate (as predictors). The result is both a fitted model object which could be used any time in the future to create different forecasts, as well as a set of predicted values (and prediction intervals) for each location 14 days after the last available time value in the data.\n\ntwo_week_ahead$epi_workflow\n\n#&gt; ══ Epi Workflow [trained] ═══════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; Postprocessor: Frosting\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 5 Recipe Steps\n#&gt; \n#&gt; • step_epi_lag()\n#&gt; • step_epi_lag()\n#&gt; • step_epi_ahead()\n#&gt; • step_naomit()\n#&gt; • step_naomit()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)    lag_0_case_rate    lag_1_case_rate    lag_2_case_rate  \n#&gt;        -0.0073358          0.0030365          0.0012467          0.0009536  \n#&gt;   lag_3_case_rate    lag_7_case_rate   lag_14_case_rate   lag_0_death_rate  \n#&gt;         0.0011425          0.0012481          0.0003041          0.1351769  \n#&gt;  lag_7_death_rate  lag_14_death_rate  \n#&gt;         0.1471127          0.1062473  \n#&gt; \n#&gt; ── Postprocessor ────────────────────────────────────────────────────────────\n#&gt; 5 Frosting Layers\n#&gt; \n#&gt; • layer_predict()\n#&gt; • layer_residual_quantiles()\n#&gt; • layer_add_forecast_date()\n#&gt; • layer_add_target_date()\n#&gt; • layer_threshold()\n\n\nThe fitted model here involved preprocessing the data to appropriately generate lagged predictors, estimating a linear model with stats::lm() and then postprocessing the results to be meaningful for epidemiological tasks. We can also examine the predictions.\n\ntwo_week_ahead$predictions\n\n#&gt; An `epi_df` object, 56 x 6 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25\n#&gt; \n#&gt; # A tibble: 56 × 6\n#&gt;   geo_value time_value .pred         .pred_distn forecast_date target_date\n#&gt; * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;              &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n#&gt; 1 ak        2021-12-31 0.449 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-14 \n#&gt; 2 al        2021-12-31 0.574 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-14 \n#&gt; 3 ar        2021-12-31 0.673 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-14 \n#&gt; 4 as        2021-12-31 0     [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-14 \n#&gt; 5 az        2021-12-31 0.679 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-14 \n#&gt; 6 ca        2021-12-31 0.575 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-14 \n#&gt; # ℹ 50 more rows\n\n\nThe results above show a distributional forecast produced using data through the end of 2021 for the 14th of January 2022. A prediction for the death rate per 100K inhabitants is available for every state (geo_value) along with a 90% predictive interval. The figure below displays the forecast for a small handful of states. The vertical black line is the forecast date. The forecast doesn’t appear to be particularly good, but our choices above were intended to be illustrative of the functionality rather than optimized for accuracy."
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "Contents",
    "text": "Contents\nThe remainder of this book examines this software in more detail, illustrating some of the flexibility that is available."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction to Epidemiological Forecasting",
    "section": "",
    "text": "COVIDcast data and other epidemiological signals for non-Covid related illnesses are available with {epidatr}, which interfaces directly to Delphi’s Epidata API.↩︎"
  },
  {
    "objectID": "why-this-package.html#a-brief-overview-about-the-epi-universe",
    "href": "why-this-package.html#a-brief-overview-about-the-epi-universe",
    "title": "1  Motivation",
    "section": "1.1 A brief overview about the epi universe",
    "text": "1.1 A brief overview about the epi universe\nSince early 2020, our research group has worked with data partners to publicaly disseminate information about numerous COVID-19 indicators, which we and others then use for nowcasting and short-term forecasting. Prior to that we worked mostly on influenza, dengue and norovirus modelling.\nOur overarching goal is to build interworking, community-driven packages for epidemiologic tracking and forcasting. To that end, the epi. universe is currently comprised of three main packages: epidatr (in Python epidatpy), epiprocess, and epipredict. Each has their own role in the forecasting process: The epidatr (epidatpy) package is used to fetch data, epiprocess is used to standardize, clean and process data, and finally the epipredict package is used for building and evaluating predictive models. Here is a diagram depicting the basic workflow for using these packages:\n\nHere’s a brief overview about what each package offers…\nepidatr - Obtain epidemiological surveillance data from Delphi’s Epidata API\n\nQuick access to data on the spread and impact of the COVID-19 pandemic\nAccess to data about other diseases, including influenza, dengue, and more\nProvide geographic and temporal detail\nTracked through several data streams\n\nepiprocess - Basic processing operations and data structures\n\nCalculate rolling statistics\nFill / impute gaps\nExamine correlations\nStore revision history smartly\nInspect revision patterns\nDetect / correct outliers\n\nepipredict - A forecasting framework\n\nFlatline forecaster\nAR-type models\nBacktest using the versioned data\nEasily create features\nQuickly pivot to new tasks\nHighly customizable for advanced users\n\nThe focus of this book is epipredict. By the end of this book, you should be equipped with the tools and knowledge to build your own basic and customizable forecasting models. At that point, our hope is that some may take things a step further and leverage this package to build on it and advance the frontier of predictive modelling."
  },
  {
    "objectID": "why-this-package.html#main-goal-for-the-epipredict-package",
    "href": "why-this-package.html#main-goal-for-the-epipredict-package",
    "title": "1  Motivation",
    "section": "1.2 Main goal for the epipredict package",
    "text": "1.2 Main goal for the epipredict package\nAt a high level, our goal with epipredict is to make running simple machine learning / statistical forecasters for epidemiology easy. However, this package is extremely extensible, and that is part of its utility. Our hope is that it is easy for users with epidemiology training and some statistics to fit baseline models while still allowing those with more nuanced statistical understanding to create complicated specializations using the same framework.\nServing both populations is the main motivation for our efforts, but at the same time, we have tried hard to make it useful."
  },
  {
    "objectID": "why-this-package.html#why-doesnt-this-package-already-exist",
    "href": "why-this-package.html#why-doesnt-this-package-already-exist",
    "title": "1  Motivation",
    "section": "1.3 Why doesn’t this package already exist?",
    "text": "1.3 Why doesn’t this package already exist?\n\nParts of it actually DO exist. There’s a universe called tidymodels. It handles pre-processing, training, and prediction, bound together, through a package called workflows. We built epipredict on top of that setup. In this way, you CAN use almost everything they provide.\nHowever, workflows doesn’t do post-processing. And nothing in the tidyverse handles panel data.\nThe tidy-team doesn’t have plans to do either of these things. (We checked).\nThere are two packages that do time series built on tidymodels, but it’s “basic” time series: 1-step AR models, exponential smoothing, STL decomposition, etc.1 Our group has not prioritized these sorts of models for epidemic forecasting, but one could also integrate these methods into our framework."
  },
  {
    "objectID": "flatline-forecaster.html#example-of-using-the-flatline-forecaster",
    "href": "flatline-forecaster.html#example-of-using-the-flatline-forecaster",
    "title": "2  Introducing the flatline forecaster",
    "section": "2.1 Example of using the flatline forecaster",
    "text": "2.1 Example of using the flatline forecaster\n\n2.1.1 Load required packages\n\nlibrary(epipredict)\nlibrary(epiprocess)\n\n\n\n2.1.2 A brief introduction to the dataset\nWe will use the case_death_rate_subset dataset that comes with the epipredict package. In brief, this is a subset of the JHU daily COVID-19 cases and deaths by state. While this dataset ranges from Dec 31, 2020 to Dec 31, 2021, we will only consider a small subset at the end of that range to keep our example relatively simple.\n\njhu &lt;- case_death_rate_subset %&gt;%\n  dplyr::filter(time_value &gt;= as.Date(\"2021-09-01\"))\n\njhu\n\n#&gt; An `epi_df` object, 6,832 x 4 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25\n#&gt; \n#&gt; # A tibble: 6,832 × 4\n#&gt;   geo_value time_value case_rate death_rate\n#&gt; * &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 ak        2021-09-01      75.3      0.198\n#&gt; 2 al        2021-09-01     113.       0.845\n#&gt; 3 ar        2021-09-01      68.5      0.919\n#&gt; 4 as        2021-09-01       0        0    \n#&gt; 5 az        2021-09-01      48.8      0.414\n#&gt; 6 ca        2021-09-01      38.4      0.246\n#&gt; # ℹ 6,826 more rows\n\n\n\n\n2.1.3 The basic mechanics of the flatline forecaster\nThe simplest to create and train a flatline forecaster to predict the death rate one week into the future, is to input the epi_df and the name of the column from it that we want to predict in the flatline_forecaster function.\n\none_week_ahead &lt;- flatline_forecaster(jhu, outcome = \"death_rate\")\none_week_ahead\n\n#&gt; $predictions\n#&gt; An `epi_df` object, 56 x 6 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25\n#&gt; \n#&gt; # A tibble: 56 × 6\n#&gt;   geo_value time_value  .pred         .pred_distn forecast_date target_date\n#&gt; * &lt;chr&gt;     &lt;date&gt;      &lt;dbl&gt;              &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n#&gt; 1 ak        2021-12-31 0.0395 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 2 al        2021-12-31 0.107  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 3 ar        2021-12-31 0.490  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 4 as        2021-12-31 0      [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 5 az        2021-12-31 0.608  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; 6 ca        2021-12-31 0.142  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-07 \n#&gt; # ℹ 50 more rows\n#&gt; \n#&gt; $epi_workflow\n#&gt; ══ Epi Workflow [trained] ═══════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; Postprocessor: Frosting\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; • step_epi_ahead()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; $residuals\n#&gt; # A tibble: 7,224 × 2\n#&gt;   geo_value .resid\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 ak            NA\n#&gt; 2 al            NA\n#&gt; 3 ar            NA\n#&gt; 4 as            NA\n#&gt; 5 az            NA\n#&gt; 6 ca            NA\n#&gt; # ℹ 7,218 more rows\n#&gt; \n#&gt; $.pred\n#&gt; # A tibble: 56 × 2\n#&gt;   geo_value  .pred\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 ak        0.0395\n#&gt; 2 al        0.107 \n#&gt; 3 ar        0.490 \n#&gt; 4 as        0     \n#&gt; 5 az        0.608 \n#&gt; 6 ca        0.142 \n#&gt; # ℹ 50 more rows\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"flatline\"\n#&gt; ── Postprocessor ────────────────────────────────────────────────────────────\n#&gt; 5 Frosting Layers\n#&gt; \n#&gt; • layer_predict()\n#&gt; • layer_residual_quantiles()\n#&gt; • layer_add_forecast_date()\n#&gt; • layer_add_target_date()\n#&gt; • layer_threshold()\n\n\nThe result is both a fitted model object which could be used any time in the future to create different forecasts, as well as a set of predicted values and prediction intervals for each location 7 days after the last available time value in the data, which is Dec 31, 2021. Note that 7 days is the default number of time steps ahead of the forecast date in which forecasts should be produced. To change this, you must change the value of the ahead parameter in the list of additional arguments flatline_args_list(). Let’s change this to 5 days to get some practice.\n\nfive_days_ahead &lt;- flatline_forecaster(\n  jhu,\n  outcome = \"death_rate\",\n  flatline_args_list(ahead = 5L)\n)\n\nfive_days_ahead\n\n#&gt; $predictions\n#&gt; An `epi_df` object, 56 x 6 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25\n#&gt; \n#&gt; # A tibble: 56 × 6\n#&gt;   geo_value time_value  .pred         .pred_distn forecast_date target_date\n#&gt; * &lt;chr&gt;     &lt;date&gt;      &lt;dbl&gt;              &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n#&gt; 1 ak        2021-12-31 0.0395 [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 2 al        2021-12-31 0.107  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 3 ar        2021-12-31 0.490  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 4 as        2021-12-31 0      [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 5 az        2021-12-31 0.608  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 6 ca        2021-12-31 0.142  [0.05, 0.95]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; # ℹ 50 more rows\n#&gt; \n#&gt; $epi_workflow\n#&gt; ══ Epi Workflow [trained] ═══════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; Postprocessor: Frosting\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; • step_epi_ahead()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; $residuals\n#&gt; # A tibble: 7,112 × 2\n#&gt;   geo_value .resid\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 ak            NA\n#&gt; 2 al            NA\n#&gt; 3 ar            NA\n#&gt; 4 as            NA\n#&gt; 5 az            NA\n#&gt; 6 ca            NA\n#&gt; # ℹ 7,106 more rows\n#&gt; \n#&gt; $.pred\n#&gt; # A tibble: 56 × 2\n#&gt;   geo_value  .pred\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 ak        0.0395\n#&gt; 2 al        0.107 \n#&gt; 3 ar        0.490 \n#&gt; 4 as        0     \n#&gt; 5 az        0.608 \n#&gt; 6 ca        0.142 \n#&gt; # ℹ 50 more rows\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"flatline\"\n#&gt; ── Postprocessor ────────────────────────────────────────────────────────────\n#&gt; 5 Frosting Layers\n#&gt; \n#&gt; • layer_predict()\n#&gt; • layer_residual_quantiles()\n#&gt; • layer_add_forecast_date()\n#&gt; • layer_add_target_date()\n#&gt; • layer_threshold()\n\n\nWe could also specify that we want a 80% predictive interval by changing the levels. The default 0.05 and 0.95 levels/quantiles give us 90% predictive interval.\n\nfive_days_ahead &lt;- flatline_forecaster(\n  jhu,\n  outcome = \"death_rate\",\n  flatline_args_list(ahead = 5L, levels = c(0.1, 0.9))\n)\n\nfive_days_ahead\n\n#&gt; $predictions\n#&gt; An `epi_df` object, 56 x 6 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25\n#&gt; \n#&gt; # A tibble: 56 × 6\n#&gt;   geo_value time_value  .pred       .pred_distn forecast_date target_date\n#&gt; * &lt;chr&gt;     &lt;date&gt;      &lt;dbl&gt;            &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n#&gt; 1 ak        2021-12-31 0.0395 [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 2 al        2021-12-31 0.107  [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 3 ar        2021-12-31 0.490  [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 4 as        2021-12-31 0      [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 5 az        2021-12-31 0.608  [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 6 ca        2021-12-31 0.142  [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; # ℹ 50 more rows\n#&gt; \n#&gt; $epi_workflow\n#&gt; ══ Epi Workflow [trained] ═══════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; Postprocessor: Frosting\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; • step_epi_ahead()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; $residuals\n#&gt; # A tibble: 7,112 × 2\n#&gt;   geo_value .resid\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 ak            NA\n#&gt; 2 al            NA\n#&gt; 3 ar            NA\n#&gt; 4 as            NA\n#&gt; 5 az            NA\n#&gt; 6 ca            NA\n#&gt; # ℹ 7,106 more rows\n#&gt; \n#&gt; $.pred\n#&gt; # A tibble: 56 × 2\n#&gt;   geo_value  .pred\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 ak        0.0395\n#&gt; 2 al        0.107 \n#&gt; 3 ar        0.490 \n#&gt; 4 as        0     \n#&gt; 5 az        0.608 \n#&gt; 6 ca        0.142 \n#&gt; # ℹ 50 more rows\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"flatline\"\n#&gt; ── Postprocessor ────────────────────────────────────────────────────────────\n#&gt; 5 Frosting Layers\n#&gt; \n#&gt; • layer_predict()\n#&gt; • layer_residual_quantiles()\n#&gt; • layer_add_forecast_date()\n#&gt; • layer_add_target_date()\n#&gt; • layer_threshold()\n\n\nTo see the other arguments that you may modify, please see ?flatline_args_list(). For now, we will move on to looking at the workflow.\n\nfive_days_ahead$epi_workflow\n\n#&gt; ══ Epi Workflow [trained] ═══════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; Postprocessor: Frosting\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; • step_epi_ahead()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; $residuals\n#&gt; # A tibble: 7,112 × 2\n#&gt;   geo_value .resid\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 ak            NA\n#&gt; 2 al            NA\n#&gt; 3 ar            NA\n#&gt; 4 as            NA\n#&gt; 5 az            NA\n#&gt; 6 ca            NA\n#&gt; # ℹ 7,106 more rows\n#&gt; \n#&gt; $.pred\n#&gt; # A tibble: 56 × 2\n#&gt;   geo_value  .pred\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 ak        0.0395\n#&gt; 2 al        0.107 \n#&gt; 3 ar        0.490 \n#&gt; 4 as        0     \n#&gt; 5 az        0.608 \n#&gt; 6 ca        0.142 \n#&gt; # ℹ 50 more rows\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"flatline\"\n#&gt; ── Postprocessor ────────────────────────────────────────────────────────────\n#&gt; 5 Frosting Layers\n#&gt; \n#&gt; • layer_predict()\n#&gt; • layer_residual_quantiles()\n#&gt; • layer_add_forecast_date()\n#&gt; • layer_add_target_date()\n#&gt; • layer_threshold()\n\n\nThe fitted model here was based on minimal pre-processing of the data, estimating a flatline model, and then post-processing the results to be meaningful for epidemiological tasks. To look deeper into the pre-processing, model and processing parts individually, you may use the $ operator after epi_workflow. For example, let’s examine the pre-processing part in more detail.\n\nfive_days_ahead$epi_workflow$pre\n\n#&gt; $actions\n#&gt; $actions$recipe\n#&gt; $recipe\n#&gt; \n#&gt; $blueprint\n#&gt; Recipe blueprint: \n#&gt;  \n#&gt; # Predictors: 0 \n#&gt;   # Outcomes: 0 \n#&gt;    Intercept: FALSE \n#&gt; Novel Levels: FALSE \n#&gt;  Composition: tibble \n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"action_recipe\" \"action_pre\"    \"action\"       \n#&gt; \n#&gt; \n#&gt; $mold\n#&gt; $mold$predictors\n#&gt; # A tibble: 7,112 × 3\n#&gt;   time_value geo_value death_rate\n#&gt;   &lt;date&gt;     &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 2021-08-27 ak                NA\n#&gt; 2 2021-08-27 al                NA\n#&gt; 3 2021-08-27 ar                NA\n#&gt; 4 2021-08-27 as                NA\n#&gt; 5 2021-08-27 az                NA\n#&gt; 6 2021-08-27 ca                NA\n#&gt; # ℹ 7,106 more rows\n#&gt; \n#&gt; $mold$outcomes\n#&gt; # A tibble: 7,112 × 1\n#&gt;   ahead_5_death_rate\n#&gt;                &lt;dbl&gt;\n#&gt; 1              0.198\n#&gt; 2              0.845\n#&gt; 3              0.919\n#&gt; 4              0    \n#&gt; 5              0.414\n#&gt; 6              0.246\n#&gt; # ℹ 7,106 more rows\n#&gt; \n#&gt; $mold$blueprint\n#&gt; Recipe blueprint: \n#&gt;  \n#&gt; # Predictors: 3 \n#&gt;   # Outcomes: 0 \n#&gt;    Intercept: FALSE \n#&gt; Novel Levels: FALSE \n#&gt;  Composition: tibble \n#&gt; \n#&gt; $mold$extras\n#&gt; $mold$extras$roles\n#&gt; $mold$extras$roles$time_value\n#&gt; # A tibble: 7,112 × 1\n#&gt;   time_value\n#&gt;   &lt;date&gt;    \n#&gt; 1 2021-08-27\n#&gt; 2 2021-08-27\n#&gt; 3 2021-08-27\n#&gt; 4 2021-08-27\n#&gt; 5 2021-08-27\n#&gt; 6 2021-08-27\n#&gt; # ℹ 7,106 more rows\n#&gt; \n#&gt; $mold$extras$roles$geo_value\n#&gt; # A tibble: 7,112 × 1\n#&gt;   geo_value\n#&gt;   &lt;chr&gt;    \n#&gt; 1 ak       \n#&gt; 2 al       \n#&gt; 3 ar       \n#&gt; 4 as       \n#&gt; 5 az       \n#&gt; 6 ca       \n#&gt; # ℹ 7,106 more rows\n#&gt; \n#&gt; $mold$extras$roles$raw\n#&gt; # A tibble: 7,112 × 1\n#&gt;   case_rate\n#&gt;       &lt;dbl&gt;\n#&gt; 1        NA\n#&gt; 2        NA\n#&gt; 3        NA\n#&gt; 4        NA\n#&gt; 5        NA\n#&gt; 6        NA\n#&gt; # ℹ 7,106 more rows\n#&gt; \n#&gt; \n#&gt; \n#&gt; \n#&gt; $case_weights\n#&gt; NULL\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"stage_pre\" \"stage\"\n\n\nUnder Operations, we can see that the pre-processing operations were to lead the death rate by 5 days (step_epi_ahead()) and that the # of recent observations used in the training window were not limited (in step_training_window() as n_training = Inf in flatline_args_list()). You should also see the molded/pre-processed training data.\nFor symmetry, let’s have a look at the post-processing.\n\nfive_days_ahead$epi_workflow$post\n\n#&gt; $actions\n#&gt; $actions$frosting\n#&gt; $frosting\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"action_post\" \"action\"     \n#&gt; \n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"stage_post\" \"stage\"\n\n\nThe post-processing operations in the order the that were performed were to create the predictions and the predictive intervals, add the forecast and target dates and bound the predictions at zero.\nWe can also easily examine the predictions themselves.\n\nfive_days_ahead$predictions\n\n#&gt; An `epi_df` object, 56 x 6 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25\n#&gt; \n#&gt; # A tibble: 56 × 6\n#&gt;   geo_value time_value  .pred       .pred_distn forecast_date target_date\n#&gt; * &lt;chr&gt;     &lt;date&gt;      &lt;dbl&gt;            &lt;dist&gt; &lt;date&gt;        &lt;date&gt;     \n#&gt; 1 ak        2021-12-31 0.0395 [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 2 al        2021-12-31 0.107  [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 3 ar        2021-12-31 0.490  [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 4 as        2021-12-31 0      [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 5 az        2021-12-31 0.608  [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; 6 ca        2021-12-31 0.142  [0.1, 0.9]&lt;q-rng&gt; 2021-12-31    2022-01-05 \n#&gt; # ℹ 50 more rows\n\n\nThe results above show a distributional forecast produced using data through the end of 2021 for the January 5, 2022. A prediction for the death rate per 100K inhabitants along with a 95% predictive interval is available for every state (geo_value).\nThe figure below displays the prediction and prediction interval for three sample states: Arizona, New York, and Florida.\n\n\nCode\nsamp_geos &lt;- c(\"az\", \"ny\", \"fl\")\n\nhist &lt;- jhu %&gt;%\n  filter(geo_value %in% samp_geos)\n\npreds &lt;- five_days_ahead$predictions %&gt;%\n  filter(geo_value %in% samp_geos) %&gt;%\n  mutate(q = nested_quantiles(.pred_distn)) %&gt;%\n  unnest(q) %&gt;%\n  pivot_wider(names_from = tau, values_from = q)\n\nggplot(hist, aes(color = geo_value)) +\n  geom_line(aes(time_value, death_rate)) +\n  theme_bw() +\n  geom_errorbar(data = preds, aes(x = target_date, ymin = `0.1`, ymax = `0.9`)) +\n  geom_point(data = preds, aes(target_date, .pred)) +\n  geom_vline(data = preds, aes(xintercept = forecast_date)) +\n  scale_colour_viridis_d(name = \"\") +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"1 month\") +\n  facet_grid(geo_value ~ ., scales = \"free_y\") +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"Incident deaths per 100K\\n inhabitants\")\n\n\n\n\n\n\n\n\n\nThe vertical black line is the forecast date. Here the forecast seems pretty reasonable based on the past observations shown. In cases where the recent past is highly predictive of the near future, a simple flatline forecast may be respectable, but in more complex situations where there is more uncertainty of what’s to come, the flatline forecaster may be best relegated to being a baseline model and nothing more.\nTake for example what happens when we consider a wider range of target dates. That is, we will now predict for several different horizons or ahead values - in our case, 5 to 25 days ahead, inclusive. Since the flatline forecaster function forecasts at a single unique ahead value, we can use the map() function from purrr to apply the forecaster to each ahead value we want to use. Then, we row bind the list of results.\n\nout_df &lt;- purrr::map(\n  1:28,\n  ~ flatline_forecaster(\n    jhu,\n    outcome = \"death_rate\",\n    args_list = flatline_args_list(ahead = .x)\n  )$predictions\n) %&gt;%\n  list_rbind()\n\nThen, we proceed as we did before. The only difference from before is that we’re using out_df where we had five_days_ahead$predictions.\n\n\nCode\npreds &lt;- out_df %&gt;%\n  filter(geo_value %in% samp_geos) %&gt;%\n  mutate(q = nested_quantiles(.pred_distn)) %&gt;%\n  unnest(q) %&gt;%\n  pivot_wider(names_from = tau, values_from = q)\n\nggplot(hist) +\n  geom_line(aes(time_value, death_rate)) +\n  geom_ribbon(\n    data = preds,\n    aes(x = target_date, ymin = `0.05`, ymax = `0.95`, fill = geo_value)\n  ) +\n  geom_point(data = preds, aes(target_date, .pred, colour = geo_value)) +\n  geom_vline(data = preds, aes(xintercept = forecast_date)) +\n  scale_colour_viridis_d() +\n  scale_fill_viridis_d(alpha = .4) +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"1 month\") +\n  scale_y_continuous(expand = expansion(c(0, .05))) +\n  facet_grid(geo_value ~ ., scales = \"free_y\") +\n  labs(x = \"\", y = \"Incident deaths per 100K\\n inhabitants\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nNow, you can really see the flat line trend in the predictions. And you may also observe that as we get further away from the forecast date, the more unnerving using a flatline prediction becomes. It feels increasingly unnatural.\nSo naturally the choice of forecaster relates to the time frame being considered. In general, using a flatline forecaster makes more sense for short-term forecasts than for long-term forecasts and for periods of great stability than in less stable times. Realistically, periods of great stability are rare. Moreover, in our model of choice we want to take into account more information about the past than just what happened at the most recent time point. So simple forecasters like the flatline forecaster don’t cut it as actual contenders in many real-life situations. However, they are not useless, just used for a different purpose. A simple model is often used to compare a more complex model to, which is why you may have seen such a model used as a baseline in the COVID Forecast Hub. The following blog post from Delphi explores the Hub’s ensemble accuracy relative to such a baseline model."
  },
  {
    "objectID": "flatline-forecaster.html#what-weve-learned-in-a-nutshell",
    "href": "flatline-forecaster.html#what-weve-learned-in-a-nutshell",
    "title": "2  Introducing the flatline forecaster",
    "section": "2.2 What we’ve learned in a nutshell",
    "text": "2.2 What we’ve learned in a nutshell\nThough the flatline forecaster is a very basic model with limited customization, it is about as steady and predictable as a model can get. So it provides a good reference or baseline to compare more complicated models to."
  },
  {
    "objectID": "tidymodels-intro.html#an-example-using-the-penguins-dataset",
    "href": "tidymodels-intro.html#an-example-using-the-penguins-dataset",
    "title": "3  Introduction to Tidymodels",
    "section": "3.1 An example using the penguins dataset",
    "text": "3.1 An example using the penguins dataset\nWe will now explore the tidymodels functions using the penguins dataset that we introduced and used in Regression in Tidymodels.\n\n3.1.1 Load packages\nNote that tidymodels automatically loads some very useful tidyverse packages for us, including fan favourites like dplyr and ggplot2.\n\nlibrary(tidymodels)\n\n\n\n3.1.2 Simplify dataset\nTo keep the focus on learning how to use tidymodels, we will work with a simplified version of the dataset in which we will only use the complete cases/rows in the penguins dataset\n\npenguins &lt;- penguins %&gt;%\n  filter(complete.cases(.))\n\nhead(penguins)\n\n#&gt; # A tibble: 6 × 7\n#&gt;   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#&gt;   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n#&gt; 1 Adelie  Torgersen           39.1          18.7               181\n#&gt; 2 Adelie  Torgersen           39.5          17.4               186\n#&gt; 3 Adelie  Torgersen           40.3          18                 195\n#&gt; 4 Adelie  Torgersen           36.7          19.3               193\n#&gt; 5 Adelie  Torgersen           39.3          20.6               190\n#&gt; 6 Adelie  Torgersen           38.9          17.8               181\n#&gt; # ℹ 2 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;\n\n\nand we will only use the species, bill_length_mm, bill_depth_mm, and flipper_length_mm variables.\n\npenguins &lt;- penguins %&gt;%\n  select(c(species, bill_length_mm, bill_depth_mm, flipper_length_mm))\n\n\n\n3.1.3 Data sampling\nAfter fitting a model, make sure it is a good model. That is, don’t forget to test how the model performs. For this reason, it is customary to split data into distinct training and test sets at the onset. The training data is used to fit the model and the test data is used to assess model performance.\nThe initial_split() function from the rsample package is what we will use to split our dataset into a training and test set. The function by default uses 3/4 of data for training and reserves the remaining 1/4 for testing. Use the prop argument to change the proportion used for training. Note that this function gives a rsplit object and not a data frame and the output of the object shows the number of rows used for testing, training and the grand total.\n\nset.seed(123) # For reproduciblity, as when we split the data below\npenguins_split &lt;- initial_split(penguins, prop = 0.7)\npenguins_split\n\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;233/100/333&gt;\n\n\nTo see what observations were used for training and testing, use the training() and testing() functions respectively.\n\npenguins_split %&gt;%\n  training() %&gt;%\n  glimpse()\n\n#&gt; Rows: 233\n#&gt; Columns: 4\n#&gt; $ species           &lt;fct&gt; Gentoo, Adelie, Gentoo, Chinstrap, Adelie, Chinst…\n#&gt; $ bill_length_mm    &lt;dbl&gt; 59.6, 34.4, 45.2, 49.0, 41.4, 51.0, 44.9, 51.1, 5…\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 17.0, 18.4, 15.8, 19.5, 18.5, 18.8, 13.8, 16.5, 1…\n#&gt; $ flipper_length_mm &lt;int&gt; 230, 184, 215, 210, 202, 203, 212, 225, 210, 211,…\n\n\nNow, we’ll create a data frame for each of the training and test set:\n\ntrain_data &lt;- training(penguins_split)\ntest_data &lt;- testing(penguins_split)\n\n\n\n3.1.4 Pre-processing\nThe main goal of this step is to use data transformations to make the data suitable for modeling. Most transformations that one required for standard data analysis tasks can be achieved by dplyr, or another tidyverse package.\n\n3.1.4.1 The pre-processing interface\nBefore training the model, a recipe can be used to carry out the pre-processing required by the model.\nThe recipe() has two main arguments: a formula (with the same format as when doing [LINK TO VIGNETTE]) and a data argument, which is usually the training set as that is the data used to create the model. Hence, we have data = train_data here.\nIn our example, suppose that our goal is to predict penguin species from bill length, bill depth and flipper length, then our recipe function would look as follows:\n\nrecipe(species ~ ., data = train_data)\n\nThe point of recipe is to be a more general purpose formula. A number of packages are not formula-based. The ever-popular glmnet() function is one example because it takes in matrices for the x and y variables instead of a formula. So a recipe is useful because you can use a package like glmnet by following the same standard formula-based recipe format and simply specify later on in the modelling stage that the you would like to use glmnet.\nNow, after saying that you are making a recipe by way of the recipe() function, simply specify the transformations that you want to apply to your data in the necessary steps. Each data transformation is one step and all of the available pre-processing transformations all have the prefix of step_. Now, while there are many step functions available (here’s a list), we will only use the following three in our example.\n\nstep_corr() to remove variables which have large absolute correlations with others\nstep_center() to normalize numeric data to have a mean of zero\nstep_scale() to normalize numeric data to have a standard deviation of one\n\nOne of the advantages of having these pre-processing steps is that they help to simplify concepts that are difficult or a pain to enforce in coding. For example, centering could be a nuisance to implement from scratch because we would first have to calculate statistics (variable averages) from the training data and then use them on both the training and on the test data. Note that centering should not be done on the test data, rather on the training data to avoid data leakage (contamination of the test data by using statistics from the test data). In a recipe, the the estimation of the variable means using the training data and the application of these to center new data sets is done automatically, under the hood, and so spares the coder from having to manually implement it. The situation is similar for scaling numeric data (step_scale()).\nAnother useful feature of the tidymodels pre-processing interface is that each step can be applied to one specified variable, a group of variables, or all variables. The all_predictors() and all_outcomes() functions are particularly convenient to help minimize the amount of typing you need to do. For instance, if you wanted to apply step_center() to only the predictor variables, simply type step_center(all_predictors()) instead of listing out each and every predictor in the step function.\nNow, let’s try this all out on our example.\n\npenguins_recipe &lt;- recipe(species ~ ., data = train_data) %&gt;%\n  step_corr(all_predictors()) %&gt;%\n  step_center(all_predictors(), -all_outcomes()) %&gt;%\n  step_scale(all_predictors(), -all_outcomes())\n\nTo summarize, we obtained a recipe object, penguins_recipe, by putting the recipe() and step functions together on our training data that we had ready to go from sampling.\nNow, to get the recipe details, simply call the recipe object. The operations section details what pre-processing steps we’ve applied to the data. Notice that the steps shown here are in the order that they were input into the recipe and they specify the variables used in each step.\n\npenguins_recipe\n\n\n\n\n3.1.5 Model Training\nRecall that in R, the same type of model could be fit using several different packages, and each such package typically has it’s own style of interface. Two popular packages to fit random forest models are ranger and randomForest. One way that their interfaces differ is in the parameter name for the number of trees - ranger() has the parameter num.trees, whereas in randomForest has parameter ntree. Such differences do not make it simple to run the model in the other package.\nTidymodels created an single interface that supports the usage of both models. Moreover, this general interface supports an even wider range of functions that use perform random forest. The key part that points to the function and package to be used is the engine.\nLet’s see how this works in practice. In the below example, we’ll use the general rand_forest() function from tidymodels. In there, we can specify the number of trees by using the trees argument. Then, in set_engine() we specify that we want to use ranger’s version of random forest. Notice this follows the model specification format introduced in the [Regression in Tidymodels] chapter.\n\npenguins_ranger &lt;- rand_forest(trees = 100, mode = \"classification\") %&gt;%\n  set_engine(\"ranger\")\n\nNow, if we wanted to use a different package’s version of random forest, we could easily do that by simply swapping out the engine. To try this out, let’s use randomForest instead of ranger.\n\npenguins_rf &lt;- rand_forest(trees = 100, mode = \"classification\") %&gt;%\n  set_engine(\"randomForest\")\n\nFor the remainder of this tutorial, we’ll stick with using ranger for simplify. At this stage, we’re ready to pre-process and model. The first task of those two is to apply our recipe before we train and test our model, in that we must\n\nProcess the recipe using the training set.\nUse the recipe on the training set to get the finalized predictor set.\nUse the recipe on the predictor set to get the test set.\n\nA workflow can be used to pair model and processing tasks together. When different recipes are needed for different models, this is very useful so that you don’t have to keep track of separate model and recipe objects in your workspace. Hence, training and testing different workflows becomes easier.\nFor our example, we’ll try tidy model’s workflows package to pair our model and our recipe together.\n\npenguins_wflow &lt;- workflow() %&gt;%\n  add_model(penguins_ranger) %&gt;%\n  add_recipe(penguins_recipe)\n\npenguins_wflow\n\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 3 Recipe Steps\n#&gt; \n#&gt; • step_corr()\n#&gt; • step_center()\n#&gt; • step_scale()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 100\n#&gt; \n#&gt; Computational engine: ranger\n\n\nAfter that, we’re ready to fit the model to our training data. The fit() function is what we will use to prepare the the recipe and train the model from the finalized predictors.\n\npenguins_fit &lt;- penguins_wflow %&gt;% fit(data = train_data)\n\nThe resulting object contains both the recipe and fitted model. To extract the model, use the helper function of extract_fit_parsnip(), and to extract the recipe object, use extract_recipe(). We extract the model object below.\n\nextract_fit_parsnip(penguins_fit)\n\n#&gt; parsnip model object\n#&gt; \n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~100,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  100 \n#&gt; Sample size:                      233 \n#&gt; Number of independent variables:  3 \n#&gt; Mtry:                             1 \n#&gt; Target node size:                 10 \n#&gt; Variable importance mode:         none \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.02954337\n\n\nOne important thing to notice is that that if we wanted to use the randomForest model instead of the ranger model, all we’d need to do is replace the engine in the model specification; the rest of the code remains the same. We shall leave it to the reader to try this on their own and marvel at the beauty of having such a unifying interface.\n\n\n3.1.6 Use a trained workflow to predict\nUp to this point we have\n\nBuilt the model (penguins_ranger)\nCreated a pre-processing recipe (penguins_recipe),\nPaired the model and recipe (penguins_wflow), and\nTrained our workflow using fit().\n\nSo the next step is to use the trained workflow, penguins_fit, to predict with the test data. This is easily done with a call to predict().\n\npredict(penguins_fit, test_data)\n\n#&gt; # A tibble: 100 × 1\n#&gt;   .pred_class\n#&gt;   &lt;fct&gt;      \n#&gt; 1 Adelie     \n#&gt; 2 Adelie     \n#&gt; 3 Adelie     \n#&gt; 4 Chinstrap  \n#&gt; 5 Adelie     \n#&gt; 6 Adelie     \n#&gt; # ℹ 94 more rows\n\n\nIf you wanted to obtain a probability for each predicted value, then simply set the type = prob in predict(). This will yield a tibble with one column per outcome type and the corresponding predicted probability for each value to be each type of outcome. Then, to add the predicted values as a new column on the test data, use the bind_cols() function from dplyr.\n\npenguins_predp &lt;- penguins_fit %&gt;%\n  predict(test_data, type = \"prob\")\n\nTo add the predicted values as a new column on the test data, you can use the bind_cols() function from dplyr.\n\nbind_cols(test_data, penguins_predp) %&gt;%\n  head() # View first six rows of output\n\n#&gt; # A tibble: 6 × 7\n#&gt;   species bill_length_mm bill_depth_mm flipper_length_mm .pred_Adelie\n#&gt;   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;        &lt;dbl&gt;\n#&gt; 1 Adelie            39.5          17.4               186        0.910\n#&gt; 2 Adelie            40.3          18                 195        0.960\n#&gt; 3 Adelie            38.7          19                 195        0.964\n#&gt; 4 Adelie            46            21.5               194        0.286\n#&gt; 5 Adelie            35.9          19.2               189        0.997\n#&gt; 6 Adelie            38.2          18.1               185        1    \n#&gt; # ℹ 2 more variables: .pred_Chinstrap &lt;dbl&gt;, .pred_Gentoo &lt;dbl&gt;\n\n\nAlternatively, we can use the augment() function to obtain the predicted probabilities and add them to the test data in a one-liner.\n\npenguins_aug &lt;- augment(penguins_fit, test_data)\n\npenguins_aug\n\n#&gt; # A tibble: 100 × 8\n#&gt;   species bill_length_mm bill_depth_mm flipper_length_mm .pred_class\n#&gt;   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt; &lt;fct&gt;      \n#&gt; 1 Adelie            39.5          17.4               186 Adelie     \n#&gt; 2 Adelie            40.3          18                 195 Adelie     \n#&gt; 3 Adelie            38.7          19                 195 Adelie     \n#&gt; 4 Adelie            46            21.5               194 Chinstrap  \n#&gt; 5 Adelie            35.9          19.2               189 Adelie     \n#&gt; 6 Adelie            38.2          18.1               185 Adelie     \n#&gt; # ℹ 94 more rows\n#&gt; # ℹ 3 more variables: .pred_Adelie &lt;dbl&gt;, .pred_Chinstrap &lt;dbl&gt;, …\n\n\nWe can see from the first couple of rows shown that our model predicted the species correctly to be Adelie (in the .pred_class column) because the .pred_Adelie probabilities are by far the largest of the three predicted probabilities for each row. So while we can informally say that our model is doing well for predicting, how can we formally assess this? We would like to calculate a metric (well, probably more than one) to tell us how well our model predicts the species of penguins.\n\n\n3.1.7 Model Validation\nThe metrics() function from the yardstick package is helps to assess model performance. As suggested by its name, it will output some metrics, and as an added bonus, these will be automatically selected for the type of model that you construct. The input for this function is a tibble that contains the actual values and the predicted values. This way we can compare how close the model estimates are to the truth. To serve this purpose, we can use penguins_aug.\n\npenguins_aug %&gt;%\n  metrics(truth = species, .pred_Adelie:.pred_Gentoo, estimate = .pred_class)\n\n#&gt; # A tibble: 4 × 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy    multiclass     0.97 \n#&gt; 2 kap         multiclass     0.954\n#&gt; 3 mn_log_loss multiclass     0.123\n#&gt; 4 roc_auc     hand_till      0.993\n\n\nLet’s briefly go through the metrics that were generated. Accuracy is simply the proportion of values that are predicted correctly, while kappa is similar to accuracy, but is normalized by the accuracy that would be expected by chance (you can think of it as a measure that compares observed accuracy to expected accuracy from random chance alone). For our example, both the accuracy and kappa value estimates are extremely high (near to the upper limit of 1) and similar in value, indicating that our model performs very well for prediction on the test data. Log loss is a measure of the performance of a classification model and a perfect model has a log loss of 0, so our model performs pretty well in that respect. Finally, roc_auc is the area under ROC curve and we’ll explain this very shortly so stay tuned (for now, just note that a value close to 1, like we have, is the goal). All in all, our model fairs very well.\nSince it is often not enough to rely purely on one number summaries of model performance, we’ll also look to graphical, curve-based metrics. We’ll walk through producing the classic ROC curve, which is computed using roc_curve() and roc_auc() from yardstick.\nTo get ourselves an ROC curve, we need to input the actual values and the columns of predicted class probabilities into roc_curve(). We finish off by piping into the autoplot() function.\n\npenguins_aug %&gt;%\n  roc_curve(truth = species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\nNotice that the x-axis displays 1 - specificity, which is otherwise known as the false positive rate. So on this plot, we can visualize the trade-off between the false positive (1 - specificity) and the true positive (sensitivity) rates. Since the best classification would be where all positives are correctly classified as positive (sensitivity = 1), and no negatives are incorrect classified as positive (specificity = 0), curves closer to the top left corner (and, hence, an area under the curve of about 1) is what we’re hoping for.\nSo, we can see that the curves for each of our species are looking pretty close to perfection (save for Adelie, which still does very well). To estimate the area under the curves, we can use roc_auc (or look to the summary of our metrics above for this very value).\n\npenguins_aug %&gt;%\n  roc_auc(truth = species, .pred_Adelie:.pred_Gentoo)\n\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc hand_till      0.993\n\n\nAs expected, the estimated area is very close to 1, indicating near-perfect discrimination.\nThe yardstick package also offers other standard tools for model assessment like a confusion matrix, from which we can inspect the counts of correct classifications and miclassifications.\n\npenguins_aug %&gt;%\n  conf_mat(truth = species, estimate = .pred_class)\n\n#&gt;            Truth\n#&gt; Prediction  Adelie Chinstrap Gentoo\n#&gt;   Adelie        40         0      0\n#&gt;   Chinstrap      3        24      0\n#&gt;   Gentoo         0         0     33\n\n\nWe could even combine this with autoplot() to get a nice heatmap visualization.\n\npenguins_aug %&gt;%\n  conf_mat(truth = species, estimate = .pred_class) %&gt;%\n  autoplot(\"heatmap\")\n\n\n\n\n\n\n\n\nThe diagonal shows the counts of correct predictions for each species, while the off-diagonal shows the counts of model misclassifications. As the metrics have indicated, our model performed magnificently on the test set as there was only three misclassifications of Adelie penguins as Chinstrap."
  },
  {
    "objectID": "tidymodels-intro.html#concluding-remarks",
    "href": "tidymodels-intro.html#concluding-remarks",
    "title": "3  Introduction to Tidymodels",
    "section": "3.2 Concluding remarks",
    "text": "3.2 Concluding remarks\nIn this vignette, we introduced tidymodels and illustrated how to its packages work together by way of example. Since this was an elementary example, so use this as a starting point and explore what more can be done with this wonderful set of packages. And yet, however wonderful they are, you may have already noticed that there are limitations like the glaring lack of a set of post-processing tools to refine the results. We fill this gap for epidemiological modelling with frosting. This will be formally introduced in a later chapter, so stay tuned!\n\n🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧"
  },
  {
    "objectID": "tidymodels-intro.html#attribution",
    "href": "tidymodels-intro.html#attribution",
    "title": "3  Introduction to Tidymodels",
    "section": "3.3 Attribution",
    "text": "3.3 Attribution\nThis vignette was largely adapted from A Gentle Introduction to Tidymodels, and Tidymodels - Getting Started, and Tidymodels.\n🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧"
  },
  {
    "objectID": "tidymodels-regression.html#libraries",
    "href": "tidymodels-regression.html#libraries",
    "title": "4  Regression in Tidymodels",
    "section": "4.1 Libraries",
    "text": "4.1 Libraries\n\nlibrary(tidymodels)\nlibrary(broom)\nlibrary(performance)"
  },
  {
    "objectID": "tidymodels-regression.html#simple-linear-regression",
    "href": "tidymodels-regression.html#simple-linear-regression",
    "title": "4  Regression in Tidymodels",
    "section": "4.2 Simple linear regression",
    "text": "4.2 Simple linear regression\nThe key steps to perform linear regression in tidymodels are to first specify the model type and then to specify the model form and the data to be used to construct it.\nTo illustrate, we shall look to penguins dataset from the tidymodels’ modeldata package. This dataset contains measurements for 344 penguins from three islands in Palmer Archipelago, Antarctica, and includes information on their species, island home, size (flipper length, body mass, bill dimensions), and sex.\n\n\n\n\n\n\n\n\n\n\n# Let's inspect the data\nhead(penguins)\n\n#&gt; # A tibble: 6 × 7\n#&gt;   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#&gt;   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n#&gt; 1 Adelie  Torgersen           39.1          18.7               181\n#&gt; 2 Adelie  Torgersen           39.5          17.4               186\n#&gt; 3 Adelie  Torgersen           40.3          18                 195\n#&gt; 4 Adelie  Torgersen           NA            NA                  NA\n#&gt; 5 Adelie  Torgersen           36.7          19.3               193\n#&gt; 6 Adelie  Torgersen           39.3          20.6               190\n#&gt; # ℹ 2 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;\n\n\nOne thing you may have spotted is that there’s missing data in this dataset in the fourth row. For simplicity, we will only work with the complete cases. This reduces the number of rows in our dataset to 333.\n\npenguins &lt;- penguins %&gt;%\n  filter(complete.cases(.))\n\nhead(penguins)\n\n#&gt; # A tibble: 6 × 7\n#&gt;   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#&gt;   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n#&gt; 1 Adelie  Torgersen           39.1          18.7               181\n#&gt; 2 Adelie  Torgersen           39.5          17.4               186\n#&gt; 3 Adelie  Torgersen           40.3          18                 195\n#&gt; 4 Adelie  Torgersen           36.7          19.3               193\n#&gt; 5 Adelie  Torgersen           39.3          20.6               190\n#&gt; 6 Adelie  Torgersen           38.9          17.8               181\n#&gt; # ℹ 2 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;\n\n\nMuch better! We will now build a simple linear regression model to model bill length as a function of bill depth.\n\n\n\n\n\n\n\n\n\nIn parsnip, the model specification is broken down into small functions such as set_mode() and set_engine() to make the interface more flexible and readable. The general structure is to first specify a mode (regression or classification) and then an engine to indicate what software (or implementation of the algorithm) will be used to fit the model. For our purposes, the mode is regression and the engine is lm for ordinary least squares. You may note that setting the mode is unnecessary for linear regression, but we include it here as it is a good practice.\n\nlm_spec &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"lm\")\n\nThe above specification does not actually carry out the regression, rather it just states what we would like to do.\n\nlm_spec\n\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\nOnce we have such a blueprint, we may fit a model by inputting data and a formula. Recall that in R, a formula takes the form y ~ x where y ix the response and x is the predictor variable. For our example, where the response of bill length and predictor of bill depth, we would write the formula as bill_length_mm ~ bill_depth_mm. Note that the names used in a formula must be identical to the variable names in the dataset.\n\nlm_fit &lt;- lm_spec %&gt;%\n  fit(bill_length_mm ~ bill_depth_mm, data = penguins)\n\nlm_fit\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ bill_depth_mm, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;   (Intercept)  bill_depth_mm  \n#&gt;       54.8909        -0.6349\n\n\nThe resulting parsnip object includes basic information about the fit such as the model coefficients. To access the underlying fit object, we could use the standard lm_fit$fit or with purrr’s pluck() function.\n\nlm_fit %&gt;%\n  pluck(\"fit\")\n\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ bill_depth_mm, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;   (Intercept)  bill_depth_mm  \n#&gt;       54.8909        -0.6349\n\n\nTo get additional information about the fit (such as standard errors, and goodness-of-fit statistics), we can get a summary of the model fit as follows:\n\nlm_fit %&gt;%\n  pluck(\"fit\") %&gt;%\n  summary()\n\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ bill_depth_mm, data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -12.9498  -3.9530  -0.3657   3.7327  15.5025 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)    54.8909     2.5673  21.380  &lt; 2e-16 ***\n#&gt; bill_depth_mm  -0.6349     0.1486  -4.273 2.53e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.332 on 331 degrees of freedom\n#&gt; Multiple R-squared:  0.05227,    Adjusted R-squared:  0.04941 \n#&gt; F-statistic: 18.26 on 1 and 331 DF,  p-value: 2.528e-05\n\n\nTo get a tidy summary of the model parameter estimates, simply use the tidy function from the broom package on the model fit. To extract model statistics, glance() can be used.\n\ntidy(lm_fit)\n\n#&gt; # A tibble: 2 × 5\n#&gt;   term          estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)     54.9       2.57      21.4  2.54e-64\n#&gt; 2 bill_depth_mm   -0.635     0.149     -4.27 2.53e- 5\n\nglance(lm_fit)\n\n#&gt; # A tibble: 1 × 12\n#&gt;   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n#&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    0.0523        0.0494  5.33      18.3 0.0000253     1 -1029. 2064. 2075.\n#&gt; # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nNow, to make predictions, we simply use predict() on the parnsip model object. In there, we must specify the dataset we want to predict on in the new_data argument. Note that this may be a different dataset than we used for fitting the model, but this input data must include all predictor variables that were used to fit the model.\n\npredict(lm_fit, new_data = penguins)\n\n#&gt; # A tibble: 333 × 1\n#&gt;   .pred\n#&gt;   &lt;dbl&gt;\n#&gt; 1  43.0\n#&gt; 2  43.8\n#&gt; 3  43.5\n#&gt; 4  42.6\n#&gt; 5  41.8\n#&gt; 6  43.6\n#&gt; # ℹ 327 more rows\n\n\nFor parnsip models, the predictions are always outputted in a tibble.\nTo specify the type of prediction made, modify type argument. If we set type = \"conf_int\", we get a 95% confidence interval.\n\npredict(lm_fit, new_data = penguins, type = \"conf_int\")\n\n#&gt; # A tibble: 333 × 2\n#&gt;   .pred_lower .pred_upper\n#&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1        42.3        43.7\n#&gt; 2        43.3        44.4\n#&gt; 3        42.8        44.1\n#&gt; 4        41.8        43.5\n#&gt; 5        40.7        43.0\n#&gt; 6        43.0        44.2\n#&gt; # ℹ 327 more rows\n\n\nTo evaluate model predictive performance, it is logical to compare the each of the observed and predicted values. To see these values side-by-side we simply bind the two vectors of interest.\n\nbind_cols(\n  predict(lm_fit, new_data = penguins),\n  penguins\n) %&gt;%\n  select(bill_length_mm, .pred)\n\n#&gt; # A tibble: 333 × 2\n#&gt;   bill_length_mm .pred\n#&gt;            &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1           39.1  43.0\n#&gt; 2           39.5  43.8\n#&gt; 3           40.3  43.5\n#&gt; 4           36.7  42.6\n#&gt; 5           39.3  41.8\n#&gt; 6           38.9  43.6\n#&gt; # ℹ 327 more rows\n\n\nA simpler way to do this is to use the nifty argument() function.\n\naugment(lm_fit, new_data = penguins) %&gt;%\n  select(bill_length_mm, .pred)\n\n#&gt; # A tibble: 333 × 2\n#&gt;   bill_length_mm .pred\n#&gt;            &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1           39.1  43.0\n#&gt; 2           39.5  43.8\n#&gt; 3           40.3  43.5\n#&gt; 4           36.7  42.6\n#&gt; 5           39.3  41.8\n#&gt; 6           38.9  43.6\n#&gt; # ℹ 327 more rows"
  },
  {
    "objectID": "tidymodels-regression.html#multiple-linear-regression",
    "href": "tidymodels-regression.html#multiple-linear-regression",
    "title": "4  Regression in Tidymodels",
    "section": "4.3 Multiple linear regression",
    "text": "4.3 Multiple linear regression\nThe only difference about fitting a multiple linear regression model in comparison to a simple linear regression model lies the formula. For multiple linear regression, the predictors are specified in the formula expression, separated by +. For example, if we have a response variable y and three predictors, x1, x2, and x3, we would write the formula as, y ~ x1 + x2 + x3.\n\nlm_fit2 &lt;- lm_spec %&gt;%\n  fit(bill_length_mm ~ bill_depth_mm + flipper_length_mm + body_mass_g, data = penguins)\n\nlm_fit2\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ bill_depth_mm + flipper_length_mm + \n#&gt;     body_mass_g, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)      bill_depth_mm  flipper_length_mm        body_mass_g  \n#&gt;        -2.571e+01          6.131e-01          2.872e-01          3.472e-04\n\n\nEverything else proceeds much the same as before. Such as obtaining parameter estimates\n\ntidy(lm_fit2)\n\n#&gt; # A tibble: 4 × 5\n#&gt;   term                estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)       -25.7       6.72        -3.83  1.55e- 4\n#&gt; 2 bill_depth_mm       0.613     0.138        4.43  1.26e- 5\n#&gt; 3 flipper_length_mm   0.287     0.0351       8.18  6.28e-15\n#&gt; 4 body_mass_g         0.000347  0.000566     0.614 5.40e- 1\n\n\nas well as predicting new values.\n\npredict(lm_fit2, new_data = penguins)\n\n#&gt; # A tibble: 333 × 1\n#&gt;   .pred\n#&gt;   &lt;dbl&gt;\n#&gt; 1  39.0\n#&gt; 2  39.7\n#&gt; 3  42.5\n#&gt; 4  42.8\n#&gt; 5  42.8\n#&gt; 6  38.4\n#&gt; # ℹ 327 more rows\n\n\nIf you would like to use all variables aside from your response as predictors, a shortcut is to use the formula form y ~ .\n\nlm_fit3 &lt;- lm_spec %&gt;%\n  fit(bill_length_mm ~ ., data = penguins)\n\nlm_fit3\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)   speciesChinstrap      speciesGentoo        islandDream  \n#&gt;         15.343291           9.835502           6.117675          -0.503815  \n#&gt;   islandTorgersen      bill_depth_mm  flipper_length_mm        body_mass_g  \n#&gt;         -0.127431           0.300670           0.069257           0.001081  \n#&gt;           sexmale  \n#&gt;          2.047859"
  },
  {
    "objectID": "tidymodels-regression.html#checking-model-assumptions",
    "href": "tidymodels-regression.html#checking-model-assumptions",
    "title": "4  Regression in Tidymodels",
    "section": "4.4 Checking model assumptions",
    "text": "4.4 Checking model assumptions\nAfter fitting a model, it is good to check whether the assumptions of linear regression are met. For this, we will use the performance package, in particular the check_model() function to produce several helpful plots we may use to check the assumptions for our first multiple linear regression model.\n\nlm_fit2 %&gt;%\n  extract_fit_engine() %&gt;%\n  check_model()\n\n\n\n\n\n\n\n\nNotice that on each plot it says what we should expect to see if the model assumption is met.\nWe shall now briefly walk you through what each plot means.\nThe first two plots help us to examine the linearity of the errors versus the fitted values. Ideally, we want this error to be relatively flat and horizontal. The third plot is for checking homogeneity of the variance, where we want the points to be roughly the same distance from the line as this indicates similar dispersion. The fourth plot helps us to see if there are high leverage points - points that have command or influence over the model fit. As a result, these can have a great effect on the model predictions. So the removal of such points or modifications to the model may be necessary to deal with them. The fifth plot helps us to discern collinearity, which is when predictors are highly correlated. Since independent variables should be independent, this can throw off simple regression models (in standard error of coefficient estimates and the estimates themselves, which would likely be sensitive to changes in the predictors that are included in the model). The last plot enables us to check the normality of residuals. If the distribution of the model error is non-normal, then that suggests a linear model may not be appropriate. For a QQ plot, we want the points to fall along a straight diagonal line.\nFor our example, we observe that there’s a pretty high correlation between body_mass_g and flipper_length_mm (not quite in the red-zone of 10 and above, but close enough for concern). That is indicative of multicollinearity between them. Intuitively, it makes sense for the body mass and flipper length variables - we’d expect that as once increases, so should the other.\nWe can take a closer look at the correlation by whipping up a correlation matrix by using base R’s cor() function. Since for collinearity we’re only usually interested in the numerical predictors, we’ll only include the four numeric variables.\n\npenguins_corr &lt;- penguins %&gt;%\n  select(body_mass_g, ends_with(\"_mm\")) %&gt;%\n  cor()\npenguins_corr\n\n#&gt;                   body_mass_g bill_length_mm bill_depth_mm flipper_length_mm\n#&gt; body_mass_g         1.0000000      0.5894511    -0.4720157         0.8729789\n#&gt; bill_length_mm      0.5894511      1.0000000    -0.2286256         0.6530956\n#&gt; bill_depth_mm      -0.4720157     -0.2286256     1.0000000        -0.5777917\n#&gt; flipper_length_mm   0.8729789      0.6530956    -0.5777917         1.0000000\n\n\nIndeed body_mass_g and flipper_length_mm are highly positively correlated. To deal with this problem, we’ll re-fit the model without body_mass_g.\n\nlm_fit2.5 &lt;- lm_spec %&gt;%\n  fit(bill_length_mm ~ bill_depth_mm + flipper_length_mm, data = penguins)\n\nlm_fit2.5\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ bill_depth_mm + flipper_length_mm, \n#&gt;     data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)      bill_depth_mm  flipper_length_mm  \n#&gt;          -27.9762             0.6200             0.3052\n\n\nand then check again to see whether the assumptions are met.\n\nlm_fit2.5 %&gt;%\n  extract_fit_engine() %&gt;%\n  check_model()\n\n\n\n\n\n\n\n\nOverall, the plots look pretty good. For details on how to interpret each of these plots and more details about model assumptions please see here and here."
  },
  {
    "objectID": "tidymodels-regression.html#interaction-terms",
    "href": "tidymodels-regression.html#interaction-terms",
    "title": "4  Regression in Tidymodels",
    "section": "4.5 Interaction terms",
    "text": "4.5 Interaction terms\nIn general, the syntax to add an interaction term to a formula is as follows:\n\nx:y denotes an interaction term between x and y.\nx*y denotes the interaction between x and y as well as x and y; that is, x + y + x*y.\n\nIt is important to note that this syntax is not compatible with all engines. Thus, we shall explain how to bypass this issue by adding an interaction term in a recipe later on. For now, let’s start simple by adding an interaction term between species and bill_length_mm, which allows for a species-specific slope.\n\nlm_fit4 &lt;- lm_spec %&gt;%\n  fit(bill_length_mm ~ species * bill_depth_mm, data = penguins)\n\nlm_fit4\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = bill_length_mm ~ species * bill_depth_mm, \n#&gt;     data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                    (Intercept)                speciesChinstrap  \n#&gt;                        23.3668                         -9.9389  \n#&gt;                  speciesGentoo                   bill_depth_mm  \n#&gt;                        -6.6966                          0.8425  \n#&gt; speciesChinstrap:bill_depth_mm     speciesGentoo:bill_depth_mm  \n#&gt;                         1.0796                          1.2178\n\n\nUsing recipes, the interaction term is specified by using step_interact(). Then we construct a workflow object, where we add the linear regression model specification and recipe. Finally, we fit the model as we did for a parsnip model. Note that the workflow object does not need the variables that were specified in the recipe to be specified again.\n\nrec_spec_interact &lt;- recipe(bill_length_mm ~ species + bill_depth_mm, data = penguins) %&gt;%\n  step_interact(~ species:bill_depth_mm)\n\nlm_wf_interact &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(rec_spec_interact)\n\nlm_wf_interact %&gt;% fit(penguins)\n\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; • step_interact()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                      (Intercept)                  speciesChinstrap  \n#&gt;                          23.3668                           -9.9389  \n#&gt;                    speciesGentoo                     bill_depth_mm  \n#&gt;                          -6.6966                            0.8425  \n#&gt; speciesChinstrap_x_bill_depth_mm     speciesGentoo_x_bill_depth_mm  \n#&gt;                           1.0796                            1.2178\n\n\nNotice the variable name for the interaction term is not the same as it is in base R (which is simply of the form x:y). In step_interact(), the default separator between the variable names is _x_. You can change this default by specifying the sep argument in the function.\nTo read more about formula syntax, see ?formula."
  },
  {
    "objectID": "tidymodels-regression.html#non-linear-transformations-of-the-predictors",
    "href": "tidymodels-regression.html#non-linear-transformations-of-the-predictors",
    "title": "4  Regression in Tidymodels",
    "section": "4.6 Non-linear transformations of the predictors",
    "text": "4.6 Non-linear transformations of the predictors\nSimilar to how we were able to add an interaction term using recipes, we can also perform a transformation as a pre-processing step. The function used for this is step_mutate() (which acts like dplyr’s mutate).\nNote that, in general, if you are specifying a recipe aim to keep as much of the pre-processing in your recipe specification as possible. This helps to ensure that the transformation will be applied to new data consistently.\n\nrec_spec_pow2 &lt;- recipe(bill_length_mm ~ bill_depth_mm, data = penguins) %&gt;%\n  step_mutate(bill_depth_mm2 = bill_depth_mm^2)\n\nlm_wf_pow2 &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(rec_spec_pow2)\n\nlm_wf_pow2 %&gt;% fit(penguins)\n\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; • step_mutate()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;    (Intercept)   bill_depth_mm  bill_depth_mm2  \n#&gt;        95.2558         -5.4431          0.1413\n\n\nThere are many transformations already built into recipes such as step_log(). So, for basic transformations, there’s often no need to make your own transformation from scratch. See here for a comprehensive list of the transformations that are offered in recipes.\n\nrec_spec_log &lt;- recipe(bill_length_mm ~ bill_depth_mm, data = penguins) %&gt;%\n  step_log(bill_depth_mm)\n\nlm_wf_log &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(rec_spec_log)\n\nlm_wf_log %&gt;% fit(penguins)\n\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; • step_log()\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;   (Intercept)  bill_depth_mm  \n#&gt;         74.95         -10.91\n\n\n\n\n🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧"
  },
  {
    "objectID": "tidymodels-regression.html#attribution",
    "href": "tidymodels-regression.html#attribution",
    "title": "4  Regression in Tidymodels",
    "section": "4.7 Attribution",
    "text": "4.7 Attribution\nThis vignette was largely adapted from Chapter 3 of ISLR tidymodels labs. Checking linear regression assumptions was founded on this article. Performance plot descriptions are based on this easystats tutorial for investigating model performance. The artwork used is by Allison Horst.\n🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧 🐧"
  },
  {
    "objectID": "preprocessing-and-models.html#introduction",
    "href": "preprocessing-and-models.html#introduction",
    "title": "5  Examples of Preprocessing and Models",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nThe epipredict package utilizes the tidymodels framework, namely {recipes} for dplyr-like pipeable sequences of feature engineering and {parsnip} for a unified interface to a range of models.\nepipredict has additional customized feature engineering and preprocessing steps, such as step_epi_lag(), step_population_scaling(), step_epi_naomit(). They can be used along with steps from the {recipes} package for more feature engineering.\nIn this vignette, we will illustrate some examples of how to use epipredict with recipes and parsnip for different purposes of epidemiological forecasting. We will focus on basic autoregressive models, in which COVID cases and deaths in the near future are predicted using a linear combination of cases and deaths in the near past.\nThe remaining vignette will be split into three sections. The first section, we will use a Poisson regression to predict death counts. In the second section, we will use a linear regression to predict death rates. Last but not least, we will create a classification model for hotspot predictions.\n\nlibrary(epidatr)\nlibrary(epiprocess)\nlibrary(epipredict)\nlibrary(recipes)\nlibrary(parsnip)\nlibrary(workflows)\nlibrary(poissonreg)"
  },
  {
    "objectID": "preprocessing-and-models.html#poisson-regression",
    "href": "preprocessing-and-models.html#poisson-regression",
    "title": "5  Examples of Preprocessing and Models",
    "section": "5.2 Poisson Regression",
    "text": "5.2 Poisson Regression\nDuring COVID-19, the U.S. Centers for Disease Control and Prevention (CDC) collected models and forecasts to characterize the state of an outbreak and its course. They use it to inform public health decision makers on potential consequences of deploying control measures.\nOne of the outcomes that the CDC forecasts is death counts from COVID-19. Although there are many state-of-the-art models, we choose to use Poisson regression, the textbook example for modeling count data, as an illustration for using the epipredict package with other existing tidymodels packages.\n\ngeos &lt;- c(\"ca\", \"fl\", \"tx\", \"ny\", \"nj\")\nx &lt;- covidcast(\n  data_source = \"jhu-csse\",\n  signals = \"confirmed_incidence_num\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20210604, 20211231),\n  geo_values = geos\n) %&gt;%\n  fetch() %&gt;%\n  select(geo_value, time_value, cases = value)\n\ny &lt;- covidcast(\n  data_source = \"jhu-csse\",\n  signals = \"deaths_incidence_num\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20210604, 20211231),\n  geo_values = geos\n) %&gt;%\n  fetch() %&gt;%\n  select(geo_value, time_value, deaths = value)\n\ncounts_subset &lt;- full_join(x, y, by = c(\"geo_value\", \"time_value\")) %&gt;%\n  as_epi_df()\n\nThe counts_subset dataset comes from the epidatr package, and contains the number of confirmed cases and deaths from June 4, 2021 to Dec 31, 2021 in some U.S. states.\nWe wish to predict the 7-day ahead death counts with lagged cases and deaths. Furthermore, we will let each state be a dummy variable. Using differential intercept coefficients, we can allow for an intercept shift between states.\nOne possible model takes the form\n\\[\\begin{aligned}\n\\log\\left( \\mu_{t+7} \\right) &{}= \\beta_0 + \\delta_1 s_{\\text{state}_1} +\n\\delta_2 s_{\\text{state}_2} + \\cdots +  \\nonumber \\\\ &\\quad\\beta_1 \\text{deaths}_{t} +\n\\beta_2 \\text{deaths}_{t-7}  + \\beta_3 \\text{cases}_{t} +\n\\beta_4 \\text{cases}_{t-7},\n\\end{aligned}\\]\nwhere \\(\\mu_{t+7} = \\mathbb{E}(\\text{deaths}_{t+7})\\), and \\(\\text{deaths}_{t+7}\\) is assumed to follow a Poisson distribution with mean \\(\\mu_{t+7}\\); \\(s_{\\text{state}}\\) are dummy variables for each state and take values of either 0 or 1.\nPreprocessing steps will be performed to prepare the data for model fitting. But before diving into them, it will be helpful to understand what roles are in the recipes framework.\n\n\nAside on recipes\nrecipes can assign one or more roles to each column in the data. The roles are not restricted to a predefined set; they can be anything. For most conventional situations, they are typically “predictor” and/or “outcome”. Additional roles enable targeted step_*() operations on specific variables or groups of variables.\nIn our case, the role predictor is given to explanatory variables on the right-hand side of the model (in the equation above). The role outcome is the response variable that we wish to predict. geo_value and time_value are predefined roles that are unique to the epipredict package. Since we work with epi_df objects, all datasets should have geo_value and time_value passed through automatically with these two roles assigned to the appropriate columns in the data.\nThe recipes package also allows manual alterations of roles in bulk. There are a few handy functions that can be used together to help us manipulate variable roles easily.\n\nupdate_role() alters an existing role in the recipe or assigns an initial role to variables that do not yet have a declared role.\nadd_role() adds an additional role to variables that already have a role in the recipe, without overwriting old roles.\nremove_role() eliminates a single existing role in the recipe.\n\n\n\nEnd aside\n\nNotice in the following preprocessing steps, we used add_role() on geo_value_factor since, currently, the default role for it is raw, but we would like to reuse this variable as a predictor.\n\ncounts_subset &lt;- counts_subset %&gt;%\n  mutate(geo_value_factor = as.factor(geo_value)) %&gt;%\n  as_epi_df()\n\nepi_recipe(counts_subset)\n\nr &lt;- epi_recipe(counts_subset) %&gt;%\n  add_role(geo_value_factor, new_role = \"predictor\") %&gt;%\n  step_dummy(geo_value_factor) %&gt;%\n  ## Occasionally, data reporting errors / corrections result in negative\n  ## cases / deaths\n  step_mutate(cases = pmax(cases, 0), deaths = pmax(deaths, 0)) %&gt;%\n  step_epi_lag(cases, deaths, lag = c(0, 7)) %&gt;%\n  step_epi_ahead(deaths, ahead = 7, role = \"outcome\") %&gt;%\n  step_epi_naomit()\n\nAfter specifying the preprocessing steps, we will use the parsnip package for modeling and producing the prediction for death count, 7 days after the latest available date in the dataset.\n\nlatest &lt;- get_test_data(r, counts_subset)\n\nwf &lt;- epi_workflow(r, parsnip::poisson_reg()) %&gt;%\n  fit(counts_subset)\n\npredict(wf, latest) %&gt;% filter(!is.na(.pred))\n\n#&gt; An `epi_df` object, 5 x 3 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2023-06-13 16:30:27\n#&gt; \n#&gt; # A tibble: 5 × 3\n#&gt;   geo_value time_value .pred\n#&gt; * &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt;\n#&gt; 1 ca        2021-12-31 108. \n#&gt; 2 fl        2021-12-31 270. \n#&gt; 3 nj        2021-12-31  22.5\n#&gt; 4 ny        2021-12-31  94.8\n#&gt; 5 tx        2021-12-31  91.0\n\n\nNote that the time_value corresponds to the last available date in the training set, NOT to the target date of the forecast (2022-01-07).\nLet’s take a look at the fit:\n\nextract_fit_engine(wf)\n\n#&gt; \n#&gt; Call:  stats::glm(formula = ..y ~ ., family = stats::poisson, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;         (Intercept)  geo_value_factor_fl  geo_value_factor_nj  \n#&gt;           3.970e+00           -1.487e-01           -1.425e+00  \n#&gt; geo_value_factor_ny  geo_value_factor_tx          lag_0_cases  \n#&gt;          -6.865e-01            3.025e-01            1.339e-05  \n#&gt;         lag_7_cases         lag_0_deaths         lag_7_deaths  \n#&gt;           1.717e-06            1.731e-03            8.566e-04  \n#&gt; \n#&gt; Degrees of Freedom: 984 Total (i.e. Null);  976 Residual\n#&gt; Null Deviance:       139600 \n#&gt; Residual Deviance: 58110     AIC: 62710\n\n\nAlternative forms of Poisson regression or particular computational approaches can be applied via arguments to parsnip::poisson_reg() for some common settings, and by using parsnip::set_engine() to use a specific Poisson regression engine and to provide additional engine-specific customization."
  },
  {
    "objectID": "preprocessing-and-models.html#linear-regression",
    "href": "preprocessing-and-models.html#linear-regression",
    "title": "5  Examples of Preprocessing and Models",
    "section": "5.3 Linear Regression",
    "text": "5.3 Linear Regression\nFor COVID-19, the CDC required submission of case and death count predictions. However, the Delphi Group preferred to train on rate data instead, because it puts different locations on a similar scale (eliminating the need for location-specific intercepts). We can use a linear regression to predict the death rates and use state population data to scale the rates to counts.1 We will do so using layer_population_scaling() from the epipredict package. (We could also use step_population_scaling() from the epipredict package to prepare rate data from count data in the preprocessing recipe.)\nAdditionally, when forecasts are submitted, prediction intervals should be provided along with the point estimates. This can be obtained via postprocessing using layer_residual_quantiles(). It is worth pointing out, however, that layer_residual_quantiles() should be used before population scaling or else the transformation will make the results uninterpretable.\nWe wish, now, to predict the 7-day ahead death counts with lagged case rates and death rates, along with some extra behaviourial predictors. Namely, we will use survey data from COVID-19 Trends and Impact Survey.\nThe survey data provides the estimated percentage of people who wore a mask for most or all of the time while in public in the past 7 days and the estimated percentage of respondents who reported that all or most people they encountered in public in the past 7 days maintained a distance of at least 6 feet.\nState-wise population data from the 2019 U.S. Census is included in this package and will be used in layer_population_scaling().\n\nbehav_ind_mask &lt;- covidcast(\n  data_source = \"fb-survey\",\n  signals = \"smoothed_wwearing_mask_7d\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20210604, 20211231),\n  geo_values = geos\n) %&gt;%\n  fetch() %&gt;%\n  select(geo_value, time_value, masking = value)\n\nbehav_ind_distancing &lt;- covidcast(\n  data_source = \"fb-survey\",\n  signals = \"smoothed_wothers_distanced_public\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20210604, 20211231),\n  geo_values = geos\n) %&gt;%\n  fetch() %&gt;%\n  select(geo_value, time_value, distancing = value)\n\npop_dat &lt;- state_census %&gt;% select(abbr, pop)\n\nbehav_ind &lt;- behav_ind_mask %&gt;%\n  full_join(behav_ind_distancing, by = c(\"geo_value\", \"time_value\"))\n\nRather than using raw mask-wearing / social-distancing metrics, for the sake of illustration, we’ll convert both into categorical predictors.\n\n\n\n\n\n\n\n\n\nWe will take a subset of death rate and case rate data from the built-in dataset case_death_rate_subset.\n\njhu &lt;- filter(\n  case_death_rate_subset,\n  time_value &gt;= \"2021-06-04\",\n  time_value &lt;= \"2021-12-31\",\n  geo_value %in% c(\"ca\", \"fl\", \"tx\", \"ny\", \"nj\")\n)\n\nPreprocessing steps will again rely on functions from the epipredict package as well as the recipes package. There are also many functions in the recipes package that allow for scalar transformations, such as log transformations and data centering. In our case, we will center the numerical predictors to allow for a more meaningful interpretation of the intercept.\n\njhu &lt;- jhu %&gt;%\n  mutate(geo_value_factor = as.factor(geo_value)) %&gt;%\n  left_join(behav_ind, by = c(\"geo_value\", \"time_value\")) %&gt;%\n  as_epi_df()\n\nr &lt;- epi_recipe(jhu) %&gt;%\n  add_role(geo_value_factor, new_role = \"predictor\") %&gt;%\n  step_dummy(geo_value_factor) %&gt;%\n  step_epi_lag(case_rate, death_rate, lag = c(0, 7, 14)) %&gt;%\n  step_mutate(\n    masking = cut_number(masking, 5),\n    distancing = cut_number(distancing, 5)\n  ) %&gt;%\n  step_epi_ahead(death_rate, ahead = 7, role = \"outcome\") %&gt;%\n  step_center(contains(\"lag\"), role = \"predictor\") %&gt;%\n  step_epi_naomit()\n\nAs a sanity check we can examine the structure of the training data:\n\nglimpse(slice_sample(bake(prep(r, jhu), jhu), n = 6))\n\n#&gt; Rows: 6\n#&gt; Columns: 17\n#&gt; $ time_value          &lt;date&gt; 2021-10-29, 2021-08-18, 2021-07-06, 2021-09-28…\n#&gt; $ geo_value           &lt;chr&gt; \"fl\", \"ca\", \"ny\", \"nj\", \"ca\", \"fl\"\n#&gt; $ case_rate           &lt;dbl&gt; 7.987081, 35.039629, 1.846962, 21.278102, 26.91…\n#&gt; $ death_rate          &lt;dbl&gt; 0.5698954, 0.1259178, 0.0258575, 0.2251651, 0.2…\n#&gt; $ masking             &lt;fct&gt; \"(52.8,60.2]\", \"(69.7,85]\", \"[42.7,52.8]\", \"(60…\n#&gt; $ distancing          &lt;fct&gt; \"(18.4,19.8]\", \"(27,43]\", \"(21.1,27]\", \"(19.8,2…\n#&gt; $ geo_value_factor_fl &lt;dbl&gt; 1, 0, 0, 0, 0, 1\n#&gt; $ geo_value_factor_nj &lt;dbl&gt; 0, 0, 0, 1, 0, 0\n#&gt; $ geo_value_factor_ny &lt;dbl&gt; 0, 0, 1, 0, 0, 0\n#&gt; $ geo_value_factor_tx &lt;dbl&gt; 0, 0, 0, 0, 0, 0\n#&gt; $ lag_0_case_rate     &lt;dbl&gt; -18.95458185, 8.09796675, -25.09470065, -5.6635…\n#&gt; $ lag_7_case_rate     &lt;dbl&gt; -17.368472, 4.657906, -25.243935, -1.325915, 5.…\n#&gt; $ lag_14_case_rate    &lt;dbl&gt; -14.5794681, -0.2855479, -25.1803998, -0.732444…\n#&gt; $ lag_0_death_rate    &lt;dbl&gt; 0.288021796, -0.155955804, -0.256016104, -0.056…\n#&gt; $ lag_7_death_rate    &lt;dbl&gt; 0.3386352965, -0.3399337035, -0.2493671035, -0.…\n#&gt; $ lag_14_death_rate   &lt;dbl&gt; 0.5016504, -0.1668420, -0.2390241, -0.0840500, …\n#&gt; $ ahead_7_death_rate  &lt;dbl&gt; 0.4364597, 0.2032103, 0.0236411, 0.2332067, 0.3…\n\n\nBefore directly predicting the results, we need to add postprocessing layers to obtain the death counts instead of death rates. Note that the rates used so far are “per 100K people” rather than “per person”. We’ll also use quantile regression with the quantile_reg engine rather than ordinary least squares to create median predictions and a 90% prediction interval.\n\nf &lt;- frosting() %&gt;%\n  layer_predict() %&gt;%\n  layer_add_target_date(\"2022-01-07\") %&gt;%\n  layer_threshold(.pred, lower = 0) %&gt;%\n  layer_quantile_distn() %&gt;%\n  layer_naomit(.pred) %&gt;%\n  layer_population_scaling(\n    .pred, .pred_distn,\n    df = pop_dat,\n    rate_rescaling = 1e5,\n    by = c(\"geo_value\" = \"abbr\"),\n    df_pop_col = \"pop\"\n  )\n\nwf &lt;- epi_workflow(r, quantile_reg(tau = c(.05, .5, .95))) %&gt;%\n  fit(jhu) %&gt;%\n  add_frosting(f)\n\nlatest &lt;- get_test_data(recipe = r, x = jhu)\np &lt;- predict(wf, latest)\np\n\n#&gt; An `epi_df` object, 5 x 7 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25\n#&gt; \n#&gt; # A tibble: 5 × 7\n#&gt;   geo_value time_value               .pred target_date         .pred_distn\n#&gt; * &lt;chr&gt;     &lt;date&gt;                  &lt;dist&gt; &lt;date&gt;                   &lt;dist&gt;\n#&gt; 1 ca        2021-12-31 [0.05, 0.95]&lt;q-rng&gt; 2022-01-07  [0.25, 0.75]&lt;q-rng&gt;\n#&gt; 2 fl        2021-12-31 [0.05, 0.95]&lt;q-rng&gt; 2022-01-07  [0.25, 0.75]&lt;q-rng&gt;\n#&gt; 3 nj        2021-12-31 [0.05, 0.95]&lt;q-rng&gt; 2022-01-07  [0.25, 0.75]&lt;q-rng&gt;\n#&gt; 4 ny        2021-12-31 [0.05, 0.95]&lt;q-rng&gt; 2022-01-07  [0.25, 0.75]&lt;q-rng&gt;\n#&gt; 5 tx        2021-12-31 [0.05, 0.95]&lt;q-rng&gt; 2022-01-07  [0.25, 0.75]&lt;q-rng&gt;\n#&gt; # ℹ 2 more variables: .pred_scaled &lt;dist&gt;, .pred_distn_scaled &lt;dist&gt;\n\n\nThe columns marked *_scaled have been rescaled to the correct units, in this case deaths rather than deaths per 100K people (these remain in .pred).\nTo look at the prediction intervals:\n\np %&gt;%\n  select(geo_value, target_date, .pred_scaled, .pred_distn_scaled) %&gt;%\n  mutate(.pred_distn_scaled = nested_quantiles(.pred_distn_scaled)) %&gt;%\n  unnest(.pred_distn_scaled) %&gt;%\n  pivot_wider(names_from = tau, values_from = q)\n\n#&gt; # A tibble: 5 × 5\n#&gt;   geo_value target_date        .pred_scaled `0.25` `0.75`\n#&gt;   &lt;chr&gt;     &lt;date&gt;                   &lt;dist&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 ca        2022-01-07  [0.05, 0.95]&lt;q-rng&gt;   48.8   94.0\n#&gt; 2 fl        2022-01-07  [0.05, 0.95]&lt;q-rng&gt;   48.4  104. \n#&gt; 3 nj        2022-01-07  [0.05, 0.95]&lt;q-rng&gt;   45.5   68.7\n#&gt; 4 ny        2022-01-07  [0.05, 0.95]&lt;q-rng&gt;  108.   163. \n#&gt; 5 tx        2022-01-07  [0.05, 0.95]&lt;q-rng&gt;   68.6  107.\n\n\nLast but not least, let’s take a look at the regression fit and check the coefficients:\n\n\n#&gt; Call:\n#&gt; quantreg::rq(formula = ..y ~ ., tau = ~c(0.05, 0.5, 0.95), data = data, \n#&gt;     na.action = function (object, ...) \n#&gt;     UseMethod(\"na.omit\"), method = \"br\", model = FALSE)\n#&gt; \n#&gt; Coefficients:\n#&gt;                        tau= 0.05     tau= 0.50    tau= 0.95\n#&gt; (Intercept)          0.210811625  0.2962574475  0.417583265\n#&gt; geo_value_factor_fl  0.032085820  0.0482361119  0.171126713\n#&gt; geo_value_factor_nj  0.007313762 -0.0033797953 -0.025251865\n#&gt; geo_value_factor_ny -0.001489163 -0.0199485947 -0.032635584\n#&gt; geo_value_factor_tx  0.029077485  0.0391980273  0.071961515\n#&gt; lag_0_case_rate     -0.001636588 -0.0011625693 -0.001430622\n#&gt; lag_7_case_rate      0.004700752  0.0057822095  0.006912655\n#&gt; lag_14_case_rate     0.001715816  0.0004224753  0.003448733\n#&gt; lag_0_death_rate     0.462341754  0.5274192012  0.164856372\n#&gt; lag_7_death_rate    -0.007368501  0.1132903956  0.172687438\n#&gt; lag_14_death_rate   -0.072500707 -0.0270474349  0.181279299\n#&gt; \n#&gt; Degrees of freedom: 950 total; 939 residual"
  },
  {
    "objectID": "preprocessing-and-models.html#classification",
    "href": "preprocessing-and-models.html#classification",
    "title": "5  Examples of Preprocessing and Models",
    "section": "5.4 Classification",
    "text": "5.4 Classification\nSometimes it is preferable to create a predictive model for surges or upswings rather than for raw values. In this case, the target is to predict if the future will have increased case rates (denoted up), decreased case rates (down), or flat case rates (flat) relative to the current level. Such models may be referred to as “hotspot prediction models”. We will follow the analysis in McDonald, Bien, Green, Hu, et al. but extend the application to predict three categories instead of two.\nHotspot prediction uses a categorical outcome variable defined in terms of the relative change of \\(Y_{\\ell, t+a}\\) compared to \\(Y_{\\ell, t}\\). Where \\(Y_{\\ell, t}\\) denotes the case rates in location \\(\\ell\\) at time \\(t\\). We define the response variables as follows:\n\\[\nZ_{\\ell, t}=\n    \\begin{cases}\n      \\text{up}, & \\text{if}\\ Y^{\\Delta}_{\\ell, t} &gt; 0.25 \\\\\n      \\text{down}, & \\text{if}\\  Y^{\\Delta}_{\\ell, t} &lt; -0.20\\\\\n      \\text{flat}, & \\text{otherwise}\n    \\end{cases}\n\\]\nwhere \\(Y^{\\Delta}_{\\ell, t} = (Y_{\\ell, t}- Y_{\\ell, t-7})\\ /\\ (Y_{\\ell, t-7})\\). We say location \\(\\ell\\) is a hotspot at time \\(t\\) when \\(Z_{\\ell,t}\\) is up, meaning the number of newly reported cases over the past 7 days has increased by at least 25% compared to the preceding week. When \\(Z_{\\ell,t}\\) is categorized as down, it suggests that there has been at least a 20% decrease in newly reported cases over the past 7 days (a 20% decrease is the inverse of a 25% increase). Otherwise, we will consider the trend to be flat.\nThe expression of the multinomial regression we will use is as follows: \\[\n\\pi_{j}(x) = \\text{Pr}(Z_{\\ell,t} = j|x) = \\frac{e^{g_j(x)}}{1 + \\sum_{k=0}^2 g_j(x) }\n\\] where \\(j\\) is either down, flat, or up\n\\[\n\\begin{aligned}\ng_{\\text{down}}(x) &= 0,\\\\\ng_{\\text{flat}}(x) &=\n\\log\\left(\\frac{Pr(Z_{\\ell,t}=\\text{flat}|x)}{Pr(Z_{\\ell,t}=\\text{down}|x)}\\right) =\n\\beta_{10} + \\beta_{11}t + \\delta_{10} s_{\\text{state}_1} +\n\\delta_{11} s_{\\text{state}_2} + \\cdots \\nonumber \\\\\n&\\quad +\\ \\beta_{12} Y^{\\Delta}_{\\ell, t} +\n\\beta_{13} Y^{\\Delta}_{\\ell, t-7}, \\\\\ng_{\\text{flat}}(x) &= \\log\\left(\\frac{Pr(Z_{\\ell,t}=\\text{up}|x)}{Pr(Z_{\\ell,t}=\\text{down}|x)}\\right) =\n\\beta_{20} + \\beta_{21}t + \\delta_{20} s_{\\text{state}_1} +\n\\delta_{21} s_{\\text{state}_2} + \\cdots \\nonumber \\\\\n&\\quad +\\ \\beta_{22} Y^{\\Delta}_{\\ell, t} +\n\\beta_{23} Y^{\\Delta}_{\\ell, t-7}.\n\\end{aligned}\n\\]\nPreprocessing steps are similar to the previous models with an additional step of categorizing the response variables. Again, we will use a subset of death rate and case rate data from our built-in dataset case_death_rate_subset.\n\njhu &lt;- case_death_rate_subset %&gt;%\n  dplyr::filter(\n    time_value &gt;= \"2021-06-04\",\n    time_value &lt;= \"2021-12-31\",\n    geo_value %in% c(\"ca\", \"fl\", \"tx\", \"ny\", \"nj\")\n  ) %&gt;%\n  mutate(geo_value_factor = as.factor(geo_value)) %&gt;%\n  as_epi_df()\n\nr &lt;- epi_recipe(jhu) %&gt;%\n  add_role(time_value, new_role = \"predictor\") %&gt;%\n  step_dummy(geo_value_factor) %&gt;%\n  step_epi_lag(case_rate, lag = c(0, 7, 14)) %&gt;%\n  step_epi_ahead(case_rate, ahead = 7, role = \"predictor\") %&gt;%\n  step_mutate(\n    pct_diff_ahead = case_when(\n      lag_7_case_rate == 0 ~ 0,\n      TRUE ~ (ahead_7_case_rate - lag_0_case_rate) / lag_0_case_rate\n    ),\n    pct_diff_wk1 = case_when(\n      lag_7_case_rate == 0 ~ 0,\n      TRUE ~ (lag_0_case_rate - lag_7_case_rate) / lag_7_case_rate\n    ),\n    pct_diff_wk2 = case_when(\n      lag_14_case_rate == 0 ~ 0,\n      TRUE ~ (lag_7_case_rate - lag_14_case_rate) / lag_14_case_rate\n    )\n  ) %&gt;%\n  step_mutate(\n    response = case_when(\n      pct_diff_ahead &lt; -0.20 ~ \"down\",\n      pct_diff_ahead &gt; 0.25 ~ \"up\",\n      TRUE ~ \"flat\"\n    ),\n    role = \"outcome\"\n  ) %&gt;%\n  step_rm(\n    death_rate, case_rate, lag_0_case_rate, lag_7_case_rate,\n    lag_14_case_rate, ahead_7_case_rate, pct_diff_ahead\n  ) %&gt;%\n  step_epi_naomit()\n\nWe will fit the multinomial regression and examine the predictions:\n\nwf &lt;- epi_workflow(r, parsnip::multinom_reg()) %&gt;%\n  fit(jhu)\n\nlatest &lt;- get_test_data(recipe = r, x = jhu)\npredict(wf, latest) %&gt;% filter(!is.na(.pred_class))\n\n#&gt; An `epi_df` object, 5 x 3 with metadata:\n#&gt; * geo_type  = state\n#&gt; * time_type = day\n#&gt; * as_of     = 2022-05-31 12:08:25\n#&gt; \n#&gt; # A tibble: 5 × 3\n#&gt;   geo_value time_value .pred_class\n#&gt; * &lt;chr&gt;     &lt;date&gt;     &lt;fct&gt;      \n#&gt; 1 ca        2021-12-31 up         \n#&gt; 2 fl        2021-12-31 up         \n#&gt; 3 nj        2021-12-31 up         \n#&gt; 4 ny        2021-12-31 up         \n#&gt; 5 tx        2021-12-31 flat\n\n\nWe can also look at the estimated coefficients and model summary information:\n\nextract_fit_engine(wf)\n\n#&gt; Call:\n#&gt; nnet::multinom(formula = ..y ~ ., data = data, trace = FALSE)\n#&gt; \n#&gt; Coefficients:\n#&gt;      (Intercept)   time_value geo_value_factor_fl geo_value_factor_nj\n#&gt; flat   -58.11177  0.003162471          -0.5978151            1.350320\n#&gt; up      46.45080 -0.002429847          -0.4682080            1.572085\n#&gt;      geo_value_factor_ny geo_value_factor_tx pct_diff_wk1 pct_diff_wk2\n#&gt; flat            3.113677          -0.3010305     1.263089     3.610543\n#&gt; up              3.172692          -0.2505232     2.194646     4.266267\n#&gt; \n#&gt; Residual Deviance: 1529.929 \n#&gt; AIC: 1561.929\n\n\nOne could also use a formula in epi_recipe() to achieve the same results as above. However, only one of add_formula(), add_recipe(), or workflow_variables() can be specified. For the purpose of demonstrating add_formula rather than add_recipe, we will prep and bake our recipe to return a data.frame that could be used for model fitting.\n\nb &lt;- bake(prep(r, jhu), jhu)\n\nepi_workflow() %&gt;%\n  add_formula(response ~ geo_value + time_value + pct_diff_wk1 + pct_diff_wk2) %&gt;%\n  add_model(parsnip::multinom_reg()) %&gt;%\n  fit(data = b)\n\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: multinom_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; response ~ geo_value + time_value + pct_diff_wk1 + pct_diff_wk2\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Call:\n#&gt; nnet::multinom(formula = ..y ~ ., data = data, trace = FALSE)\n#&gt; \n#&gt; Coefficients:\n#&gt;      (Intercept) geo_valuefl geo_valuenj geo_valueny geo_valuetx\n#&gt; flat   -58.11158  -0.5978159    1.350325    3.113684  -0.3010308\n#&gt; up      46.45071  -0.4682087    1.572090    3.172698  -0.2505236\n#&gt;        time_value pct_diff_wk1 pct_diff_wk2\n#&gt; flat  0.003162461     1.263093     3.610536\n#&gt; up   -0.002429839     2.194649     4.266259\n#&gt; \n#&gt; Residual Deviance: 1529.929 \n#&gt; AIC: 1561.929"
  },
  {
    "objectID": "preprocessing-and-models.html#benefits-of-lagging-and-leading-in-epipredict",
    "href": "preprocessing-and-models.html#benefits-of-lagging-and-leading-in-epipredict",
    "title": "5  Examples of Preprocessing and Models",
    "section": "5.5 Benefits of Lagging and Leading in epipredict",
    "text": "5.5 Benefits of Lagging and Leading in epipredict\nThe step_epi_ahead and step_epi_lag functions in the epipredict package is handy for creating correct lags and leads for future predictions.\nLet’s start with a simple dataset and preprocessing:\n\nex &lt;- filter(\n  case_death_rate_subset,\n  time_value &gt;= \"2021-12-01\",\n  time_value &lt;= \"2021-12-31\",\n  geo_value == \"ca\"\n)\n\ndim(ex)\n\n#&gt; [1] 31  4\n\n\nWe want to predict death rates on 2022-01-07, which is 7 days ahead of the latest available date in our dataset.\nWe will compare two methods of trying to create lags and leads:\n\np1 &lt;- epi_recipe(ex) %&gt;%\n  step_epi_lag(case_rate, lag = c(0, 7, 14)) %&gt;%\n  step_epi_lag(death_rate, lag = c(0, 7, 14)) %&gt;%\n  step_epi_ahead(death_rate, ahead = 7, role = \"outcome\") %&gt;%\n  step_epi_naomit() %&gt;%\n  prep()\n\nb1 &lt;- bake(p1, ex)\nb1\n\n#&gt; # A tibble: 17 × 11\n#&gt;   time_value geo_value case_rate death_rate lag_0_case_rate lag_7_case_rate\n#&gt;   &lt;date&gt;     &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1 2021-12-15 ca             15.8      0.157            15.8            18.0\n#&gt; 2 2021-12-16 ca             16.3      0.155            16.3            17.4\n#&gt; 3 2021-12-17 ca             16.9      0.158            16.9            17.4\n#&gt; 4 2021-12-18 ca             17.6      0.164            17.6            17.2\n#&gt; 5 2021-12-19 ca             19.1      0.165            19.1            16.3\n#&gt; 6 2021-12-20 ca             20.6      0.164            20.6            16.0\n#&gt; # ℹ 11 more rows\n#&gt; # ℹ 5 more variables: lag_14_case_rate &lt;dbl&gt;, lag_0_death_rate &lt;dbl&gt;, …\n\np2 &lt;- epi_recipe(ex) %&gt;%\n  step_mutate(\n    lag0case_rate = lag(case_rate, 0),\n    lag7case_rate = lag(case_rate, 7),\n    lag14case_rate = lag(case_rate, 14),\n    lag0death_rate = lag(death_rate, 0),\n    lag7death_rate = lag(death_rate, 7),\n    lag14death_rate = lag(death_rate, 14),\n    ahead7death_rate = lead(death_rate, 7)\n  ) %&gt;%\n  step_epi_naomit() %&gt;%\n  prep()\n\nb2 &lt;- bake(p2, ex)\nb2\n\n#&gt; # A tibble: 10 × 11\n#&gt;   time_value geo_value case_rate death_rate lag0case_rate lag7case_rate\n#&gt;   &lt;date&gt;     &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 2021-12-15 ca             15.8      0.157          15.8          18.0\n#&gt; 2 2021-12-16 ca             16.3      0.155          16.3          17.4\n#&gt; 3 2021-12-17 ca             16.9      0.158          16.9          17.4\n#&gt; 4 2021-12-18 ca             17.6      0.164          17.6          17.2\n#&gt; 5 2021-12-19 ca             19.1      0.165          19.1          16.3\n#&gt; 6 2021-12-20 ca             20.6      0.164          20.6          16.0\n#&gt; # ℹ 4 more rows\n#&gt; # ℹ 5 more variables: lag14case_rate &lt;dbl&gt;, lag0death_rate &lt;dbl&gt;, …\n\n\nNotice the difference in number of rows b1 and b2 returns. This is because the second version, the one that doesn’t use step_epi_ahead and step_epi_lag, has omitted dates compared to the one that used the epipredict functions.\n\ndates_used_in_training1 &lt;- b1 %&gt;%\n  select(-ahead_7_death_rate) %&gt;%\n  na.omit() %&gt;%\n  select(time_value)\ndates_used_in_training1\n\n#&gt; # A tibble: 17 × 1\n#&gt;   time_value\n#&gt;   &lt;date&gt;    \n#&gt; 1 2021-12-15\n#&gt; 2 2021-12-16\n#&gt; 3 2021-12-17\n#&gt; 4 2021-12-18\n#&gt; 5 2021-12-19\n#&gt; 6 2021-12-20\n#&gt; # ℹ 11 more rows\n\ndates_used_in_training2 &lt;- b2 %&gt;%\n  select(-ahead7death_rate) %&gt;%\n  na.omit() %&gt;%\n  select(time_value)\ndates_used_in_training2\n\n#&gt; # A tibble: 10 × 1\n#&gt;   time_value\n#&gt;   &lt;date&gt;    \n#&gt; 1 2021-12-15\n#&gt; 2 2021-12-16\n#&gt; 3 2021-12-17\n#&gt; 4 2021-12-18\n#&gt; 5 2021-12-19\n#&gt; 6 2021-12-20\n#&gt; # ℹ 4 more rows\n\n\nThe model that is trained based on the {recipes} functions will predict 7 days ahead from 2021-12-24 instead of 7 days ahead from 2021-12-31."
  },
  {
    "objectID": "preprocessing-and-models.html#references",
    "href": "preprocessing-and-models.html#references",
    "title": "5  Examples of Preprocessing and Models",
    "section": "5.6 References",
    "text": "5.6 References\nMcDonald, Bien, Green, Hu, et al. “Can auxiliary indicators improve COVID-19 forecasting and hotspot prediction?.” Proceedings of the National Academy of Sciences 118.51 (2021): e2111453118. doi:10.1073/pnas.2111453118"
  },
  {
    "objectID": "preprocessing-and-models.html#attribution",
    "href": "preprocessing-and-models.html#attribution",
    "title": "5  Examples of Preprocessing and Models",
    "section": "5.7 Attribution",
    "text": "5.7 Attribution\nThis vignette contains a modified part of the COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University as republished in the COVIDcast Epidata API.. See the COVIDcast Epidata API documentation for its modifications, and the code above for further modifications. This data set is licensed under the terms of the Creative Commons Attribution 4.0 International license by the Johns Hopkins University on behalf of its Center for Systems Science in Engineering. Copyright Johns Hopkins University 2020."
  },
  {
    "objectID": "preprocessing-and-models.html#footnotes",
    "href": "preprocessing-and-models.html#footnotes",
    "title": "5  Examples of Preprocessing and Models",
    "section": "",
    "text": "We could continue with the Poisson model, but we’ll switch to the Gaussian likelihood just for simplicity.↩︎"
  },
  {
    "objectID": "sliding-forecasters.html#comparing-different-forecasting-engines",
    "href": "sliding-forecasters.html#comparing-different-forecasting-engines",
    "title": "6  Demonstrations of sliding AR and ARX forecasters",
    "section": "6.1 Comparing different forecasting engines",
    "text": "6.1 Comparing different forecasting engines\n\n6.1.1 Example using CLI and case data from US states\nFirst, we download the version history (i.e. archive) of the percentage of doctor’s visits with CLI (COVID-like illness) computed from medical insurance claims and the number of new confirmed COVID-19 cases per 100,000 population (daily) for all 50 states from the COVIDcast API. We process as before, with the modification that we use sync = \"locf\" in epix_merge() so that the last version of each observation can be carried forward to extrapolate unavailable versions for the less up-to-date input archive.\n\nus_raw_history_dfs &lt;-\n  readRDS(system.file(\"extdata\", \"all_states_covidcast_signals.rds\",\n    package = \"epipredict\", mustWork = TRUE\n  ))\n\nus_cli_archive &lt;- us_raw_history_dfs[[1]] %&gt;%\n  select(geo_value, time_value, version = issue, percent_cli = value) %&gt;%\n  as_epi_archive(compactify = TRUE)\nus_cases_archive &lt;- us_raw_history_dfs[[2]] %&gt;%\n  select(geo_value, time_value, version = issue, case_rate = value) %&gt;%\n  as_epi_archive(compactify = TRUE)\n\nus_archive &lt;- epix_merge(us_cli_archive, us_cases_archive,\n  sync = \"locf\", compactify = TRUE\n)\n\nAfter obtaining the latest snapshot of the data, we produce forecasts on that data using the default engine of simple linear regression and compare to a random forest.\nNote that all of the warnings about the forecast date being less than the most recent update date of the data have been suppressed to avoid cluttering the output.\n\n# Latest snapshot of data, and forecast dates\nus_latest &lt;- epix_as_of(us_archive, max_version = max(us_archive$versions_end))\nfc_time_values &lt;- seq(as.Date(\"2020-08-01\"), as.Date(\"2021-11-01\"),\n  by = \"1 month\"\n)\n\nk_week_ahead &lt;- function(epi_df, outcome, predictors, ahead = 7, engine) {\n  epi_df %&gt;%\n    epi_slide(\n      ~ arx_forecaster(\n        .x, outcome, predictors, engine,\n        args_list = arx_args_list(ahead = ahead)\n      ) %&gt;%\n        extract2(\"predictions\") %&gt;%\n        select(-c(geo_value, time_value)),\n      before = 120L - 1L,\n      ref_time_values = fc_time_values,\n      new_col_name = \"fc\"\n    ) %&gt;%\n    select(geo_value, time_value, starts_with(\"fc\")) %&gt;%\n    mutate(engine_type = engine$engine)\n}\n\n# Generate the forecasts and bind them together\nfc &lt;- bind_rows(\n  purrr::map_dfr(\n    c(7, 14, 21, 28),\n    ~ k_week_ahead(\n      us_latest, \"case_rate\", c(\"case_rate\", \"percent_cli\"), .x,\n      engine = linear_reg()\n    )\n  ),\n  purrr::map_dfr(\n    c(7, 14, 21, 28),\n    ~ k_week_ahead(\n      us_latest, \"case_rate\", c(\"case_rate\", \"percent_cli\"), .x,\n      engine = rand_forest(mode = \"regression\")\n    )\n  )\n) %&gt;%\n  mutate(.pred_distn = nested_quantiles(fc_.pred_distn)) %&gt;%\n  unnest(.pred_distn) %&gt;%\n  pivot_wider(names_from = tau, values_from = q)\n\nHere, arx_forecaster() does all the heavy lifting. It creates leads of the target (respecting time stamps and locations) along with lags of the features (here, the response and doctors visits), estimates a forecasting model using the specified engine, creates predictions, and non-parametric confidence bands.\nTo see how the predictions compare, we plot them on top of the latest case rates. Note that even though we’ve fitted the model on all states, we’ll just display the results for two states, California (CA) and Florida (FL), to get a sense of the model performance while keeping the graphic simple.\n\n\nCode\nfc_cafl &lt;- fc %&gt;% filter(geo_value %in% c(\"ca\", \"fl\"))\nlatest_cafl &lt;- us_latest %&gt;% filter(geo_value %in% c(\"ca\", \"fl\"))\n\nggplot(fc_cafl, aes(fc_target_date, group = time_value, fill = engine_type)) +\n  geom_line(\n    data = latest_cafl, aes(x = time_value, y = case_rate),\n    inherit.aes = FALSE, color = \"gray50\"\n  ) +\n  geom_ribbon(aes(ymin = `0.05`, ymax = `0.95`), alpha = 0.4) +\n  geom_line(aes(y = fc_.pred)) +\n  geom_point(aes(y = fc_.pred), size = 0.5) +\n  geom_vline(aes(xintercept = time_value), linetype = 2, alpha = 0.5) +\n  facet_grid(vars(geo_value), vars(engine_type), scales = \"free\") +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %y\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"Date\", y = \"Reported COVID-19 case rates\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFor the two states of interest, simple linear regression clearly performs better than random forest in terms of accuracy of the predictions and does not result in such in overconfident predictions (overly narrow confidence bands). Though, in general, neither approach produces amazingly accurate forecasts. This could be because the behaviour is rather different across states and the effects of other notable factors such as age and public health measures may be important to account for in such forecasting. Including such factors as well as making enhancements such as correcting for outliers are some improvements one could make to this simple model.1\n\n\n6.1.2 Example using case data from Canada\nBy leveraging the flexibility of epiprocess, we can apply the same techniques to data from other sources. Since some collaborators are in British Columbia, Canada, we’ll do essentially the same thing for Canada as we did above.\nThe COVID-19 Canada Open Data Working Group collects daily time series data on COVID-19 cases, deaths, recoveries, testing and vaccinations at the health region and province levels. Data are collected from publicly available sources such as government datasets and news releases. Unfortunately, there is no simple versioned source, so we have created our own from the Github commit history.\nFirst, we load versioned case rates at the provincial level. After converting these to 7-day averages (due to highly variable provincial reporting mismatches), we then convert the data to an epi_archive object, and extract the latest version from it. Finally, we run the same forcasting exercise as for the American data, but here we compare the forecasts produced from using simple linear regression with those from using boosted regression trees.\n\n# source(\"drafts/canada-case-rates.R)\ncan &lt;- readRDS(\n  system.file(\"extdata\", \"can_prov_cases.rds\",\n    package = \"epipredict\", mustWork = TRUE\n  )\n) %&gt;%\n  group_by(version, geo_value) %&gt;%\n  arrange(time_value) %&gt;%\n  mutate(cr_7dav = RcppRoll::roll_meanr(case_rate, n = 7L)) # %&gt;%\n# filter(geo_value %in% c('Alberta', \"BC\"))\ncan &lt;- as_epi_archive(can, compactify = TRUE)\ncan_latest &lt;- epix_as_of(can, max_version = max(can$DT$version))\n\n\n# Generate the forecasts, and bind them together\ncan_fc &lt;- bind_rows(\n  purrr:::map_dfr(\n    c(7, 14, 21, 28),\n    ~ k_week_ahead(can_latest, \"cr_7dav\", \"cr_7dav\", .x, linear_reg())\n  ),\n  purrr::map_dfr(\n    c(7, 14, 21, 28),\n    ~ k_week_ahead(\n      can_latest, \"cr_7dav\", \"cr_7dav\", .x,\n      boost_tree(mode = \"regression\", trees = 20)\n    )\n  )\n) %&gt;%\n  mutate(.pred_distn = nested_quantiles(fc_.pred_distn)) %&gt;%\n  unnest(.pred_distn) %&gt;%\n  pivot_wider(names_from = tau, values_from = q)\n\nThe first figure shows the results for all of the provinces using linear regression.\n\n\nCode\nggplot(\n  can_fc %&gt;% filter(engine_type == \"lm\"),\n  aes(x = fc_target_date, group = time_value)\n) +\n  coord_cartesian(xlim = lubridate::ymd(c(\"2020-12-01\", NA))) +\n  geom_line(\n    data = can_latest, aes(x = time_value, y = cr_7dav),\n    inherit.aes = FALSE, color = \"gray50\"\n  ) +\n  geom_ribbon(aes(ymin = `0.05`, ymax = `0.95`, fill = geo_value),\n    alpha = 0.4\n  ) +\n  geom_line(aes(y = fc_.pred)) +\n  geom_point(aes(y = fc_.pred), size = 0.5) +\n  geom_vline(aes(xintercept = time_value), linetype = 2, alpha = 0.5) +\n  facet_wrap(~geo_value, scales = \"free_y\", ncol = 3) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %y\") +\n  labs(\n    title = \"Using simple linear regression\", x = \"Date\",\n    y = \"Reported COVID-19 case rates\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCompare those forecasts with a related set using Gradient Boosting.\n\n\nCode\nggplot(\n  can_fc %&gt;% filter(engine_type == \"xgboost\"),\n  aes(x = fc_target_date, group = time_value)\n) +\n  coord_cartesian(xlim = lubridate::ymd(c(\"2020-12-01\", NA))) +\n  geom_line(\n    data = can_latest, aes(x = time_value, y = cr_7dav),\n    inherit.aes = FALSE, color = \"gray50\"\n  ) +\n  geom_ribbon(aes(ymin = `0.05`, ymax = `0.95`, fill = geo_value),\n    alpha = 0.4\n  ) +\n  geom_line(aes(y = fc_.pred)) +\n  geom_point(aes(y = fc_.pred), size = 0.5) +\n  geom_vline(aes(xintercept = time_value), linetype = 2, alpha = 0.5) +\n  facet_wrap(~geo_value, scales = \"free_y\", ncol = 3) +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %y\") +\n  labs(\n    title = \"Using boosted regression trees\", x = \"Date\",\n    y = \"Reported COVID-19 case rates\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nBoth approaches tend to produce quite volatile forecasts (point predictions) and/or are overly confident (very narrow bands), particularly when boosted regression trees are used. But as this is meant to be a simple demonstration of sliding with different engines in arx_forecaster, we may devote another vignette to work on improving the predictive modelling using the suite of tools available in epipredict."
  },
  {
    "objectID": "sliding-forecasters.html#pseudoprospective-vs.-unfaithful-retrospective-forecasting",
    "href": "sliding-forecasters.html#pseudoprospective-vs.-unfaithful-retrospective-forecasting",
    "title": "6  Demonstrations of sliding AR and ARX forecasters",
    "section": "6.2 Pseudoprospective vs. unfaithful retrospective forecasting",
    "text": "6.2 Pseudoprospective vs. unfaithful retrospective forecasting\n\n6.2.1 Example using case data from US states\nWe will now run pseudoprospective forecasts based on properly-versioned data (that would have been available in real-time) to forecast future COVID-19 case rates from current and past COVID-19 case rates for all states. That is, we can make forecasts on the archive, us_archive, and compare those to forecasts on (time windows of) the latest data, us_latest, using the same general set-up as above. For pseudoprospective forecasting, note that us_archive is fed into epix_slide(), while for simpler (unfaithful) retrospective forecasting, us_latest is fed into epi_slide(). #%% update to include percent_cli after that issue is fixed?\n\nk_week_ahead_as_of &lt;- function(ahead, version = c(\"faithful\", \"unfaithful\")) {\n  version &lt;- match.arg(version)\n  if (version == \"faithful\") {\n    epix_slide(\n      us_archive,\n      ~ arx_forecaster(\n        .x,\n        \"case_rate\",\n        c(\"case_rate\", \"percent_cli\"),\n        args_list = arx_args_list(ahead = ahead)\n      ) %&gt;%\n        extract2(\"predictions\") %&gt;%\n        select(-time_value),\n      before = 120 - 1,\n      ref_time_values = fc_time_values,\n      new_col_name = \"fc\"\n    ) %&gt;%\n      mutate(version = \"version faithful\") %&gt;%\n      rename(geo_value = \"fc_geo_value\")\n  } else {\n    k_week_ahead(\n      us_latest, \"case_rate\", c(\"case_rate\", \"percent_cli\"),\n      ahead, linear_reg()\n    ) %&gt;%\n      mutate(version = \"not version faithful\")\n  }\n}\n\n# Generate the forecasts, and bind them together\nfc &lt;- bind_rows(\n  purrr::map_dfr(c(7, 14, 21, 28), ~ k_week_ahead_as_of(.x)),\n  purrr::map_dfr(c(7, 14, 21, 28), ~ k_week_ahead_as_of(.x, \"unfaithful\"))\n) %&gt;%\n  mutate(.pred_distn = nested_quantiles(fc_.pred_distn)) %&gt;%\n  unnest(.pred_distn) %&gt;%\n  pivot_wider(names_from = tau, values_from = q)\n\nNow we can plot the results on top of the latest case rates. As before, we will only display and focus on the results for FL and CA for simplicity.\n\n\nCode\nfc_cafl &lt;- fc %&gt;% filter(geo_value %in% c(\"ca\", \"fl\"))\nlatest_cafl &lt;- us_latest %&gt;% filter(geo_value %in% c(\"ca\", \"fl\"))\n\nggplot(fc_cafl, aes(x = fc_target_date, group = time_value)) +\n  geom_line(\n    data = latest_cafl, aes(x = time_value, y = case_rate),\n    inherit.aes = FALSE, color = \"gray50\"\n  ) +\n  geom_ribbon(aes(ymin = `0.05`, ymax = `0.95`, fill = version), alpha = 0.4) +\n  geom_line(aes(y = fc_.pred)) +\n  geom_point(aes(y = fc_.pred), size = 0.5) +\n  geom_vline(aes(xintercept = time_value), linetype = 2, alpha = 0.5) +\n  facet_grid(geo_value ~ version, scales = \"free\") +\n  scale_x_date(minor_breaks = \"month\", date_labels = \"%b %y\") +\n  labs(x = \"Date\", y = \"Reported COVID-19 case rates\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nAgain, we observe that the results are not great for these two states, but that’s likely due to the simplicity of the model (ex. the omission of key factors such as age and public health measures) and the quality of the data (ex. we have not personally corrected for anomalies in the data).\nWe shall leave it to the reader to try the above version aware and unaware forecasting exercise on the Canadian case rate data. The above code for the American state data should be readily adaptable for this purpose."
  },
  {
    "objectID": "sliding-forecasters.html#attribution",
    "href": "sliding-forecasters.html#attribution",
    "title": "6  Demonstrations of sliding AR and ARX forecasters",
    "section": "6.3 Attribution",
    "text": "6.3 Attribution\nThis object contains a modified part of the COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University as republished in the COVIDcast Epidata API. This data set is licensed under the terms of the Creative Commons Attribution 4.0 International license by Johns Hopkins University on behalf of its Center for Systems Science in Engineering. Copyright Johns Hopkins University 2020.\nModifications:\n\nFrom the COVIDcast Doctor Visits API: The signal percent_cli is taken directly from the API without changes.\nFrom the COVIDcast Epidata API: case_rate_7d_av signal was computed by Delphi from the original JHU-CSSE data by calculating moving averages of the preceding 7 days, so the signal for June 7 is the average of the underlying data for June 1 through 7, inclusive.\nOther modifications shown in code above."
  },
  {
    "objectID": "sliding-forecasters.html#footnotes",
    "href": "sliding-forecasters.html#footnotes",
    "title": "6  Demonstrations of sliding AR and ARX forecasters",
    "section": "",
    "text": "Note that, despite the above caveats, simple models like this tend to out-perform many far more complicated models in the online Covid forecasting due to those models high variance predictions.↩︎"
  }
]
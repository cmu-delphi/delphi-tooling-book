# Tidymodels with epidemiological time series data

```{r}
#| echo: false
source("_common.R")
```

## A gentle introduction to tidymodels - `epipredict` edition

The `epipredict` package builds off of the `tidymodels` framework to help it run smoothly with epidemiological time series data that can be composed as an `epi_df`. To this end, let's see how some of the techniques we applied in the previous chapter fare on some real-world `epi_df` data.

We'll start off by loading the `epipredict` and `tidymodels` packages.

```{r, message = FALSE}
library(epipredict)
library(tidymodels)
```

The data we'll work with is a subset of the built-in `case_death_rate_subset` dataset that contains COVID-19 death and case rates from Johns Hopkins University. We will only use the data for California, New York, Arkansas, Texas, and Hawaii over the modest time window of Jan. 8, 2021 to March 8, 2021. 

```{r}
jhu <- case_death_rate_subset %>%
  dplyr::filter(time_value >= "2021-01-08",
                time_value <= "2021-03-08", 
                geo_value %in% c("ca", "ny", "ar", "tx", "hi"))
```

Our goal for this example is to use this data to train a basic linear regression forecaster to predict the death rate one week ahead of the last day using past (lagged) deaths and cases for each state. 

We'll use `jhu` as our training data and we will hold off on creating the test set for now. One reason for this is because we cannot reasonably apply the `initial_split()` function or any variant of it from the `rsample` package to time series data. This is because randomly splitting the data into the training and test sets (as we did with the `penguins` dataset) does not make sense for data that is ordered by time and, moreover, for such data that is grouped by state. Ideally, what we want is a function that accounts for both time and grouping. And because there's no `initial_time_group_split()` function in existence just yet, we must take an alternative approach to split the data.

### Pre-processing 

The pre-processing steps are added to an `epi_recipe()` in the same way as for an ordinary `recipe()`. The major benefit from using an `epi_recipe()` is that we have more epi-themed steps at our disposal which allow for analysis that's more tailored to epidemiological data. The pre-processing transformations that are designed for such data include the infix of `epi` as in `step_epi` to make the distinction between them and the more general steps that come with `tidymodels`. 

While there are a number of epi recipe preprocessing steps available (which are listed under the [functions reference page of the epipredict website](https://cmu-delphi.github.io/epipredict/reference/index.html), we will focus on the following four in our example for simplicity.

:::: {.columns}

::: {.column width="52%"}
- `step_epi_lag()` to lag specified columns by some amount (generally these would be particular numeric predictors)

- `step_epi_ahead()` to shift specified data ahead by some amount (generally this would be the response)

- `step_center()` to normalize numeric data to have a mean of zero

- `step_epi_naomit()` to omit NAs from both predictors and outcomes at training time to fit the model
:::

::: {.column width="1%"}
<!-- empty column to create gap -->
:::

::: {.column width="47%"}

```{r, echo = FALSE}
knitr::include_graphics("img/epi_recipe_card.png")
```
:::

::::

In our case, we center the numerical predictors for a more meaningful interpretation of the intercept.

```{r}
r <- epi_recipe(jhu) %>%
  step_epi_lag(case_rate, death_rate, lag = c(0, 7, 14)) %>%
  step_epi_ahead(death_rate, ahead = 7) %>%
  step_center(contains("lag")) %>%
  step_epi_naomit()
```

### Get test data

Now that we have our recipe ready, let's revisit the problem of getting test data. Our approach is to utilize a function from `epipredict` that is designed for epidemiological time series data. This function, `get_test_data()` has two main inputs, an `epi_recipe` and a `epi_df`. It obtains test data for predictions from the inputted `epi_df` based on the longest lag period in the `epi_recipe`. The reason that we use the longest lag is that it is the farthest back we need to go to obtain the required values of the predictors to make the forecast. In our above recipe, the longest lag is 14 days. So `get_test_data()` obtains the last 14 + 1 = 15 days for each `geo_value` and all together these comprise the test data, `latest`.

```{r}
latest <- get_test_data(recipe = r, x = jhu) 
```

### Model and using a trained epi_workflow to predict

An `epi_workflow` extends the functionality of a `workflow` to handle the typical panel data structures found in epidemiology. Before we proceed to create an `epi_workflow` where we can specify our pre-processing and model, we should note one major benefit of this over a traditional `workflow`. Whereas a `workflow` can take in a pre-processor and a parsnip model specification, an `epi_workflow` may take in up to three arguments: a pre-processor, a parsnip model specification, and a post-processor. So there is the additional capability to post-process the results. For our example, since we only have a recipe (`r`), and a parsnip model (`parsnip::linear_reg()`), we will only fill in the first two arguments.

```{r}
wf <- epi_workflow(r, parsnip::linear_reg()) %>%
  fit(jhu) 
```
While we opted for the classic least squares regression model, we could also consider to use quantile regression (using the `quantile_reg` engine) to create median predictions because it is more robust than least squares to the type of distribution. 

Let's extract the model to briefly inspect it. We expect that it contains three coefficients for the 0, 7, and 14 day lagged case rate predictors and three coefficients for the 0, 7, and 14 day lagged death rate predictors.

```{r}
wf %>% 
  extract_fit_parsnip() 
```
Now we can use our trained workflow to predict the one-week-ahead death rate for each sampled state. 

```{r}
p <- predict(wf, latest)
p
```
Success! At this point, one might consider cleaning up these predictions by bounding them to ensure that they are at least 0 and by converting them from rates to counts. While post-processing is not currently supported by `tidymodels`, `epipredict` has several post-processing operations built into the package. And what's great is that the the work flow mirrors that for adding pre-processing steps. 

### Post-processing

:::: {.columns}

::: {.column width="75%"}

Essentially, each post-processing step is a layer to be added to the `frosting()` post-processing container, which is akin to a `recipe()`. To our `frosting()`, we'll add the following four layers

- `layer_predict()` to add a prediction layer for post-processing

- `layer_add_target_date()` to add the target date (ie. the date that the forecast is for)

- `layer_threshold()` to set the predictions to be at least 0

- `layer_population_scaling()` to "undo" per-capita scaling by state to get counts from rates
:::

::: {.column width="1%"}
<!-- empty column to create gap -->
:::

::: {.column width="24%"}

```{r, echo = FALSE}
knitr::include_graphics("img/postprocessing_cupcake.png")
```
:::

::::

Note that the ordering matters for the layers of frosting like it does for the steps of a recipe. In general, put `layer_predict()` in first and then what you want to do to the predictions after (in the order that you want them to happen). In our example, it is logical to add the layer to bound our predictions at 0 before scaling them.

But first we should ready the dataframe that contains the population data. We'll use the 2019 US Census data that comes built-in with the `epipredict` package and select the two columns that we need - the state abbreviation and corresponding population estimate. 

```{r}
pop_dat <- state_census %>% select(abbr, pop)
```
Let's go ahead and implement the post-processing that we outlined above.

```{r}
f <- frosting() %>%
  layer_predict() %>% 
  layer_add_target_date() %>% 
  layer_threshold(.pred, lower = 0) %>%
  layer_population_scaling(
    .pred,  
    df = pop_dat, 
    rate_rescaling = 1e5,
    by = c("geo_value" = "abbr"), 
    df_pop_col = "pop")
```
Note that `1e5` was selected for the `rescaling_rate` because the rates are "per 100,000 people" rather than "per person" as can be seen in the dataset documentation (see `?case_death_rate_subset`).

Now that the post-processing is set-up, do not forget to add the `frosting` to the `epi_workflow` before you predict. If you skip adding the `frosting` to the `epi_workflow` and go straight to predicting, then you will get the following:

```{r}
p <- predict(wf, latest)
p
```
Notice that we have a negative prediction for `hi`. And the predictions are rates and not counts. Based on these observations, is clear that the postprocessing was not implemented at all.

Let's fix this by adding our frosting object, `f`, to our `epi_workflow` and then using the updated workflow to `predict()`. As a result of this, we should get predictions that align more with our expectations.

```{r}
wf_fixed <- wf %>%  
  add_frosting(f)
wf_fixed
```

```{r}
p_fixed <- predict(wf_fixed, latest)
p_fixed
```
Much better! 

### Model validation

Let's compare our predicted death rates to the the true death rates that we can obtain from the `case_death_rate_subset` data. One thing to keep in mind is that in epidemiological modelling, there often times are not be true values to validate against - especially in the midst of an epidemic or pandemic. For instance, if you are predicting case or death rates one week ahead during the COVID-19 pandemic, at the time you are predicting you will not have the true case or death rates at your disposal. In contrast, when you are doing retrospective analysis, you should have some estimate of the truth at your disposal. 

```{r}
true_death_rates <- case_death_rate_subset %>% 
  filter(geo_value %in% c("ca", "ny", "ar", "tx", "hi") &
      time_value == "2021-03-15") %>% 
  select(geo_value, death_rate) 
```
To join the two dataframes containing the truth and the predictions, we could force `bind_cols()` to work, but since that function binds the rows in order of their appearance, the order of rows in both dataframes must match (otherwise we risk matching data meant for one state to another). In our example, notice that the first `geo_value` in `true_death_rates` is for `ar`, while it is for `ak` in `p_fixed`. And if we had more states to manage, it could become especially bothersome to have to re-organize the rows. In such situations, we can turn to `dplyr`'s `left_join()` function to add `true_death_rates` values as a column to the `p_fixed` dataframe by `geo_value`. The order of the rows does not matter for `left_join()` because it matches based on the linking variable `geo_value`.

```{r}
p_w_truth <- left_join(p_fixed, true_death_rates, by = "geo_value")
p_w_truth
```

Now let's use `metrics()` function to briefly assess our model performance. 

```{r}
p_w_truth %>%
  metrics(truth = death_rate, estimate = .pred)
```
As before, let's briefly go through the metrics that were produced. Root mean square error (RMSE) is the square root of the average squared difference between the predicted and the actual values. So it is a measure of accuracy because it informs us how well the model hit the mark (the truth). Since this is an error, lower values are better, and the absolute minimum value that could be achieved is 0. 

Mean absolute error (MAE) is the average absolute difference between the predicted and the actual values. It's interpretation is similar to RMSE in that lower values are better and the minimum that can be achieved is 0. A major difference between them is how much each error contributes. RMSE is more sensitive to outliers (consider what happens when you square a large value - there's a large difference between the predicted and the truth). The impact of larger errors is magnified, while smaller errors don't contribute as much. In contrast, the error contribution in MAE increases linearly (ex. an error of 4 would contribute twice as much as an error of 2). 

While it can be difficult to pinpoint how well exactly each metric is performing when using real life data (what are "good" values of each metric depends on the situation), from a quick inspection of the values that `metrics()` gives us, the RMSE and MAE are small enough such that there is no serious cause for concern.

We will leave it at that and not venture further into model-validation territory here, but feel free to venture further on your own. We recommend [this book](https://www.google.ca/books/edition/Medical_Risk_Prediction_Models/VQEWEAAAQBAJ?hl=en&gbpv=1&printsec=frontcover) on medical risk prediction models for further reading on the subject.

## Concluding remarks

In these past two chapters, we introduced `tidymodels` and illustrated how to its packages work together by way of example. Since they were both elementary examples, use them as a starting point to explore what more can be done with this wonderful set of packages. And yet, however wonderful they are, there are limitations like the glaring lack of a set of post-processing tools to refine the results. We fill this gap for epidemiological modelling with [frosting](https://cmu-delphi.github.io/epipredict/reference/add_frosting.html), which will be demonstrated in greater detail in later chapters, so stay tuned!

## Attribution

This vignette was largely adapted from [A Gentle Introduction to Tidymodels](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/) as well as [Tidymodels - Getting Started](https://www.tidymodels.org/start/recipes/) and [Tidymodels](https://wec.wur.nl/dse/24-tidymodels.html). 

